{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from drf import drf\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    }
   ],
   "source": [
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/CLUSTERING', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/CLUSTERING/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=2\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# New new implementation\n",
    "N_CLUSTERS=20\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_scaled)\n",
    "distances=[]\n",
    "mahalanobis_dist=[]\n",
    "counts=[]\n",
    "ideal_len=len(kmeans.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances.append(np.abs(np.sum(kmeans.labels_==i)-ideal_len))\n",
    "    counts.append(np.sum(kmeans.labels_==i))\n",
    "    mean_k= np.mean(X.loc[kmeans.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist.append(mahalanobis(mean_k, mean, np.linalg.inv(cov)))\n",
    "\n",
    "dist_df=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist, 'count': counts}, index=np.arange(N_CLUSTERS))\n",
    "dist_df=dist_df.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df['cumulative_count']=dist_df['count'].cumsum()\n",
    "dist_df['abs_diff']=np.abs(dist_df['cumulative_count']-ideal_len)\n",
    "\n",
    "final=(np.where(dist_df['abs_diff']==np.min(dist_df['abs_diff']))[0])[0]\n",
    "labelss=dist_df.index[0:final+1].to_list()\n",
    "labels=pd.Series(kmeans.labels_).isin(labelss)\n",
    "labels.index=X.index\n",
    "close_index=labels.index[np.where(labels==False)[0]]\n",
    "far_index=labels.index[np.where(labels==True)[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean_ = np.mean(X_train, axis=0)\n",
    "cov_ = np.cov(X_train.T)\n",
    "scaler_ = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_train_scaled = scaler_.fit_transform(X_train)\n",
    "\n",
    "kmeans_ = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_train_scaled)\n",
    "distances_=[]\n",
    "counts_=[]\n",
    "mahalanobis_dist_=[]\n",
    "ideal_len_=len(kmeans_.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances_.append(np.abs(np.sum(kmeans_.labels_==i)-ideal_len_))\n",
    "    counts_.append(np.sum(kmeans_.labels_==i))\n",
    "    mean_k_= np.mean(X_train.loc[kmeans_.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist_.append(mahalanobis(mean_k_, mean_, np.linalg.inv(cov_)))\n",
    "\n",
    "dist_df_=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist_, 'count': counts_}, index=np.arange(N_CLUSTERS))\n",
    "dist_df_=dist_df_.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df_['cumulative_count']=dist_df_['count'].cumsum()\n",
    "dist_df_['abs_diff']=np.abs(dist_df_['cumulative_count']-ideal_len_)\n",
    "\n",
    "final_=(np.where(dist_df_['abs_diff']==np.min(dist_df_['abs_diff']))[0])[0]\n",
    "labelss_=dist_df_.index[0:final_+1].to_list()\n",
    "labels_=pd.Series(kmeans_.labels_).isin(labelss_)\n",
    "labels_.index=X_train.index\n",
    "close_index_=labels_.index[np.where(labels_==False)[0]]\n",
    "far_index_=labels_.index[np.where(labels_==True)[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:02:45,213] A new study created in memory with name: no-name-356d681b-1170-40ba-b0c6-b3f0b278d480\n",
      "[I 2024-03-04 19:02:52,404] Trial 0 finished with value: 6.937608925970823 and parameters: {'learning_rate': 0.0713003929222653, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:02:55,046] Trial 1 finished with value: 12.527109038809398 and parameters: {'learning_rate': 0.0006784471913345375, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:02:55,049] A new study created in memory with name: no-name-ea90304b-f111-4642-bd43-36333a33cebc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:03:00,524] Trial 0 finished with value: 9.07003089184482 and parameters: {'num_trees': 409, 'mtry': 1, 'min_node_size': 67}. Best is trial 0 with value: 9.07003089184482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:03:08,247] Trial 1 finished with value: 7.275542030588492 and parameters: {'num_trees': 400, 'mtry': 15, 'min_node_size': 30}. Best is trial 1 with value: 7.275542030588492.\n",
      "[I 2024-03-04 19:03:08,250] A new study created in memory with name: no-name-7a4ea8ea-e886-40fc-89d9-9a08012593dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4376,  E(|Y-Yhat|): 1.0720,  E(|Yhat-Yhat'|): 1.2687\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4711,  E(|Yhat-Yhat'|): 0.4575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1010,  E(|Y-Yhat|): 2.3519,  E(|Yhat-Yhat'|): 2.5017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:04:49,599] Trial 0 finished with value: 3.041407315526319 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6360,  E(|Y-Yhat|): 0.8290,  E(|Yhat-Yhat'|): 0.3860\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4422,  E(|Yhat-Yhat'|): 0.4230\n",
      "[Epoch 200 (25%), batch 6] energy-loss: 0.2261,  E(|Y-Yhat|): 0.4416,  E(|Yhat-Yhat'|): 0.4311\n",
      "[Epoch 300 (38%), batch 6] energy-loss: 0.2274,  E(|Y-Yhat|): 0.4418,  E(|Yhat-Yhat'|): 0.4289\n",
      "[Epoch 400 (51%), batch 6] energy-loss: 0.2288,  E(|Y-Yhat|): 0.4356,  E(|Yhat-Yhat'|): 0.4136\n",
      "[Epoch 500 (64%), batch 6] energy-loss: 0.2395,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.4183\n",
      "[Epoch 600 (76%), batch 6] energy-loss: 0.2258,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.4220\n",
      "[Epoch 700 (89%), batch 6] energy-loss: 0.1808,  E(|Y-Yhat|): 0.4118,  E(|Yhat-Yhat'|): 0.4620\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0890,  E(|Y-Yhat|): 2.1953,  E(|Yhat-Yhat'|): 2.2128\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:06:17,801] Trial 1 finished with value: 4.233781359117593 and parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n",
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3068,  E(|Y-Yhat|): 0.8871,  E(|Yhat-Yhat'|): 1.1607\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1209,  E(|Y-Yhat|): 0.2849,  E(|Yhat-Yhat'|): 0.3282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2372,  E(|Y-Yhat|): 2.8857,  E(|Yhat-Yhat'|): 3.2971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "CRPS linear regression:  30.335993731879395\n",
      "CRPS boosted trees 9.56145418608216\n",
      "CRPS random forest 13.631153757122418\n",
      "CRPS engression 9.167586232235394\n"
     ]
    }
   ],
   "source": [
    "dtrain_ = lgb.Dataset(torch.tensor(X_train_.values, dtype=torch.float32).clone().detach(), label=y_train_.values)\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "    }\n",
    "    opt_params = params.copy()\n",
    "    n_rounds = opt_params[\"n_estimators\"]\n",
    "    del opt_params[\"n_estimators\"]\n",
    "    opt_params['feature_pre_filter']=False\n",
    "\n",
    "    # Use LightGBMLossGuideRegressor for distributional prediction\n",
    "    boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "    boosted_tree_model.train(opt_params, dtrain_, num_boost_round=n_rounds)\n",
    "\n",
    "    # Predict both the mean and standard deviation\n",
    "    pred_params=boosted_tree_model.predict(X_val, pred_type=\"parameters\")\n",
    "    y_val_hat_boost=pred_params['loc']\n",
    "    y_val_hat_std = pred_params['scale']\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_gaussian(y_val_np[i], mu=y_val_hat_boost[i], sig=y_val_hat_std[i]) for i in range(len(y_val))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "np.random.seed(seed)\n",
    "quantiles=list(np.random.uniform(0,1,N_SAMPLES))\n",
    "def rf(trial):\n",
    "    params = {'num_trees': trial.suggest_int('num_trees', 100, 500),\n",
    "        'mtry': trial.suggest_int('mtry', 1, 30),\n",
    "        'min_node_size': trial.suggest_int('min_node_size', 10, 100)}\n",
    "    \n",
    "    drf_model = drf(**params, seed=seed)\n",
    "    drf_model.fit(X_train_, y_train_)\n",
    "    \n",
    "    # Generate a sample from the drf model for each data point\n",
    "    y_val_hat=drf_model.predict(newdata = X_val, functional = \"quantile\", quantiles=quantiles)\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], y_val_hat.quantile[i].reshape(-1)) for i in range(len(y_val_np))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_drf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_drf = optuna.create_study(sampler=sampler_drf, direction='minimize')\n",
    "study_drf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(torch.tensor(X_train.values, dtype=torch.float32).clone().detach(), label=y_train.values)\n",
    "opt_params = study_boost.best_params.copy()\n",
    "n_rounds = opt_params[\"n_estimators\"]\n",
    "del opt_params[\"n_estimators\"]\n",
    "opt_params['feature_pre_filter']=False\n",
    "# Use LightGBMLossGuideRegressor for distributional prediction\n",
    "boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "boosted_tree_model.train(opt_params, dtrain, num_boost_round=n_rounds)\n",
    "# Predict both the mean and standard deviation\n",
    "pred_params=boosted_tree_model.predict(X_test, pred_type=\"parameters\")\n",
    "y_test_hat_boost=pred_params['loc']\n",
    "y_test_hat_std = pred_params['scale']\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_boost[i], sig=y_test_hat_std[i]) for i in range(len(y_test))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_boosted=np.mean(crps_values)\n",
    "\n",
    "drf_model=drf(**study_drf.best_params, seed=seed)\n",
    "drf_model.fit(X_train, y_train)\n",
    "# Generate a sample from the drf model for each data point\n",
    "y_test_hat_drf=drf_model.predict(newdata = X_test, functional = \"quantile\", quantiles=quantiles)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], y_test_hat_drf.quantile[i].reshape(-1)) for i in range(len(y_test_np))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_rf=np.mean(crps_values)\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_test - y_test_hat_linreg)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_linreg[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_linreg = np.mean(crps_values)\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1,1))\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:30,657] A new study created in memory with name: no-name-e2a9d29b-e253-4507-af13-0e3a103bf9bd\n",
      "[I 2024-03-04 19:08:33,473] Trial 0 finished with value: 6.937608925970823 and parameters: {'learning_rate': 0.0713003929222653, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:08:36,708] Trial 1 finished with value: 12.527109038809398 and parameters: {'learning_rate': 0.0006784471913345375, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:08:36,711] A new study created in memory with name: no-name-4f377d26-6161-4e4f-8457-c817e189b3ef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:47,006] Trial 0 finished with value: 9.07003089184482 and parameters: {'num_trees': 409, 'mtry': 1, 'min_node_size': 67}. Best is trial 0 with value: 9.07003089184482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:59,240] Trial 1 finished with value: 7.275542030588492 and parameters: {'num_trees': 400, 'mtry': 15, 'min_node_size': 30}. Best is trial 1 with value: 7.275542030588492.\n",
      "[I 2024-03-04 19:08:59,270] A new study created in memory with name: no-name-53c4582e-504f-440f-9a36-d447215e770f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4376,  E(|Y-Yhat|): 1.0720,  E(|Yhat-Yhat'|): 1.2687\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4711,  E(|Yhat-Yhat'|): 0.4575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1010,  E(|Y-Yhat|): 2.3519,  E(|Yhat-Yhat'|): 2.5017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:11:02,423] Trial 0 finished with value: 3.041407315526319 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6360,  E(|Y-Yhat|): 0.8290,  E(|Yhat-Yhat'|): 0.3860\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4422,  E(|Yhat-Yhat'|): 0.4230\n",
      "[Epoch 200 (25%), batch 6] energy-loss: 0.2261,  E(|Y-Yhat|): 0.4416,  E(|Yhat-Yhat'|): 0.4311\n",
      "[Epoch 300 (38%), batch 6] energy-loss: 0.2274,  E(|Y-Yhat|): 0.4418,  E(|Yhat-Yhat'|): 0.4289\n",
      "[Epoch 400 (51%), batch 6] energy-loss: 0.2288,  E(|Y-Yhat|): 0.4356,  E(|Yhat-Yhat'|): 0.4136\n",
      "[Epoch 500 (64%), batch 6] energy-loss: 0.2395,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.4183\n",
      "[Epoch 600 (76%), batch 6] energy-loss: 0.2258,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.4220\n",
      "[Epoch 700 (89%), batch 6] energy-loss: 0.1808,  E(|Y-Yhat|): 0.4118,  E(|Yhat-Yhat'|): 0.4620\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0890,  E(|Y-Yhat|): 2.1953,  E(|Yhat-Yhat'|): 2.2128\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:13:02,384] Trial 1 finished with value: 4.233781359117593 and parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n",
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3068,  E(|Y-Yhat|): 0.8871,  E(|Yhat-Yhat'|): 1.1607\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1209,  E(|Y-Yhat|): 0.2849,  E(|Yhat-Yhat'|): 0.3282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2372,  E(|Y-Yhat|): 2.8857,  E(|Yhat-Yhat'|): 3.2971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "CRPS linear regression:  30.335993731879395\n",
      "CRPS boosted trees 9.56145418608216\n",
      "CRPS random forest 13.631153757122418\n",
      "CRPS engression 9.167586232235394\n"
     ]
    }
   ],
   "source": [
    "dtrain_ = lgb.Dataset(torch.tensor(X_train_.values, dtype=torch.float32).clone().detach(), label=y_train_.values)\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "    }\n",
    "    opt_params = params.copy()\n",
    "    n_rounds = opt_params[\"n_estimators\"]\n",
    "    del opt_params[\"n_estimators\"]\n",
    "    opt_params['feature_pre_filter']=False\n",
    "\n",
    "    # Use LightGBMLossGuideRegressor for distributional prediction\n",
    "    boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "    boosted_tree_model.train(opt_params, dtrain_, num_boost_round=n_rounds)\n",
    "\n",
    "    # Predict both the mean and standard deviation\n",
    "    pred_params=boosted_tree_model.predict(X_val, pred_type=\"parameters\")\n",
    "    y_val_hat_boost=pred_params['loc']\n",
    "    y_val_hat_std = pred_params['scale']\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_gaussian(y_val_np[i], mu=y_val_hat_boost[i], sig=y_val_hat_std[i]) for i in range(len(y_val))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "np.random.seed(seed)\n",
    "quantiles=list(np.random.uniform(0,1,N_SAMPLES))\n",
    "def rf(trial):\n",
    "    params = {'num_trees': trial.suggest_int('num_trees', 100, 500),\n",
    "        'mtry': trial.suggest_int('mtry', 1, 30),\n",
    "        'min_node_size': trial.suggest_int('min_node_size', 10, 100)}\n",
    "    \n",
    "    drf_model = drf(**params, seed=seed)\n",
    "    drf_model.fit(X_train_, y_train_)\n",
    "    \n",
    "    # Generate a sample from the drf model for each data point\n",
    "    y_val_hat=drf_model.predict(newdata = X_val, functional = \"quantile\", quantiles=quantiles)\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], y_val_hat.quantile[i].reshape(-1)) for i in range(len(y_val_np))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_drf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_drf = optuna.create_study(sampler=sampler_drf, direction='minimize')\n",
    "study_drf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(torch.tensor(X_train.values, dtype=torch.float32).clone().detach(), label=y_train.values)\n",
    "opt_params = study_boost.best_params.copy()\n",
    "n_rounds = opt_params[\"n_estimators\"]\n",
    "del opt_params[\"n_estimators\"]\n",
    "opt_params['feature_pre_filter']=False\n",
    "# Use LightGBMLossGuideRegressor for distributional prediction\n",
    "boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "boosted_tree_model.train(opt_params, dtrain, num_boost_round=n_rounds)\n",
    "# Predict both the mean and standard deviation\n",
    "pred_params=boosted_tree_model.predict(X_test, pred_type=\"parameters\")\n",
    "y_test_hat_boost=pred_params['loc']\n",
    "y_test_hat_std = pred_params['scale']\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_boost[i], sig=y_test_hat_std[i]) for i in range(len(y_test))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_boosted=np.mean(crps_values)\n",
    "\n",
    "drf_model=drf(**study_drf.best_params, seed=seed)\n",
    "drf_model.fit(X_train, y_train)\n",
    "# Generate a sample from the drf model for each data point\n",
    "y_test_hat_drf=drf_model.predict(newdata = X_test, functional = \"quantile\", quantiles=quantiles)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], y_test_hat_drf.quantile[i].reshape(-1)) for i in range(len(y_test_np))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_rf=np.mean(crps_values)\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_test - y_test_hat_linreg)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_linreg[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_linreg = np.mean(crps_values)\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1,1))\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([85., 81., 87.,  ..., 85., 85., 87.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_tensor.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 10:01:42,647] A new study created in memory with name: no-name-e547f643-08e0-47ef-9c4c-a9171ff6ac54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4338,  E(|Y-Yhat|): 1.0227,  E(|Yhat-Yhat'|): 1.1779\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2444,  E(|Y-Yhat|): 0.4991,  E(|Yhat-Yhat'|): 0.5094\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1617,  E(|Y-Yhat|): 2.4525,  E(|Yhat-Yhat'|): 2.5816\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 10:03:14,305] Trial 0 finished with value: 3.997297018729463 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.997297018729463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.5307,  E(|Y-Yhat|): 0.7151,  E(|Yhat-Yhat'|): 0.3689\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2189,  E(|Y-Yhat|): 0.4321,  E(|Yhat-Yhat'|): 0.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-05 10:03:29,467] Trial 1 failed with parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_6472\\2127494930.py\", line 14, in engressor_NN\n",
      "    engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py\", line 47, in engression\n",
      "    engressor.train(x, y, num_epoches=num_epoches, batch_size=batch_size,\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py\", line 262, in train\n",
      "    y_sample1 = self.model(x_batch)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py\", line 223, in forward\n",
      "    return self.out_layer(self.inter_layer(self.input_layer(x)))\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py\", line 35, in forward\n",
      "    out = self.layer(out)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-05 10:03:29,504] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m sampler_engression \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     25\u001b[0m study_engression \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_engression, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mstudy_engression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengressor_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check if CUDA is available and if so, move the tensors and the model to the GPU\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mengressor_NN\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     12\u001b[0m     engressor_model\u001b[38;5;241m=\u001b[39mengression(X_train__tensor, y_train__tensor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], num_epoches\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m],num_layer\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layer\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], noise_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, resblock\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresblock\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     engressor_model\u001b[38;5;241m=\u001b[39m\u001b[43mengression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train__tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train__tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epoches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnoise_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresblock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Generate a sample from the engression model for each data point\u001b[39;00m\n\u001b[0;32m     17\u001b[0m y_val_hat_engression_samples \u001b[38;5;241m=\u001b[39m [engressor_model\u001b[38;5;241m.\u001b[39msample(torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray([X_val\u001b[38;5;241m.\u001b[39mvalues[i]])), sample_size\u001b[38;5;241m=\u001b[39mN_SAMPLES) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_val))]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py:47\u001b[0m, in \u001b[0;36mengression\u001b[1;34m(x, y, sigmoid, num_layer, hidden_dim, noise_dim, add_bn, resblock, beta, lr, num_epoches, batch_size, print_every_nepoch, print_times_per_epoch, device, standardize, verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sample sizes for the covariates and response do not match. Please check.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m engressor \u001b[38;5;241m=\u001b[39m Engressor(in_dim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], out_dim\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m     43\u001b[0m                       num_layer\u001b[38;5;241m=\u001b[39mnum_layer, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, noise_dim\u001b[38;5;241m=\u001b[39mnoise_dim, \n\u001b[0;32m     44\u001b[0m                       sigmoid\u001b[38;5;241m=\u001b[39msigmoid, resblock\u001b[38;5;241m=\u001b[39mresblock, add_bn\u001b[38;5;241m=\u001b[39madd_bn, beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m     45\u001b[0m                       lr\u001b[38;5;241m=\u001b[39mlr, num_epoches\u001b[38;5;241m=\u001b[39mnum_epoches, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m     46\u001b[0m                       standardize\u001b[38;5;241m=\u001b[39mstandardize, device\u001b[38;5;241m=\u001b[39mdevice, check_device\u001b[38;5;241m=\u001b[39mverbose, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mengressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprint_every_nepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every_nepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_times_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_times_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstandardize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engressor\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py:262\u001b[0m, in \u001b[0;36mEngressor.train\u001b[1;34m(self, x, y, num_epoches, batch_size, print_every_nepoch, print_times_per_epoch, standardize, verbose)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 262\u001b[0m     y_sample1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     y_sample2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_batch)\n\u001b[0;32m    264\u001b[0m     loss, loss1, loss2 \u001b[38;5;241m=\u001b[39m energy_loss_two_sample(y_batch, y_sample1, y_sample2, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py:223\u001b[0m, in \u001b[0;36mStoNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(x)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter_layer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py:35\u001b[0m, in \u001b[0;36mStoLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, eps], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_act(out)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from drf import drf\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark_suite.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(cov)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     50\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     51\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     50\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     51\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16599, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "np.linalg.inv(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.21391060e+04, -1.91769834e+02,  1.01992189e+01,\n",
       "        -6.20669455e+00, -4.06397490e+01, -2.38630219e+01,\n",
       "         7.67521688e+02,  7.20824919e-01, -2.46737556e+01,\n",
       "         1.17628657e-02,  1.22922006e-02,  1.22922006e-02,\n",
       "         1.24216423e-02, -8.98351803e-04, -1.40631499e-04,\n",
       "         1.24216423e-02],\n",
       "       [-1.91769834e+02,  5.87296133e+02,  5.15133831e-01,\n",
       "         9.07528413e-01,  5.10515565e-01,  7.26653674e+00,\n",
       "        -1.05121549e+02, -1.36988171e-02, -1.96439028e-01,\n",
       "         4.29649933e-04,  4.58733734e-04,  4.58733734e-04,\n",
       "         4.74203922e-04, -3.41005033e-05, -1.72670663e-05,\n",
       "         4.74203922e-04],\n",
       "       [ 1.01992189e+01,  5.15133831e-01,  7.73643481e-02,\n",
       "         3.00655351e-03,  1.20565177e-03,  1.36462404e-02,\n",
       "        -2.73906768e-01,  5.96023040e-04, -1.70299365e-02,\n",
       "         1.27764791e-05,  1.67190809e-05,  1.67190809e-05,\n",
       "         1.78566964e-05, -5.95066117e-06, -1.55978696e-06,\n",
       "         1.78566964e-05],\n",
       "       [-6.20669455e+00,  9.07528413e-01,  3.00655351e-03,\n",
       "         1.06015784e-02,  8.77643127e-03, -1.52741018e-01,\n",
       "        -8.89562710e-01, -2.66577129e-04, -1.25284688e-02,\n",
       "        -1.61572458e-05, -1.60832605e-05, -1.60832605e-05,\n",
       "        -1.60945499e-05, -3.87390013e-08,  2.35541395e-08,\n",
       "        -1.60945499e-05],\n",
       "       [-4.06397490e+01,  5.10515565e-01,  1.20565177e-03,\n",
       "         8.77643127e-03,  8.69933275e-01,  7.00245844e-01,\n",
       "        -1.69754424e+00, -7.33985344e-03,  5.56671600e-03,\n",
       "        -8.46883934e-05, -8.42867846e-05, -8.42867846e-05,\n",
       "        -8.41007984e-05, -1.30529309e-07, -1.32361901e-07,\n",
       "        -8.41007984e-05],\n",
       "       [-2.38630219e+01,  7.26653674e+00,  1.36462404e-02,\n",
       "        -1.52741018e-01,  7.00245844e-01,  1.49046160e+01,\n",
       "        -1.43241604e+00, -7.05927428e-03, -1.20865369e-02,\n",
       "         9.06751521e-04,  9.11530716e-04,  9.11530716e-04,\n",
       "         9.15829176e-04, -9.42821097e-06, -6.22981431e-06,\n",
       "         9.15829176e-04],\n",
       "       [ 7.67521688e+02, -1.05121549e+02, -2.73906768e-01,\n",
       "        -8.89562710e-01, -1.69754424e+00, -1.43241604e+00,\n",
       "         1.00119190e+02,  3.73537725e-02, -1.39415927e-01,\n",
       "         5.69335614e-04,  5.58799142e-04,  5.58799142e-04,\n",
       "         5.56632099e-04,  9.31120060e-06,  2.45929184e-06,\n",
       "         5.56632099e-04],\n",
       "       [ 7.20824919e-01, -1.36988171e-02,  5.96023040e-04,\n",
       "        -2.66577129e-04, -7.33985344e-03, -7.05927428e-03,\n",
       "         3.73537725e-02,  1.31903185e-04, -1.49383034e-04,\n",
       "         1.82989498e-06,  1.90689369e-06,  1.90689369e-06,\n",
       "         1.92291737e-06, -1.21163991e-07, -2.26046654e-08,\n",
       "         1.92291737e-06],\n",
       "       [-2.46737556e+01, -1.96439028e-01, -1.70299365e-02,\n",
       "        -1.25284688e-02,  5.56671600e-03, -1.20865369e-02,\n",
       "        -1.39415927e-01, -1.49383034e-04,  3.90108876e-01,\n",
       "        -1.30960074e-06, -1.95454044e-06, -1.95454044e-06,\n",
       "        -2.14103550e-06,  1.17854625e-06,  3.13912440e-07,\n",
       "        -2.14103550e-06],\n",
       "       [ 1.17628657e-02,  4.29649933e-04,  1.27764791e-05,\n",
       "        -1.61572458e-05, -8.46883934e-05,  9.06751521e-04,\n",
       "         5.69335614e-04,  1.82989498e-06, -1.30960074e-06,\n",
       "         1.11783055e-07,  1.12243295e-07,  1.12243295e-07,\n",
       "         1.12419714e-07, -1.18886337e-09, -2.96238691e-10,\n",
       "         1.12419714e-07],\n",
       "       [ 1.22922006e-02,  4.58733734e-04,  1.67190809e-05,\n",
       "        -1.60832605e-05, -8.42867846e-05,  9.11530716e-04,\n",
       "         5.58799142e-04,  1.90689369e-06, -1.95454044e-06,\n",
       "         1.12243295e-07,  1.14226057e-07,  1.14226057e-07,\n",
       "         1.14588406e-07, -3.06554979e-09, -5.71425316e-10,\n",
       "         1.14588406e-07],\n",
       "       [ 1.22922006e-02,  4.58733734e-04,  1.67190809e-05,\n",
       "        -1.60832605e-05, -8.42867846e-05,  9.11530716e-04,\n",
       "         5.58799142e-04,  1.90689369e-06, -1.95454044e-06,\n",
       "         1.12243295e-07,  1.14226057e-07,  1.14226057e-07,\n",
       "         1.14588406e-07, -3.06554979e-09, -5.71425316e-10,\n",
       "         1.14588406e-07],\n",
       "       [ 1.24216423e-02,  4.74203922e-04,  1.78566964e-05,\n",
       "        -1.60945499e-05, -8.41007984e-05,  9.15829176e-04,\n",
       "         5.56632099e-04,  1.92291737e-06, -2.14103550e-06,\n",
       "         1.12419714e-07,  1.14588406e-07,  1.14588406e-07,\n",
       "         1.15309025e-07, -3.34674752e-09, -9.81576857e-10,\n",
       "         1.15309025e-07],\n",
       "       [-8.98351803e-04, -3.41005033e-05, -5.95066117e-06,\n",
       "        -3.87390013e-08, -1.30529309e-07, -9.42821097e-06,\n",
       "         9.31120060e-06, -1.21163991e-07,  1.17854625e-06,\n",
       "        -1.18886337e-09, -3.06554979e-09, -3.06554979e-09,\n",
       "        -3.34674752e-09,  3.16963985e-09,  4.25300826e-10,\n",
       "        -3.34674752e-09],\n",
       "       [-1.40631499e-04, -1.72670663e-05, -1.55978696e-06,\n",
       "         2.35541395e-08, -1.32361901e-07, -6.22981431e-06,\n",
       "         2.45929184e-06, -2.26046654e-08,  3.13912440e-07,\n",
       "        -2.96238691e-10, -5.71425316e-10, -5.71425316e-10,\n",
       "        -9.81576857e-10,  4.25300826e-10,  6.22385521e-10,\n",
       "        -9.81576857e-10],\n",
       "       [ 1.24216423e-02,  4.74203922e-04,  1.78566964e-05,\n",
       "        -1.60945499e-05, -8.41007984e-05,  9.15829176e-04,\n",
       "         5.56632099e-04,  1.92291737e-06, -2.14103550e-06,\n",
       "         1.12419714e-07,  1.14588406e-07,  1.14588406e-07,\n",
       "         1.15309025e-07, -3.34674752e-09, -9.81576857e-10,\n",
       "         1.15309025e-07]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:MahalanobisND:Wrong format in calib_entries argument. Must be convertible to integer, list or np.array\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong format in calib_entries argument. Must be convertible to integer, list or np.array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmahalanobis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mahalanobis\n\u001b[1;32m----> 2\u001b[0m \u001b[43mMahalanobis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:512\u001b[0m, in \u001b[0;36mMahalanobis\u001b[1;34m(input_array, calib_rows, nan_subst_method)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ShapeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmpty input array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_subst_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:390\u001b[0m, in \u001b[0;36mMahalanobisND.__init__\u001b[1;34m(self, array, calib_entries, nan_method)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m array\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalib_entries \u001b[38;5;241m=\u001b[39m calib_entries\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_calibration_subarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_nans()\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibration_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalibration_chunk\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# mean to which array entries will be compared\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:83\u001b[0m, in \u001b[0;36mMahalanobisBenchmark._select_calibration_subarray\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrong format in calib_entries argument. Must be convertible to integer, list or np.array\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensionality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong format in calib_entries argument. Must be convertible to integer, list or np.array"
     ]
    }
   ],
   "source": [
    "from mahalanobis import Mahalanobis\n",
    "Mahalanobis(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 17:27:38,141] A new study created in memory with name: no-name-21501702-7a56-4a74-98ed-f8366ed6a79e\n",
      "[I 2024-03-05 17:27:38,755] Trial 0 finished with value: 22.947252239814784 and parameters: {'n_splines': 17, 'lam': 0.001154132971137168}. Best is trial 0 with value: 22.947252239814784.\n",
      "[I 2024-03-05 17:27:38,823] Trial 1 finished with value: 10.740112483235158 and parameters: {'n_splines': 15, 'lam': 0.17636469336159113}. Best is trial 1 with value: 10.740112483235158.\n",
      "[I 2024-03-05 17:27:38,875] Trial 2 finished with value: 10.896560638127578 and parameters: {'n_splines': 12, 'lam': 0.00472487079152679}. Best is trial 1 with value: 10.740112483235158.\n",
      "[I 2024-03-05 17:27:38,936] Trial 3 finished with value: 10.703338919026589 and parameters: {'n_splines': 8, 'lam': 0.19124590142517375}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:38,990] Trial 4 finished with value: 11.200486892602623 and parameters: {'n_splines': 7, 'lam': 0.0018408544111075849}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,040] Trial 5 finished with value: 10.720930978839837 and parameters: {'n_splines': 15, 'lam': 0.7247363402746422}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,075] Trial 6 finished with value: 10.784504830252562 and parameters: {'n_splines': 5, 'lam': 0.03440145332328687}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,144] Trial 7 finished with value: 10.98992076901203 and parameters: {'n_splines': 18, 'lam': 0.06879837817714736}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,203] Trial 8 finished with value: 12.971812468294816 and parameters: {'n_splines': 16, 'lam': 0.007509797119626296}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,256] Trial 9 finished with value: 10.876867451236265 and parameters: {'n_splines': 19, 'lam': 0.13922824544531598}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,339] Trial 10 finished with value: 10.712530052622382 and parameters: {'n_splines': 10, 'lam': 0.8536439312999228}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,408] Trial 11 finished with value: 10.710547077298443 and parameters: {'n_splines': 10, 'lam': 0.7029820442590767}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,453] Trial 12 finished with value: 10.706705196949272 and parameters: {'n_splines': 9, 'lam': 0.28864747460782875}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,507] Trial 13 finished with value: 10.705114803939338 and parameters: {'n_splines': 8, 'lam': 0.24853970486508473}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,562] Trial 14 finished with value: 10.71634708082109 and parameters: {'n_splines': 5, 'lam': 0.06482547888607312}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,622] Trial 15 finished with value: 10.708039089323746 and parameters: {'n_splines': 8, 'lam': 0.30262433161267577}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,680] Trial 16 finished with value: 10.78788458645363 and parameters: {'n_splines': 12, 'lam': 0.01305628749775024}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,730] Trial 17 finished with value: 10.717766258805705 and parameters: {'n_splines': 7, 'lam': 0.08205899786080603}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,809] Trial 18 finished with value: 10.733514833597695 and parameters: {'n_splines': 11, 'lam': 0.025726384630819565}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,884] Trial 19 finished with value: 10.724623629085979 and parameters: {'n_splines': 14, 'lam': 0.4132275001264877}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,947] Trial 20 finished with value: 10.69165906297434 and parameters: {'n_splines': 7, 'lam': 0.1416966751064176}. Best is trial 20 with value: 10.69165906297434.\n",
      "[I 2024-03-05 17:27:40,005] Trial 21 finished with value: 10.69011567790952 and parameters: {'n_splines': 7, 'lam': 0.1558279525021782}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,053] Trial 22 finished with value: 11.110018067269309 and parameters: {'n_splines': 6, 'lam': 0.12923874230939328}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,111] Trial 23 finished with value: 10.73418916915517 and parameters: {'n_splines': 7, 'lam': 0.040548324084896954}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,161] Trial 24 finished with value: 10.716293739787838 and parameters: {'n_splines': 9, 'lam': 0.11107468382701746}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,205] Trial 25 finished with value: 10.855595908395024 and parameters: {'n_splines': 6, 'lam': 0.4945327252466708}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,269] Trial 26 finished with value: 10.70713880673174 and parameters: {'n_splines': 9, 'lam': 0.18771804473308543}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,313] Trial 27 finished with value: 10.72538886965758 and parameters: {'n_splines': 5, 'lam': 0.047298348679016916}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,426] Trial 28 finished with value: 10.739009303955115 and parameters: {'n_splines': 11, 'lam': 0.01849738124111256}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,481] Trial 29 finished with value: 10.720902380992936 and parameters: {'n_splines': 8, 'lam': 0.45352670391090916}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,543] Trial 30 finished with value: 11.119070141131916 and parameters: {'n_splines': 6, 'lam': 0.08640477515525971}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,593] Trial 31 finished with value: 10.704860412012883 and parameters: {'n_splines': 8, 'lam': 0.2425199691366182}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,662] Trial 32 finished with value: 10.69369241288308 and parameters: {'n_splines': 7, 'lam': 0.19815659631339977}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,719] Trial 33 finished with value: 10.689977997044839 and parameters: {'n_splines': 7, 'lam': 0.16650510779941946}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,779] Trial 34 finished with value: 11.104111883955335 and parameters: {'n_splines': 6, 'lam': 0.13643197532853657}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,838] Trial 35 finished with value: 10.733215497921089 and parameters: {'n_splines': 7, 'lam': 0.056269701602484797}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,895] Trial 36 finished with value: 10.727652859326133 and parameters: {'n_splines': 13, 'lam': 0.18793008113552953}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,961] Trial 37 finished with value: 10.710774240809304 and parameters: {'n_splines': 10, 'lam': 0.35230920872081417}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,017] Trial 38 finished with value: 12.867875842498968 and parameters: {'n_splines': 5, 'lam': 0.0014389084003192712}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,069] Trial 39 finished with value: 10.702242628361851 and parameters: {'n_splines': 7, 'lam': 0.10875707643577283}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,129] Trial 40 finished with value: 10.716818999166847 and parameters: {'n_splines': 9, 'lam': 0.6346532240312879}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,186] Trial 41 finished with value: 10.709198780673457 and parameters: {'n_splines': 7, 'lam': 0.09571942558037608}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,244] Trial 42 finished with value: 11.084180114955695 and parameters: {'n_splines': 6, 'lam': 0.1573796114609918}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,311] Trial 43 finished with value: 10.696115431328924 and parameters: {'n_splines': 7, 'lam': 0.20931468859991814}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,363] Trial 44 finished with value: 10.703759717261539 and parameters: {'n_splines': 8, 'lam': 0.21014968613900983}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,438] Trial 45 finished with value: 17.09773056199712 and parameters: {'n_splines': 17, 'lam': 0.0023242092566905016}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,494] Trial 46 finished with value: 10.710114228005105 and parameters: {'n_splines': 11, 'lam': 0.6002972219063147}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,559] Trial 47 finished with value: 10.810001677124259 and parameters: {'n_splines': 5, 'lam': 0.3254362148982656}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,612] Trial 48 finished with value: 10.721593872057928 and parameters: {'n_splines': 10, 'lam': 0.06957715682643138}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,670] Trial 49 finished with value: 10.724183386937533 and parameters: {'n_splines': 7, 'lam': 0.03048336120424258}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,738] Trial 50 finished with value: 10.706229275681416 and parameters: {'n_splines': 9, 'lam': 0.24362812130219916}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,778] Trial 51 finished with value: 10.702113893331642 and parameters: {'n_splines': 7, 'lam': 0.1090277941160679}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,827] Trial 52 finished with value: 10.703122195168408 and parameters: {'n_splines': 8, 'lam': 0.17520318734936818}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,861] Trial 53 finished with value: 11.105143032805707 and parameters: {'n_splines': 6, 'lam': 0.13522990541552923}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,909] Trial 54 finished with value: 10.805671162426858 and parameters: {'n_splines': 7, 'lam': 0.8756486902051627}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,975] Trial 55 finished with value: 10.70554526912651 and parameters: {'n_splines': 8, 'lam': 0.07649216734648331}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,037] Trial 56 finished with value: 10.901055408579944 and parameters: {'n_splines': 6, 'lam': 0.37670825670339686}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,142] Trial 57 finished with value: 10.717274314799205 and parameters: {'n_splines': 5, 'lam': 0.05350023625794819}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,213] Trial 58 finished with value: 10.7063107268843 and parameters: {'n_splines': 9, 'lam': 0.2583673358724434}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,286] Trial 59 finished with value: 10.709823653350293 and parameters: {'n_splines': 7, 'lam': 0.09466047705508453}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,346] Trial 60 finished with value: 10.703600728025528 and parameters: {'n_splines': 8, 'lam': 0.11854752846129753}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,420] Trial 61 finished with value: 11.022708651945115 and parameters: {'n_splines': 20, 'lam': 0.1124545581580333}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,471] Trial 62 finished with value: 10.696257664052531 and parameters: {'n_splines': 7, 'lam': 0.2099123317771141}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,520] Trial 63 finished with value: 11.013694324196877 and parameters: {'n_splines': 6, 'lam': 0.22537184624307616}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,572] Trial 64 finished with value: 10.689940143720507 and parameters: {'n_splines': 7, 'lam': 0.16149924969158563}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,619] Trial 65 finished with value: 10.703054553498703 and parameters: {'n_splines': 8, 'lam': 0.15393825588758575}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,667] Trial 66 finished with value: 10.709366512468872 and parameters: {'n_splines': 10, 'lam': 0.5096391140332179}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,726] Trial 67 finished with value: 11.069834017120066 and parameters: {'n_splines': 6, 'lam': 0.17122181488535343}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,771] Trial 68 finished with value: 10.749296909034452 and parameters: {'n_splines': 7, 'lam': 0.00783476715336149}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,819] Trial 69 finished with value: 10.70852291477327 and parameters: {'n_splines': 9, 'lam': 0.3688194722806357}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,860] Trial 70 finished with value: 10.798076948120489 and parameters: {'n_splines': 5, 'lam': 0.2937780201854044}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,907] Trial 71 finished with value: 10.693745702544632 and parameters: {'n_splines': 7, 'lam': 0.19842799701003408}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,958] Trial 72 finished with value: 10.70341876997132 and parameters: {'n_splines': 8, 'lam': 0.19551401465223667}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,011] Trial 73 finished with value: 10.952195955066175 and parameters: {'n_splines': 6, 'lam': 0.29577647103370375}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,075] Trial 74 finished with value: 10.745624878779875 and parameters: {'n_splines': 15, 'lam': 0.14910303504977906}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,126] Trial 75 finished with value: 10.70015364940289 and parameters: {'n_splines': 7, 'lam': 0.22484833166746326}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,177] Trial 76 finished with value: 10.76420502583025 and parameters: {'n_splines': 7, 'lam': 0.4369798741401498}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,234] Trial 77 finished with value: 10.706440943819333 and parameters: {'n_splines': 9, 'lam': 0.27076820868789264}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,277] Trial 78 finished with value: 11.119764984825766 and parameters: {'n_splines': 6, 'lam': 0.08742004512002642}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,319] Trial 79 finished with value: 10.706401363002046 and parameters: {'n_splines': 8, 'lam': 0.062196666103272305}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,365] Trial 80 finished with value: 10.767817517407908 and parameters: {'n_splines': 5, 'lam': 0.1917685653021189}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,418] Trial 81 finished with value: 10.701116632624942 and parameters: {'n_splines': 7, 'lam': 0.2282366056740934}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,459] Trial 82 finished with value: 10.692583586119813 and parameters: {'n_splines': 7, 'lam': 0.13683759087427624}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,519] Trial 83 finished with value: 11.109854770706013 and parameters: {'n_splines': 6, 'lam': 0.12944947250507843}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,560] Trial 84 finished with value: 10.703050628307473 and parameters: {'n_splines': 8, 'lam': 0.15537055470730246}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,609] Trial 85 finished with value: 10.707857976065755 and parameters: {'n_splines': 7, 'lam': 0.09804074912062231}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,640] Trial 86 finished with value: 10.736966184391175 and parameters: {'n_splines': 7, 'lam': 0.337621496137364}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,690] Trial 87 finished with value: 10.703202721878162 and parameters: {'n_splines': 8, 'lam': 0.18239720283604452}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,727] Trial 88 finished with value: 11.106619870838058 and parameters: {'n_splines': 6, 'lam': 0.1334731796290905}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,776] Trial 89 finished with value: 10.749759312072767 and parameters: {'n_splines': 13, 'lam': 0.07633791179009217}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,824] Trial 90 finished with value: 10.73881341647395 and parameters: {'n_splines': 9, 'lam': 0.047333313820304576}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,887] Trial 91 finished with value: 10.697396817146313 and parameters: {'n_splines': 7, 'lam': 0.21453821876217216}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,941] Trial 92 finished with value: 10.694618649143301 and parameters: {'n_splines': 7, 'lam': 0.20267881130298912}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,988] Trial 93 finished with value: 10.969135170929526 and parameters: {'n_splines': 6, 'lam': 0.27440270350415535}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,040] Trial 94 finished with value: 10.703058135656168 and parameters: {'n_splines': 8, 'lam': 0.16537477885630056}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,103] Trial 95 finished with value: 10.697872644587733 and parameters: {'n_splines': 7, 'lam': 0.11894229285198582}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,155] Trial 96 finished with value: 10.703518308459776 and parameters: {'n_splines': 8, 'lam': 0.20027010005037627}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,203] Trial 97 finished with value: 10.846175466392118 and parameters: {'n_splines': 6, 'lam': 0.5326347976685996}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,241] Trial 98 finished with value: 10.841803961526011 and parameters: {'n_splines': 5, 'lam': 0.40216963881506307}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,289] Trial 99 finished with value: 10.691620823617571 and parameters: {'n_splines': 7, 'lam': 0.14192466473457643}. Best is trial 64 with value: 10.689940143720507.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1586 is out of bounds for axis 0 with size 1586",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate the CRPS\u001b[39;00m\n\u001b[0;32m     37\u001b[0m std_dev_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(y_test \u001b[38;5;241m-\u001b[39m y_test_hat_gam)\n\u001b[1;32m---> 38\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m [crps_gaussian(y_test_np[i], mu\u001b[38;5;241m=\u001b[39my_test_hat_gam[i], sig\u001b[38;5;241m=\u001b[39mstd_dev_error) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_hat_gam))]\n\u001b[0;32m     39\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(crps_gam)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRPS GAM: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crps_gam)\n",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate the CRPS\u001b[39;00m\n\u001b[0;32m     37\u001b[0m std_dev_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(y_test \u001b[38;5;241m-\u001b[39m y_test_hat_gam)\n\u001b[1;32m---> 38\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m [crps_gaussian(\u001b[43my_test_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, mu\u001b[38;5;241m=\u001b[39my_test_hat_gam[i], sig\u001b[38;5;241m=\u001b[39mstd_dev_error) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_hat_gam))]\n\u001b[0;32m     39\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(crps_gam)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRPS GAM: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crps_gam)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1586 is out of bounds for axis 0 with size 1586"
     ]
    }
   ],
   "source": [
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "            'lam': trial.suggest_float('lam', 1e-3, 1, log=True)}\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the CRPS\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    std_dev_error = np.std(y_val - y_val_hat_gam)\n",
    "    crps_gam = [crps_gaussian(y_val_np[i], mu=y_val_hat_gam[i], sig=std_dev_error) for i in range(len(y_val_hat_gam))]\n",
    "    crps_gam = np.mean(crps_gam)\n",
    "\n",
    "    return crps_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LinearGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "std_dev_error = np.std(y_test - y_test_hat_gam)\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LogisticGAM, s\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp', 'windspeed_max', 'windspeed_avg', 'precipitation', 'dew_point',\n",
       "       'humidity', 'hour', 'dayminute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16714, 10)\n",
      "NumberOfDependents 9\n",
      "361060\n",
      "(38474, 7)\n",
      "361061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566602, 10)\n",
      "361062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10082, 26)\n",
      "361063\n",
      "(13488, 16)\n",
      "361065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13376, 10)\n",
      "361066\n",
      "(10578, 7)\n",
      "361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72998, 50)\n",
      "361069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940160, 24)\n",
      "361070\n",
      "(7608, 20)\n",
      "361273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71090, 7)\n",
      "361274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57580, 54)\n",
      "361275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13272, 20)\n",
      "361276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3434, 419)\n",
      "D337 9\n",
      "D350 8\n",
      "D478 9\n",
      "D551 9\n",
      "D553 9\n",
      "D607 9\n",
      "D608 9\n",
      "D703 9\n",
      "361277\n",
      "(20634, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361278\n",
      "(10000, 22)\n"
     ]
    }
   ],
   "source": [
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)\n",
    "    for col in X.columns:\n",
    "        if X[col].nunique() < 10:\n",
    "            print(col, X[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361072\n",
      "Task 361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from drf import drf\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/SPATIAL_DEPTH', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/SPATIAL_DEPTH/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features\n",
    "X = X.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# activate pandas conversion for rpy2\n",
    "pandas2ri.activate()\n",
    "\n",
    "# import R's \"ddalpha\" package\n",
    "ddalpha = importr('ddalpha')\n",
    "\n",
    "# explicitly import the projDepth function\n",
    "spatialDepth = robjects.r['depth.spatial']\n",
    "\n",
    "# calculate the spatial depth for each data point\n",
    "spatial_depth = spatialDepth(X, X)\n",
    "\n",
    "spatial_depth=pd.Series(spatial_depth,index=X.index)\n",
    "far_index=spatial_depth.index[np.where(spatial_depth<=np.quantile(spatial_depth,0.2))[0]]\n",
    "close_index=spatial_depth.index[np.where(spatial_depth>np.quantile(spatial_depth,0.2))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# convert the R vector to a pandas Series\n",
    "spatial_depth_ = spatialDepth(X_train, X_train)\n",
    "\n",
    "spatial_depth_=pd.Series(spatial_depth_,index=X_train.index)\n",
    "far_index_=spatial_depth_.index[np.where(spatial_depth_<=np.quantile(spatial_depth_,0.2))[0]]\n",
    "close_index_=spatial_depth_.index[np.where(spatial_depth_>np.quantile(spatial_depth_,0.2))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "std_X_train_ = np.std(X_train_, axis=0)\n",
    "X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define d_out and d_in\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.228778214732248"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction_train = np.array([np.mean(y_train)]*len(y_train))\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_train - constant_prediction_train)\n",
    "constant_prediction = np.array([np.mean(y_train)]*len(y_test))\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=constant_prediction[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_constant = np.mean(crps_values)\n",
    "CRPS_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88.82481306, 88.82481306, 88.82481306, ..., 88.82481306,\n",
       "       88.82481306, 88.82481306])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.67837458822024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRPS_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.782906346427744,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 26.814233393032794,\n",
       " 187.49243491262348,\n",
       " 26.838596400639656,\n",
       " 177.8113249737905,\n",
       " 157.74788152939988,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 180.70825991211495,\n",
       " 26.782906346427744,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 176.84718750657188,\n",
       " 26.869917088485956,\n",
       " 167.25097941447643,\n",
       " 168.20667799635712,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 177.8113249737905,\n",
       " 183.61169292788176,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 165.34238663820142,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 184.58088626734454,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 162.4867573925009,\n",
       " 26.838596400639656,\n",
       " 174.92125813966973,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 26.782906346427744,\n",
       " 190.4096787244055,\n",
       " 169.16328974732107,\n",
       " 183.61169292788176,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 26.786387306053033,\n",
       " 185.55074938195384,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 171.07918915374282,\n",
       " 187.49243491262348,\n",
       " 189.43665132362818,\n",
       " 188.4642327804397,\n",
       " 163.43764332853803,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 26.838596400639656,\n",
       " 177.8113249737905,\n",
       " 185.55074938195384,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 26.869917088485956,\n",
       " 182.64318210388237,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 180.70825991211495,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 176.84718750657188,\n",
       " 178.77622543118278,\n",
       " 170.12079871377787,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 190.4096787244055,\n",
       " 190.4096787244055,\n",
       " 161.53688486242672,\n",
       " 181.67536672505793,\n",
       " 181.67536672505793,\n",
       " 161.53688486242672,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 163.43764332853803,\n",
       " 184.58088626734454,\n",
       " 187.49243491262348,\n",
       " 184.58088626734454,\n",
       " 179.74187497834234,\n",
       " 161.53688486242672,\n",
       " 26.79682991991504,\n",
       " 188.4642327804397,\n",
       " 189.43665132362818,\n",
       " 180.70825991211495,\n",
       " 178.77622543118278,\n",
       " 168.20667799635712,\n",
       " 190.4096787244055,\n",
       " 178.77622543118278,\n",
       " 168.20667799635712,\n",
       " 26.814233393032794,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 180.70825991211495,\n",
       " 182.64318210388237,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 182.64318210388237,\n",
       " 180.70825991211495,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 190.4096787244055,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 191.38330334453087,\n",
       " 190.4096787244055,\n",
       " 182.64318210388237,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 173.95949504068867,\n",
       " 26.908193073262286,\n",
       " 185.55074938195384,\n",
       " 187.49243491262348,\n",
       " 26.869917088485956,\n",
       " 166.29621016818717,\n",
       " 187.49243491262348,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 181.67536672505793,\n",
       " 26.782906346427744,\n",
       " 186.52126971917383,\n",
       " 156.80334252270154,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 158.69352351632753,\n",
       " 178.77622543118278,\n",
       " 178.77622543118278,\n",
       " 185.55074938195384,\n",
       " 26.814233393032794,\n",
       " 185.55074938195384,\n",
       " 184.58088626734454,\n",
       " 189.43665132362818,\n",
       " 161.53688486242672,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 185.55074938195384,\n",
       " 26.95342144314358,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 170.12079871377787,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 161.53688486242672,\n",
       " 177.8113249737905,\n",
       " 191.38330334453087,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 171.07918915374282,\n",
       " 26.786387306053033,\n",
       " 26.782906346427744,\n",
       " 163.43764332853803,\n",
       " 186.52126971917383,\n",
       " 190.4096787244055,\n",
       " 176.84718750657188,\n",
       " 179.74187497834234,\n",
       " 180.70825991211495,\n",
       " 165.34238663820142,\n",
       " 188.4642327804397,\n",
       " 175.88382712870967,\n",
       " 160.58804320837632,\n",
       " 184.58088626734454,\n",
       " 162.4867573925009,\n",
       " 181.67536672505793,\n",
       " 164.38952542069475,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.908193073262286,\n",
       " 184.58088626734454,\n",
       " 26.79682991991504,\n",
       " 178.77622543118278,\n",
       " 185.55074938195384,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 169.16328974732107,\n",
       " 171.07918915374282,\n",
       " 171.07918915374282,\n",
       " 184.58088626734454,\n",
       " 185.55074938195384,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 178.77622543118278,\n",
       " 174.92125813966973,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 26.79682991991504,\n",
       " 187.49243491262348,\n",
       " 190.4096787244055,\n",
       " 178.77622543118278,\n",
       " 176.84718750657188,\n",
       " 189.43665132362818,\n",
       " 190.4096787244055,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 176.84718750657188,\n",
       " 174.92125813966973,\n",
       " 180.70825991211495,\n",
       " 184.58088626734454,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 26.869917088485956,\n",
       " 184.58088626734454,\n",
       " 172.03844553550053,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 173.95949504068867,\n",
       " 179.74187497834234,\n",
       " 173.95949504068867,\n",
       " 26.786387306053033,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 188.4642327804397,\n",
       " 162.4867573925009,\n",
       " 172.03844553550053,\n",
       " 172.99855253624364,\n",
       " 185.55074938195384,\n",
       " 190.4096787244055,\n",
       " 175.88382712870967,\n",
       " 175.88382712870967,\n",
       " 177.8113249737905,\n",
       " 171.07918915374282,\n",
       " 191.38330334453087,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 164.38952542069475,\n",
       " 170.12079871377787,\n",
       " 26.79682991991504,\n",
       " 168.20667799635712,\n",
       " 165.34238663820142,\n",
       " 155.85992508343276,\n",
       " 172.99855253624364,\n",
       " 26.79682991991504,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 178.77622543118278,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.95342144314358,\n",
       " 178.77622543118278,\n",
       " 178.77622543118278,\n",
       " 26.869917088485956,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 26.869917088485956,\n",
       " 186.52126971917383,\n",
       " 172.03844553550053,\n",
       " 182.64318210388237,\n",
       " 185.55074938195384,\n",
       " 189.43665132362818,\n",
       " 160.58804320837632,\n",
       " 27.005598758453633,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 172.03844553550053,\n",
       " 176.84718750657188,\n",
       " 26.782906346427744,\n",
       " 178.77622543118278,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 26.782906346427744,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 160.58804320837632,\n",
       " 163.43764332853803,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 188.4642327804397,\n",
       " 190.4096787244055,\n",
       " 177.8113249737905,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 156.80334252270154,\n",
       " 26.838596400639656,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 182.64318210388237,\n",
       " 168.20667799635712,\n",
       " 176.84718750657188,\n",
       " 177.8113249737905,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 27.064721052449833,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 172.99855253624364,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 111.45625151651896,\n",
       " 26.782906346427744,\n",
       " 180.70825991211495,\n",
       " 187.49243491262348,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 26.79682991991504,\n",
       " 158.69352351632753,\n",
       " 182.64318210388237,\n",
       " 176.84718750657188,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 172.03844553550053,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 166.29621016818717,\n",
       " 190.4096787244055,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 170.12079871377787,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 26.782906346427744,\n",
       " 172.03844553550053,\n",
       " 183.61169292788176,\n",
       " 167.25097941447643,\n",
       " 191.38330334453087,\n",
       " 26.838596400639656,\n",
       " 179.74187497834234,\n",
       " 26.95342144314358,\n",
       " 166.29621016818717,\n",
       " 26.782906346427744,\n",
       " 175.88382712870967,\n",
       " 111.45625151651896,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 178.77622543118278,\n",
       " 26.908193073262286,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 179.74187497834234,\n",
       " 191.38330334453087,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 165.34238663820142,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 162.4867573925009,\n",
       " 185.55074938195384,\n",
       " 187.49243491262348,\n",
       " 164.38952542069475,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 172.03844553550053,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 180.70825991211495,\n",
       " 190.4096787244055,\n",
       " 155.85992508343276,\n",
       " 191.38330334453087,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 188.4642327804397,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 185.55074938195384,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 190.4096787244055,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 179.74187497834234,\n",
       " 163.43764332853803,\n",
       " 186.52126971917383,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 26.782906346427744,\n",
       " 187.49243491262348,\n",
       " 27.064721052449833,\n",
       " 172.99855253624364,\n",
       " 26.814233393032794,\n",
       " 189.43665132362818,\n",
       " 181.67536672505793,\n",
       " 191.38330334453087,\n",
       " 175.88382712870967,\n",
       " 180.70825991211495,\n",
       " 181.67536672505793,\n",
       " 179.74187497834234,\n",
       " 168.20667799635712,\n",
       " 168.20667799635712,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 181.67536672505793,\n",
       " 181.67536672505793,\n",
       " 26.814233393032794,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 180.70825991211495,\n",
       " 188.4642327804397,\n",
       " 190.4096787244055,\n",
       " 111.45625151651896,\n",
       " 175.88382712870967,\n",
       " 179.74187497834234,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 162.4867573925009,\n",
       " 159.6402501217417,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 27.130783832227916,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 157.74788152939988,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 178.77622543118278,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 162.4867573925009,\n",
       " 26.782906346427744,\n",
       " 191.38330334453087,\n",
       " 180.70825991211495,\n",
       " 187.49243491262348,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 190.4096787244055,\n",
       " 26.79682991991504,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 26.908193073262286,\n",
       " 26.782906346427744,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 164.38952542069475,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 174.92125813966973,\n",
       " 110.59774647665711,\n",
       " 26.869917088485956,\n",
       " 180.70825991211495,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 169.16328974732107,\n",
       " 189.43665132362818,\n",
       " 26.79682991991504,\n",
       " 26.814233393032794,\n",
       " 181.67536672505793,\n",
       " 183.61169292788176,\n",
       " 172.03844553550053,\n",
       " 179.74187497834234,\n",
       " 188.4642327804397,\n",
       " 26.782906346427744,\n",
       " 188.4642327804397,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 164.38952542069475,\n",
       " 111.45625151651896,\n",
       " 165.34238663820142,\n",
       " 174.92125813966973,\n",
       " 160.58804320837632,\n",
       " 26.869917088485956,\n",
       " 165.34238663820142,\n",
       " 191.38330334453087,\n",
       " 182.64318210388237,\n",
       " 172.03844553550053,\n",
       " 168.20667799635712,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 177.8113249737905,\n",
       " 173.95949504068867,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 159.6402501217417,\n",
       " 190.4096787244055,\n",
       " 176.84718750657188,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 159.6402501217417,\n",
       " 174.92125813966973,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 165.34238663820142,\n",
       " 191.38330334453087,\n",
       " 172.99855253624364,\n",
       " 171.07918915374282,\n",
       " 173.95949504068867,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 181.67536672505793,\n",
       " 179.74187497834234,\n",
       " 111.45625151651896,\n",
       " 188.4642327804397,\n",
       " 176.84718750657188,\n",
       " 169.16328974732107,\n",
       " 110.59774647665711,\n",
       " 164.38952542069475,\n",
       " 187.49243491262348,\n",
       " 170.12079871377787,\n",
       " 164.38952542069475,\n",
       " 187.49243491262348,\n",
       " 184.58088626734454,\n",
       " 26.782906346427744,\n",
       " 172.99855253624364,\n",
       " 168.20667799635712,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 172.03844553550053,\n",
       " 26.838596400639656,\n",
       " 111.45625151651896,\n",
       " 177.8113249737905,\n",
       " 189.43665132362818,\n",
       " 26.814233393032794,\n",
       " 183.61169292788176,\n",
       " 175.88382712870967,\n",
       " 173.95949504068867,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 26.838596400639656,\n",
       " 166.29621016818717,\n",
       " 191.38330334453087,\n",
       " 26.838596400639656,\n",
       " 172.99855253624364,\n",
       " 179.74187497834234,\n",
       " 177.8113249737905,\n",
       " 164.38952542069475,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 185.55074938195384,\n",
       " 26.838596400639656,\n",
       " 161.53688486242672,\n",
       " 186.52126971917383,\n",
       " 162.4867573925009,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 184.58088626734454,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 166.29621016818717,\n",
       " 177.8113249737905,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 176.84718750657188,\n",
       " 189.43665132362818,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 175.88382712870967,\n",
       " 166.29621016818717,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 181.67536672505793,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 172.03844553550053,\n",
       " 26.782906346427744,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 152.0978506875872,\n",
       " 189.43665132362818,\n",
       " 175.88382712870967,\n",
       " 182.64318210388237,\n",
       " 26.786387306053033,\n",
       " 178.77622543118278,\n",
       " 176.84718750657188,\n",
       " 159.6402501217417,\n",
       " 26.786387306053033,\n",
       " 190.4096787244055,\n",
       " 180.70825991211495,\n",
       " 188.4642327804397,\n",
       " 177.8113249737905,\n",
       " 176.84718750657188,\n",
       " 191.38330334453087,\n",
       " 180.70825991211495,\n",
       " 165.34238663820142,\n",
       " 183.61169292788176,\n",
       " 180.70825991211495,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 179.74187497834234,\n",
       " 170.12079871377787,\n",
       " 182.64318210388237,\n",
       " 26.79682991991504,\n",
       " 170.12079871377787,\n",
       " 176.84718750657188,\n",
       " 178.77622543118278,\n",
       " 163.43764332853803,\n",
       " 174.92125813966973,\n",
       " 172.99855253624364,\n",
       " 111.45625151651896,\n",
       " 174.92125813966973,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 184.58088626734454,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 165.34238663820142,\n",
       " 184.58088626734454,\n",
       " 163.43764332853803,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 26.814233393032794,\n",
       " 184.58088626734454,\n",
       " 169.16328974732107,\n",
       " 166.29621016818717,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 179.74187497834234,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 168.20667799635712,\n",
       " 26.79682991991504,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 160.58804320837632,\n",
       " 183.61169292788176,\n",
       " 154.91764802519648,\n",
       " 172.99855253624364,\n",
       " 172.03844553550053,\n",
       " 155.85992508343276,\n",
       " 178.77622543118278,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 184.58088626734454,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 185.55074938195384,\n",
       " 164.38952542069475,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 26.786387306053033,\n",
       " 26.79682991991504,\n",
       " 184.58088626734454,\n",
       " 183.61169292788176,\n",
       " 174.92125813966973,\n",
       " 160.58804320837632,\n",
       " 27.203782079746166,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 182.64318210388237,\n",
       " 183.61169292788176,\n",
       " 160.58804320837632,\n",
       " 162.4867573925009,\n",
       " 191.38330334453087,\n",
       " 173.95949504068867,\n",
       " 159.6402501217417,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 189.43665132362818,\n",
       " 169.16328974732107,\n",
       " 160.58804320837632,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 156.80334252270154,\n",
       " 189.43665132362818,\n",
       " 172.03844553550053,\n",
       " 169.16328974732107,\n",
       " 179.74187497834234,\n",
       " 190.4096787244055,\n",
       " 181.67536672505793,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 186.52126971917383,\n",
       " 165.34238663820142,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 175.88382712870967,\n",
       " 184.58088626734454,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 169.16328974732107,\n",
       " 26.814233393032794,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 26.95342144314358,\n",
       " 110.59774647665711,\n",
       " 27.064721052449833,\n",
       " 176.84718750657188,\n",
       " 155.85992508343276,\n",
       " 191.38330334453087,\n",
       " 185.55074938195384,\n",
       " 173.95949504068867,\n",
       " 179.74187497834234,\n",
       " 153.97653038890778,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 178.77622543118278,\n",
       " 26.79682991991504,\n",
       " 26.786387306053033,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 26.79682991991504,\n",
       " 186.52126971917383,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 190.4096787244055,\n",
       " 181.67536672505793,\n",
       " 186.52126971917383,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 182.64318210388237,\n",
       " 172.99855253624364,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 168.20667799635712,\n",
       " 167.25097941447643,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 26.814233393032794,\n",
       " 185.55074938195384,\n",
       " 180.70825991211495,\n",
       " 190.4096787244055,\n",
       " 186.52126971917383,\n",
       " 180.70825991211495,\n",
       " 162.4867573925009,\n",
       " 169.16328974732107,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 176.84718750657188,\n",
       " 171.07918915374282,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 26.786387306053033,\n",
       " 185.55074938195384,\n",
       " 26.782906346427744,\n",
       " 186.52126971917383,\n",
       " 183.61169292788176,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 185.55074938195384,\n",
       " 178.77622543118278,\n",
       " 164.38952542069475,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 165.34238663820142,\n",
       " 110.59774647665711,\n",
       " 163.43764332853803,\n",
       " 158.69352351632753,\n",
       " 172.03844553550053,\n",
       " 171.07918915374282,\n",
       " 176.84718750657188,\n",
       " 191.38330334453087,\n",
       " 168.20667799635712,\n",
       " 26.782906346427744,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 183.61169292788176,\n",
       " 168.20667799635712,\n",
       " 173.95949504068867,\n",
       " 188.4642327804397,\n",
       " 163.43764332853803,\n",
       " 180.70825991211495,\n",
       " 26.786387306053033,\n",
       " 183.61169292788176,\n",
       " 188.4642327804397,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 181.67536672505793,\n",
       " 172.03844553550053,\n",
       " 185.55074938195384,\n",
       " 175.88382712870967,\n",
       " 184.58088626734454,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 164.38952542069475,\n",
       " 26.814233393032794,\n",
       " 175.88382712870967,\n",
       " 176.84718750657188,\n",
       " 186.52126971917383,\n",
       " 188.4642327804397,\n",
       " 165.34238663820142,\n",
       " 187.49243491262348,\n",
       " 187.49243491262348,\n",
       " 179.74187497834234,\n",
       " 189.43665132362818,\n",
       " 177.8113249737905,\n",
       " 172.03844553550053,\n",
       " 179.74187497834234,\n",
       " 26.79682991991504,\n",
       " 171.07918915374282,\n",
       " 178.77622543118278,\n",
       " 169.16328974732107,\n",
       " 187.49243491262348,\n",
       " 182.64318210388237,\n",
       " 191.38330334453087,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 171.07918915374282,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 189.43665132362818,\n",
       " 168.20667799635712,\n",
       " 26.782906346427744,\n",
       " 183.61169292788176,\n",
       " 190.4096787244055,\n",
       " 111.45625151651896,\n",
       " 172.99855253624364,\n",
       " 172.99855253624364,\n",
       " 26.95342144314358,\n",
       " 26.782906346427744,\n",
       " 182.64318210388237,\n",
       " 181.67536672505793,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88, 88, 88, ..., 88, 88, 88], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90\n",
       "2       85\n",
       "4       79\n",
       "5       92\n",
       "6       82\n",
       "        ..\n",
       "8186    85\n",
       "8188    88\n",
       "8189    92\n",
       "8190    96\n",
       "8191    80\n",
       "Name: usr, Length: 6553, dtype: uint8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "361055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16714, 10)\n",
      "361060\n",
      "(38474, 7)\n",
      "361061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566602, 10)\n",
      "361062\n",
      "(10082, 26)\n",
      "361063\n",
      "(13488, 16)\n",
      "361065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13376, 10)\n",
      "361066\n",
      "(10578, 7)\n",
      "361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72998, 50)\n",
      "361069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940160, 24)\n",
      "361070\n",
      "(7608, 20)\n",
      "361273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71090, 7)\n",
      "361274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57580, 54)\n",
      "361275\n",
      "(13272, 20)\n",
      "361276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3434, 419)\n",
      "361277\n",
      "(20634, 8)\n",
      "361278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 22)\n"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 336 # Regression on numerical features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 337 # Classification on numerical features\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 335 # Regression on numerical and categorical features\u001b[39;00m\n\u001b[0;32m      4\u001b[0m SUITE_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m334\u001b[39m \u001b[38;5;66;03m# Classification on numerical and categorical features\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m benchmark_suite \u001b[38;5;241m=\u001b[39m \u001b[43mopenml\u001b[49m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mget_suite(SUITE_ID)  \u001b[38;5;66;03m# obtain the benchmark suite\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(benchmark_suite\u001b[38;5;241m.\u001b[39mtasks))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m benchmark_suite\u001b[38;5;241m.\u001b[39mtasks:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openml' is not defined"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)\n",
    "    print(X.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361072,\n",
       " 361073,\n",
       " 361074,\n",
       " 361076,\n",
       " 361077,\n",
       " 361078,\n",
       " 361079,\n",
       " 361080,\n",
       " 361081,\n",
       " 361082,\n",
       " 361083,\n",
       " 361084,\n",
       " 361085,\n",
       " 361086,\n",
       " 361087,\n",
       " 361088,\n",
       " 361279,\n",
       " 361280,\n",
       " 361281]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_suite.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361074\n",
      "Task 361074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "task_id=361074\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climbRate</th>\n",
       "      <th>Sgz</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>curRoll</th>\n",
       "      <th>absRoll</th>\n",
       "      <th>diffClb</th>\n",
       "      <th>diffRollRate</th>\n",
       "      <th>diffDiffClb</th>\n",
       "      <th>SaTime1</th>\n",
       "      <th>SaTime2</th>\n",
       "      <th>SaTime3</th>\n",
       "      <th>SaTime4</th>\n",
       "      <th>diffSaTime1</th>\n",
       "      <th>diffSaTime3</th>\n",
       "      <th>Sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-358.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-411.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16594</th>\n",
       "      <td>299.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16595</th>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>-208.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>-146.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>282.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16599 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       climbRate   Sgz     p     q  curRoll  absRoll  diffClb  diffRollRate  \\\n",
       "0          118.0 -55.0 -0.28 -0.08     -0.2    -11.0     11.0         0.005   \n",
       "1          390.0 -45.0 -0.06 -0.07     -0.6    -12.0     11.0         0.010   \n",
       "2           68.0   6.0  0.11  0.15      0.6    -10.0     -9.0        -0.003   \n",
       "3         -358.0 -12.0 -0.20  0.13     -0.3    -11.0     -7.0         0.001   \n",
       "4         -411.0 -19.0 -0.18  0.02     -0.5    -11.0     -3.0         0.002   \n",
       "...          ...   ...   ...   ...      ...      ...      ...           ...   \n",
       "16594      299.0 -28.0  0.08 -0.12     -0.3     -9.0     15.0         0.010   \n",
       "16595       84.0   0.0  0.14  0.14      1.1     -8.0    -11.0        -0.014   \n",
       "16596     -208.0  -6.0 -0.48  0.09      0.2     -9.0     -7.0        -0.010   \n",
       "16597     -146.0 -14.0 -0.38 -0.03     -0.8    -10.0     10.0         0.010   \n",
       "16598      282.0 -11.0  0.10 -0.12     -1.2    -10.0     16.0         0.016   \n",
       "\n",
       "       diffDiffClb  SaTime1  SaTime2  SaTime3  SaTime4  diffSaTime1  \\\n",
       "0             -0.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "1             -0.2  -0.0008  -0.0008  -0.0008  -0.0008       0.0000   \n",
       "2             -0.2  -0.0011  -0.0010  -0.0010  -0.0010      -0.0002   \n",
       "3             -0.1  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "4              1.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "...            ...      ...      ...      ...      ...          ...   \n",
       "16594         -0.2  -0.0005  -0.0005  -0.0005  -0.0005       0.0000   \n",
       "16595         -0.6  -0.0009  -0.0009  -0.0009  -0.0009       0.0000   \n",
       "16596         -0.1  -0.0009  -0.0009  -0.0009  -0.0009       0.0000   \n",
       "16597         -1.0  -0.0005  -0.0005  -0.0005  -0.0005       0.0000   \n",
       "16598         -0.1  -0.0004  -0.0004  -0.0004  -0.0004       0.0000   \n",
       "\n",
       "       diffSaTime3      Sa  \n",
       "0              0.0 -0.0010  \n",
       "1              0.0 -0.0008  \n",
       "2              0.0 -0.0010  \n",
       "3              0.0 -0.0010  \n",
       "4              0.0 -0.0010  \n",
       "...            ...     ...  \n",
       "16594          0.0 -0.0005  \n",
       "16595          0.0 -0.0009  \n",
       "16596          0.0 -0.0009  \n",
       "16597          0.0 -0.0005  \n",
       "16598          0.0 -0.0004  \n",
       "\n",
       "[16599 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    if i==2:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
