{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "#import utils\n",
    "#import utils_no_early_stopping \n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=300\n",
    "GP_ITERATIONS=1000\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 18:33:42,029] A new study created in memory with name: no-name-458c4038-7304-46bd-b33f-59a8e782b101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538fd3c72d3042a4af821505807a7816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 18:33:44,563] Trial 0 finished with value: 278.87677001953125 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 203}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db58dd4730d4a66bd7c4a17036a973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 18:33:50,465] Trial 1 finished with value: 10187.9775390625 and parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08, 'n_epochs': 300}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370b6611d6f7431f95e83906368ae09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MLP:  tensor(101.5319, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### MLP\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False)\n",
    "    n_epochs=train(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train__tensor, y_train__tensor, X_val_tensor, y_val_tensor, early_stopping)\n",
    "    \n",
    "    #n_epochs = early_stopping.n_epochs\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_MLP = (MLP_model(X_val_tensor).reshape(-1,))\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_MLP = (MLP_model(X_test_tensor).reshape(-1,))\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP\n",
    "\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=5\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "print(X_train__tensor.device)\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:21:56,922] A new study created in memory with name: no-name-98442c6d-1dd1-4db1-af39-1243d1cff0be\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8887defb6254a6aa00d5da6d6b3b101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:22:00,294] Trial 0 finished with value: 278.87677001953125 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 203}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f913252331e43a0bc40e9606c652cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:22:09,992] Trial 1 finished with value: 4307.978515625 and parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08, 'n_epochs': 576}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2370a5630a54f309c96a8907578d39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:22:20,617] Trial 2 finished with value: 605.7820434570312 and parameters: {'n_blocks': 4, 'd_block': 478, 'dropout': 0.003948266327914451, 'learning_rate': 0.002412079153798176, 'weight_decay': 0.00011563912803570738, 'n_epochs': 48}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14945bf1e6bc4b3eb1b571fc00343bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:22:55,959] Trial 3 finished with value: 612.689208984375 and parameters: {'n_blocks': 4, 'd_block': 364, 'dropout': 0.29187606817063316, 'learning_rate': 0.029994721053560828, 'weight_decay': 3.7400629930578146e-05, 'n_epochs': 225}. Best is trial 0 with value: 278.87677001953125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5148a7f74245e0a2321b7724e8e5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:22:58,143] Trial 4 finished with value: 79.98308563232422 and parameters: {'n_blocks': 3, 'd_block': 79, 'dropout': 0.3733407600514692, 'learning_rate': 0.006598821883612051, 'weight_decay': 1.618698156523955e-06, 'n_epochs': 108}. Best is trial 4 with value: 79.98308563232422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93406ffc7a82481b9a5db5d4c2e9c872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MLP:  tensor(101.8235, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### MLP\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False)\n",
    "    n_epochs=train(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train__tensor, y_train__tensor, X_val_tensor, y_val_tensor, early_stopping)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_MLP = (MLP_model(X_val_tensor).reshape(-1,))\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_MLP = (MLP_model(X_test_tensor).reshape(-1,))\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2(model, criterion, loss_list, optimizer, n_epochs, train_loader, X_val_tensor, y_val_tensor, early_stopping):\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Move batch to device\n",
    "            if torch.cuda.is_available():\n",
    "                batch_X = batch_X.cuda()\n",
    "                batch_y = batch_y.cuda()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X).reshape(-1,)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validate the model \n",
    "        y_val_hat = model(X_val_tensor).reshape(-1,)\n",
    "        val_loss = criterion(y_val_hat, y_val_tensor)\n",
    "\n",
    "        # check if early stopping condition is met\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return n_epochs\n",
    "\n",
    "# Similar changes should be made to the train_no_early_stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:31:12,712] A new study created in memory with name: no-name-cb6c6a86-b0a0-402c-8033-73bcc5096db9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:32:17,307] Trial 0 finished with value: 25.29632568359375 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 1000}. Best is trial 0 with value: 25.29632568359375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:34:20,909] Trial 1 finished with value: 87.48767852783203 and parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08, 'n_epochs': 1000}. Best is trial 0 with value: 25.29632568359375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:39:16,342] Trial 2 finished with value: 487.8714294433594 and parameters: {'n_blocks': 4, 'd_block': 478, 'dropout': 0.003948266327914451, 'learning_rate': 0.002412079153798176, 'weight_decay': 0.00011563912803570738, 'n_epochs': 1000}. Best is trial 0 with value: 25.29632568359375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:44:57,998] Trial 3 finished with value: 24.827110290527344 and parameters: {'n_blocks': 4, 'd_block': 364, 'dropout': 0.29187606817063316, 'learning_rate': 0.029994721053560828, 'weight_decay': 3.7400629930578146e-05, 'n_epochs': 1000}. Best is trial 3 with value: 24.827110290527344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:47:26,837] Trial 4 finished with value: 26.08611297607422 and parameters: {'n_blocks': 3, 'd_block': 79, 'dropout': 0.3733407600514692, 'learning_rate': 0.006598821883612051, 'weight_decay': 1.618698156523955e-06, 'n_epochs': 1000}. Best is trial 3 with value: 24.827110290527344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b216de69d41a4e0d9f75fe01e375bfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MLP:  tensor(72.0846, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### MLP\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # Define your batch size\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create TensorDatasets for training and validation sets\n",
    "    train_dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False)\n",
    "    n_epochs=train_2(MLP_model, criterion, loss_Adam, optimizer, n_epochs, train_loader, X_val_tensor, y_val_tensor, early_stopping)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_MLP = (MLP_model(X_val_tensor).reshape(-1,))\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_MLP = (MLP_model(X_test_tensor).reshape(-1,))\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:57:34,317] A new study created in memory with name: no-name-47a5fff5-4807-402c-be8d-6e3fa1743a6d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 19:58:59,672] Trial 0 finished with value: 25.075040817260742 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 1000}. Best is trial 0 with value: 25.075040817260742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 20:00:01,544] Trial 1 finished with value: 284.2603454589844 and parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08, 'n_epochs': 1000}. Best is trial 0 with value: 25.075040817260742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 39 out of 40\n",
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 20:00:44,124] Trial 2 finished with value: 25.508087158203125 and parameters: {'n_blocks': 4, 'd_block': 478, 'dropout': 0.003948266327914451, 'learning_rate': 0.002412079153798176, 'weight_decay': 0.00011563912803570738, 'n_epochs': 1000}. Best is trial 0 with value: 25.075040817260742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 20:01:15,271] Trial 3 finished with value: 263.349853515625 and parameters: {'n_blocks': 4, 'd_block': 364, 'dropout': 0.29187606817063316, 'learning_rate': 0.029994721053560828, 'weight_decay': 3.7400629930578146e-05, 'n_epochs': 1000}. Best is trial 0 with value: 25.075040817260742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 1 out of 40\n",
      "EarlyStopping counter: 2 out of 40\n",
      "EarlyStopping counter: 3 out of 40\n",
      "EarlyStopping counter: 4 out of 40\n",
      "EarlyStopping counter: 5 out of 40\n",
      "EarlyStopping counter: 6 out of 40\n",
      "EarlyStopping counter: 7 out of 40\n",
      "EarlyStopping counter: 8 out of 40\n",
      "EarlyStopping counter: 9 out of 40\n",
      "EarlyStopping counter: 10 out of 40\n",
      "EarlyStopping counter: 11 out of 40\n",
      "EarlyStopping counter: 12 out of 40\n",
      "EarlyStopping counter: 13 out of 40\n",
      "EarlyStopping counter: 14 out of 40\n",
      "EarlyStopping counter: 15 out of 40\n",
      "EarlyStopping counter: 16 out of 40\n",
      "EarlyStopping counter: 17 out of 40\n",
      "EarlyStopping counter: 18 out of 40\n",
      "EarlyStopping counter: 19 out of 40\n",
      "EarlyStopping counter: 20 out of 40\n",
      "EarlyStopping counter: 21 out of 40\n",
      "EarlyStopping counter: 22 out of 40\n",
      "EarlyStopping counter: 23 out of 40\n",
      "EarlyStopping counter: 24 out of 40\n",
      "EarlyStopping counter: 25 out of 40\n",
      "EarlyStopping counter: 26 out of 40\n",
      "EarlyStopping counter: 27 out of 40\n",
      "EarlyStopping counter: 28 out of 40\n",
      "EarlyStopping counter: 29 out of 40\n",
      "EarlyStopping counter: 30 out of 40\n",
      "EarlyStopping counter: 31 out of 40\n",
      "EarlyStopping counter: 32 out of 40\n",
      "EarlyStopping counter: 33 out of 40\n",
      "EarlyStopping counter: 34 out of 40\n",
      "EarlyStopping counter: 35 out of 40\n",
      "EarlyStopping counter: 36 out of 40\n",
      "EarlyStopping counter: 37 out of 40\n",
      "EarlyStopping counter: 38 out of 40\n",
      "EarlyStopping counter: 39 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-19 20:02:39,800] Trial 4 finished with value: 27.705896377563477 and parameters: {'n_blocks': 3, 'd_block': 79, 'dropout': 0.3733407600514692, 'learning_rate': 0.006598821883612051, 'weight_decay': 1.618698156523955e-06, 'n_epochs': 1000}. Best is trial 0 with value: 25.075040817260742.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 40 out of 40\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3583f76828841c6ae05b50c6dc6fade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MLP:  tensor(82.2031, grad_fn=<SqrtBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### MLP\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "\n",
    "    # Define your batch size\n",
    "    batch_size = 256\n",
    "\n",
    "    # Create TensorDatasets for training and validation sets\n",
    "    train_dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False)\n",
    "    n_epochs=train_2(MLP_model, criterion, loss_Adam, optimizer, n_epochs, train_loader, X_val_tensor, y_val_tensor, early_stopping)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_MLP = (MLP_model(X_val_tensor).reshape(-1,))\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, loss_Adam, optimizer, n_epochs, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_MLP = (MLP_model(X_test_tensor).reshape(-1,))\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: rpy2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Version: 3.5.15\n",
      "Summary: Python interface to the R language (embedded R)\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Laurent Gautier <lgautier@gmail.com>\n",
      "License: GPLv2+\n",
      "Location: c:\\users\\dalma\\desktop\\thesis_eth_new\\code\\.venv\\lib\\site-packages\n",
      "Requires: cffi, jinja2, packaging, tzlocal\n",
      "Required-by: drf\n"
     ]
    }
   ],
   "source": [
    "pip show rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361072,\n",
       " 361073,\n",
       " 361074,\n",
       " 361076,\n",
       " 361077,\n",
       " 361078,\n",
       " 361079,\n",
       " 361080,\n",
       " 361081,\n",
       " 361082,\n",
       " 361083,\n",
       " 361084,\n",
       " 361085,\n",
       " 361086,\n",
       " 361087,\n",
       " 361088,\n",
       " 361279,\n",
       " 361280,\n",
       " 361281]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "benchmark_suite.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361078,\n",
       " 361079,\n",
       " 361080,\n",
       " 361081,\n",
       " 361082,\n",
       " 361083,\n",
       " 361084,\n",
       " 361085,\n",
       " 361086,\n",
       " 361087,\n",
       " 361088,\n",
       " 361279,\n",
       " 361280,\n",
       " 361281]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_suite.tasks[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(cov)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     52\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     53\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     52\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     53\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "task_id=361074\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = X.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of X\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features\n",
    "X = X.drop(high_corr_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climbRate</th>\n",
       "      <th>Sgz</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>curRoll</th>\n",
       "      <th>absRoll</th>\n",
       "      <th>diffClb</th>\n",
       "      <th>diffRollRate</th>\n",
       "      <th>diffDiffClb</th>\n",
       "      <th>SaTime1</th>\n",
       "      <th>diffSaTime1</th>\n",
       "      <th>diffSaTime3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-358.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-411.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16594</th>\n",
       "      <td>299.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16595</th>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>-208.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>-146.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>282.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16599 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       climbRate   Sgz     p     q  curRoll  absRoll  diffClb  diffRollRate  \\\n",
       "0          118.0 -55.0 -0.28 -0.08     -0.2    -11.0     11.0         0.005   \n",
       "1          390.0 -45.0 -0.06 -0.07     -0.6    -12.0     11.0         0.010   \n",
       "2           68.0   6.0  0.11  0.15      0.6    -10.0     -9.0        -0.003   \n",
       "3         -358.0 -12.0 -0.20  0.13     -0.3    -11.0     -7.0         0.001   \n",
       "4         -411.0 -19.0 -0.18  0.02     -0.5    -11.0     -3.0         0.002   \n",
       "...          ...   ...   ...   ...      ...      ...      ...           ...   \n",
       "16594      299.0 -28.0  0.08 -0.12     -0.3     -9.0     15.0         0.010   \n",
       "16595       84.0   0.0  0.14  0.14      1.1     -8.0    -11.0        -0.014   \n",
       "16596     -208.0  -6.0 -0.48  0.09      0.2     -9.0     -7.0        -0.010   \n",
       "16597     -146.0 -14.0 -0.38 -0.03     -0.8    -10.0     10.0         0.010   \n",
       "16598      282.0 -11.0  0.10 -0.12     -1.2    -10.0     16.0         0.016   \n",
       "\n",
       "       diffDiffClb  SaTime1  diffSaTime1  diffSaTime3  \n",
       "0             -0.2  -0.0010       0.0000          0.0  \n",
       "1             -0.2  -0.0008       0.0000          0.0  \n",
       "2             -0.2  -0.0011      -0.0002          0.0  \n",
       "3             -0.1  -0.0010       0.0000          0.0  \n",
       "4              1.2  -0.0010       0.0000          0.0  \n",
       "...            ...      ...          ...          ...  \n",
       "16594         -0.2  -0.0005       0.0000          0.0  \n",
       "16595         -0.6  -0.0009       0.0000          0.0  \n",
       "16596         -0.1  -0.0009       0.0000          0.0  \n",
       "16597         -1.0  -0.0005       0.0000          0.0  \n",
       "16598         -0.1  -0.0004       0.0000          0.0  \n",
       "\n",
       "[16599 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openml\n",
      "Version: 0.14.1\n",
      "Summary: Python API for OpenML\n",
      "Home-page: https://openml.org/\n",
      "Author: Matthias Feurer, Jan van Rijn, Arlind Kadra, Pieter Gijsbers, Neeratyoy Mallik, Sahithya Ravi, Andreas Mller, Joaquin Vanschoren and Frank Hutter\n",
      "Author-email: feurerm@informatik.uni-freiburg.de\n",
      "License: BSD 3-clause\n",
      "Location: c:\\users\\dalma\\desktop\\thesis_eth_new\\code\\.venv\\lib\\site-packages\n",
      "Requires: liac-arff, minio, numpy, pandas, pyarrow, python-dateutil, requests, scikit-learn, scipy, xmltodict\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-27 22:22:51,032] A new study created in memory with name: no-name-9c1642ca-8d7b-4ea2-9e93-b88890d73d31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.2750,  E(|Y-Yhat|): 0.7905,  E(|Yhat-Yhat'|): 1.0311\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2249,  E(|Y-Yhat|): 0.4709,  E(|Yhat-Yhat'|): 0.4920\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1575,  E(|Y-Yhat|): 2.5551,  E(|Yhat-Yhat'|): 2.7952\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-27 22:24:35,829] Trial 0 finished with value: 9.43673324584961 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 9.43673324584961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6348,  E(|Y-Yhat|): 0.8215,  E(|Yhat-Yhat'|): 0.3733\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.1723,  E(|Y-Yhat|): 0.3907,  E(|Yhat-Yhat'|): 0.4366\n",
      "[Epoch 200 (25%), batch 6] energy-loss: 0.2508,  E(|Y-Yhat|): 0.4260,  E(|Yhat-Yhat'|): 0.3503\n",
      "[Epoch 300 (38%), batch 6] energy-loss: 0.2311,  E(|Y-Yhat|): 0.4403,  E(|Yhat-Yhat'|): 0.4185\n",
      "[Epoch 400 (51%), batch 6] energy-loss: 0.1729,  E(|Y-Yhat|): 0.3637,  E(|Yhat-Yhat'|): 0.3817\n",
      "[Epoch 500 (64%), batch 6] energy-loss: 0.2336,  E(|Y-Yhat|): 0.4323,  E(|Yhat-Yhat'|): 0.3974\n",
      "[Epoch 600 (76%), batch 6] energy-loss: 0.1737,  E(|Y-Yhat|): 0.3741,  E(|Yhat-Yhat'|): 0.4008\n",
      "[Epoch 700 (89%), batch 6] energy-loss: 0.1944,  E(|Y-Yhat|): 0.4111,  E(|Yhat-Yhat'|): 0.4332\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0915,  E(|Y-Yhat|): 2.1540,  E(|Yhat-Yhat'|): 2.1250\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-27 22:26:32,803] Trial 1 finished with value: 22.02121353149414 and parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False}. Best is trial 0 with value: 9.43673324584961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3846,  E(|Y-Yhat|): 1.0179,  E(|Yhat-Yhat'|): 1.2667\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1076,  E(|Y-Yhat|): 0.2529,  E(|Yhat-Yhat'|): 0.2906\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.5237,  E(|Y-Yhat|): 3.4340,  E(|Yhat-Yhat'|): 3.8205\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "RMSE engression tensor(10.0587)\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=2\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else: \n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression=engressor_model.predict(X_val_tensor, target=\"mean\")\n",
    "    RMSE_engression=torch.sqrt(torch.mean(torch.square(y_val_tensor.reshape(-1,1) - y_val_hat_engression)))\n",
    "\n",
    "    return RMSE_engression\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else: \n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "y_test_hat_engression=engressor_model.predict(X_test_tensor, target=\"mean\")\n",
    "RMSE_engression=torch.sqrt(torch.mean(torch.square(y_test_tensor.reshape(-1,1) - y_test_hat_engression)))\n",
    "\n",
    "print(\"RMSE engression\", RMSE_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361084\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/CLUSTERING', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/CLUSTERING/task_{task_id}.pt'\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features\n",
    "X = X.drop(high_corr_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515884</td>\n",
       "      <td>0.576671</td>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.356967</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.303093</td>\n",
       "      <td>0.154178</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.129473</td>\n",
       "      <td>0.391638</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.008031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.515884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754665</td>\n",
       "      <td>0.087740</td>\n",
       "      <td>0.664983</td>\n",
       "      <td>0.685342</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>0.024573</td>\n",
       "      <td>0.223042</td>\n",
       "      <td>0.568634</td>\n",
       "      <td>0.087175</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.005304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_living</th>\n",
       "      <td>0.576671</td>\n",
       "      <td>0.754665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172826</td>\n",
       "      <td>0.762704</td>\n",
       "      <td>0.876597</td>\n",
       "      <td>0.435043</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.052529</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.756420</td>\n",
       "      <td>0.183286</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.007246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_lot</th>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.087740</td>\n",
       "      <td>0.172826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>0.183512</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>0.229521</td>\n",
       "      <td>0.144608</td>\n",
       "      <td>0.718557</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.000634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <td>0.356967</td>\n",
       "      <td>0.664983</td>\n",
       "      <td>0.762704</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755923</td>\n",
       "      <td>0.168392</td>\n",
       "      <td>0.446963</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.114084</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>0.713202</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.012483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_above</th>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.685342</td>\n",
       "      <td>0.876597</td>\n",
       "      <td>0.183512</td>\n",
       "      <td>0.755923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051943</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.343803</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.194050</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_basement</th>\n",
       "      <td>0.303093</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.435043</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.168392</td>\n",
       "      <td>0.051943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.110538</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.010405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr_built</th>\n",
       "      <td>0.154178</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.446963</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.224874</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.409356</td>\n",
       "      <td>0.326229</td>\n",
       "      <td>0.070958</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.005766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr_renovated</th>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.224874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.008127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat</th>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.024573</td>\n",
       "      <td>0.052529</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>0.114084</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.110538</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135512</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.015655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.129473</td>\n",
       "      <td>0.223042</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.229521</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>0.343803</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>0.409356</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.135512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334605</td>\n",
       "      <td>0.254451</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.007331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_living15</th>\n",
       "      <td>0.391638</td>\n",
       "      <td>0.568634</td>\n",
       "      <td>0.756420</td>\n",
       "      <td>0.144608</td>\n",
       "      <td>0.713202</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>0.326229</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.334605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183192</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.008539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_lot15</th>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.087175</td>\n",
       "      <td>0.183286</td>\n",
       "      <td>0.718557</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>0.194050</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.070958</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.254451</td>\n",
       "      <td>0.183192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.002871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_month</th>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_day</th>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.060664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bedrooms  bathrooms  sqft_living  sqft_lot     grade  \\\n",
       "bedrooms       1.000000   0.515884     0.576671  0.031703  0.356967   \n",
       "bathrooms      0.515884   1.000000     0.754665  0.087740  0.664983   \n",
       "sqft_living    0.576671   0.754665     1.000000  0.172826  0.762704   \n",
       "sqft_lot       0.031703   0.087740     0.172826  1.000000  0.113621   \n",
       "grade          0.356967   0.664983     0.762704  0.113621  1.000000   \n",
       "sqft_above     0.477600   0.685342     0.876597  0.183512  0.755923   \n",
       "sqft_basement  0.303093   0.283770     0.435043  0.015286  0.168392   \n",
       "yr_built       0.154178   0.506019     0.318049  0.053080  0.446963   \n",
       "yr_renovated   0.018841   0.050739     0.055363  0.007644  0.014414   \n",
       "lat            0.008931   0.024573     0.052529  0.085683  0.114084   \n",
       "long           0.129473   0.223042     0.240223  0.229521  0.198372   \n",
       "sqft_living15  0.391638   0.568634     0.756420  0.144608  0.713202   \n",
       "sqft_lot15     0.029244   0.087175     0.183286  0.718557  0.119248   \n",
       "date_month     0.001533   0.007392     0.011810  0.002369  0.008376   \n",
       "date_day       0.008031   0.005304     0.007246  0.000634  0.012483   \n",
       "\n",
       "               sqft_above  sqft_basement  yr_built  yr_renovated       lat  \\\n",
       "bedrooms         0.477600       0.303093  0.154178      0.018841  0.008931   \n",
       "bathrooms        0.685342       0.283770  0.506019      0.050739  0.024573   \n",
       "sqft_living      0.876597       0.435043  0.318049      0.055363  0.052529   \n",
       "sqft_lot         0.183512       0.015286  0.053080      0.007644  0.085683   \n",
       "grade            0.755923       0.168392  0.446963      0.014414  0.114084   \n",
       "sqft_above       1.000000       0.051943  0.423898      0.023285  0.000816   \n",
       "sqft_basement    0.051943       1.000000  0.133124      0.071323  0.110538   \n",
       "yr_built         0.423898       0.133124  1.000000      0.224874  0.148122   \n",
       "yr_renovated     0.023285       0.071323  0.224874      1.000000  0.029398   \n",
       "lat              0.000816       0.110538  0.148122      0.029398  1.000000   \n",
       "long             0.343803       0.144765  0.409356      0.068372  0.135512   \n",
       "sqft_living15    0.731870       0.200355  0.326229      0.002673  0.048858   \n",
       "sqft_lot15       0.194050       0.017276  0.070958      0.007854  0.086419   \n",
       "date_month       0.009872       0.006035  0.006226      0.012827  0.014961   \n",
       "date_day         0.002475       0.010405  0.005766      0.008127  0.015655   \n",
       "\n",
       "                   long  sqft_living15  sqft_lot15  date_month  date_day  \n",
       "bedrooms       0.129473       0.391638    0.029244    0.001533  0.008031  \n",
       "bathrooms      0.223042       0.568634    0.087175    0.007392  0.005304  \n",
       "sqft_living    0.240223       0.756420    0.183286    0.011810  0.007246  \n",
       "sqft_lot       0.229521       0.144608    0.718557    0.002369  0.000634  \n",
       "grade          0.198372       0.713202    0.119248    0.008376  0.012483  \n",
       "sqft_above     0.343803       0.731870    0.194050    0.009872  0.002475  \n",
       "sqft_basement  0.144765       0.200355    0.017276    0.006035  0.010405  \n",
       "yr_built       0.409356       0.326229    0.070958    0.006226  0.005766  \n",
       "yr_renovated   0.068372       0.002673    0.007854    0.012827  0.008127  \n",
       "lat            0.135512       0.048858    0.086419    0.014961  0.015655  \n",
       "long           1.000000       0.334605    0.254451    0.008134  0.007331  \n",
       "sqft_living15  0.334605       1.000000    0.183192    0.002449  0.008539  \n",
       "sqft_lot15     0.254451       0.183192    1.000000    0.003546  0.002871  \n",
       "date_month     0.008134       0.002449    0.003546    1.000000  0.060664  \n",
       "date_day       0.007331       0.008539    0.002871    0.060664  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515884</td>\n",
       "      <td>0.576671</td>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.356967</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.303093</td>\n",
       "      <td>0.154178</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.129473</td>\n",
       "      <td>0.391638</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.008031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.515884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754665</td>\n",
       "      <td>0.087740</td>\n",
       "      <td>0.664983</td>\n",
       "      <td>0.685342</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>0.024573</td>\n",
       "      <td>0.223042</td>\n",
       "      <td>0.568634</td>\n",
       "      <td>0.087175</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.005304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_living</th>\n",
       "      <td>0.576671</td>\n",
       "      <td>0.754665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172826</td>\n",
       "      <td>0.762704</td>\n",
       "      <td>0.876597</td>\n",
       "      <td>0.435043</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.052529</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.756420</td>\n",
       "      <td>0.183286</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.007246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_lot</th>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.087740</td>\n",
       "      <td>0.172826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>0.183512</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>0.229521</td>\n",
       "      <td>0.144608</td>\n",
       "      <td>0.718557</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.000634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <td>0.356967</td>\n",
       "      <td>0.664983</td>\n",
       "      <td>0.762704</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755923</td>\n",
       "      <td>0.168392</td>\n",
       "      <td>0.446963</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.114084</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>0.713202</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.012483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_above</th>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.685342</td>\n",
       "      <td>0.876597</td>\n",
       "      <td>0.183512</td>\n",
       "      <td>0.755923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051943</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.343803</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.194050</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_basement</th>\n",
       "      <td>0.303093</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.435043</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.168392</td>\n",
       "      <td>0.051943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.110538</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.010405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr_built</th>\n",
       "      <td>0.154178</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.446963</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.133124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.224874</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.409356</td>\n",
       "      <td>0.326229</td>\n",
       "      <td>0.070958</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.005766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr_renovated</th>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.224874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.008127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat</th>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.024573</td>\n",
       "      <td>0.052529</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>0.114084</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.110538</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135512</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.015655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.129473</td>\n",
       "      <td>0.223042</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.229521</td>\n",
       "      <td>0.198372</td>\n",
       "      <td>0.343803</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>0.409356</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.135512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334605</td>\n",
       "      <td>0.254451</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.007331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_living15</th>\n",
       "      <td>0.391638</td>\n",
       "      <td>0.568634</td>\n",
       "      <td>0.756420</td>\n",
       "      <td>0.144608</td>\n",
       "      <td>0.713202</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>0.326229</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.334605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183192</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.008539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sqft_lot15</th>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.087175</td>\n",
       "      <td>0.183286</td>\n",
       "      <td>0.718557</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>0.194050</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.070958</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.254451</td>\n",
       "      <td>0.183192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.002871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_month</th>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_day</th>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.060664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bedrooms  bathrooms  sqft_living  sqft_lot     grade  \\\n",
       "bedrooms       1.000000   0.515884     0.576671  0.031703  0.356967   \n",
       "bathrooms      0.515884   1.000000     0.754665  0.087740  0.664983   \n",
       "sqft_living    0.576671   0.754665     1.000000  0.172826  0.762704   \n",
       "sqft_lot       0.031703   0.087740     0.172826  1.000000  0.113621   \n",
       "grade          0.356967   0.664983     0.762704  0.113621  1.000000   \n",
       "sqft_above     0.477600   0.685342     0.876597  0.183512  0.755923   \n",
       "sqft_basement  0.303093   0.283770     0.435043  0.015286  0.168392   \n",
       "yr_built       0.154178   0.506019     0.318049  0.053080  0.446963   \n",
       "yr_renovated   0.018841   0.050739     0.055363  0.007644  0.014414   \n",
       "lat            0.008931   0.024573     0.052529  0.085683  0.114084   \n",
       "long           0.129473   0.223042     0.240223  0.229521  0.198372   \n",
       "sqft_living15  0.391638   0.568634     0.756420  0.144608  0.713202   \n",
       "sqft_lot15     0.029244   0.087175     0.183286  0.718557  0.119248   \n",
       "date_month     0.001533   0.007392     0.011810  0.002369  0.008376   \n",
       "date_day       0.008031   0.005304     0.007246  0.000634  0.012483   \n",
       "\n",
       "               sqft_above  sqft_basement  yr_built  yr_renovated       lat  \\\n",
       "bedrooms         0.477600       0.303093  0.154178      0.018841  0.008931   \n",
       "bathrooms        0.685342       0.283770  0.506019      0.050739  0.024573   \n",
       "sqft_living      0.876597       0.435043  0.318049      0.055363  0.052529   \n",
       "sqft_lot         0.183512       0.015286  0.053080      0.007644  0.085683   \n",
       "grade            0.755923       0.168392  0.446963      0.014414  0.114084   \n",
       "sqft_above       1.000000       0.051943  0.423898      0.023285  0.000816   \n",
       "sqft_basement    0.051943       1.000000  0.133124      0.071323  0.110538   \n",
       "yr_built         0.423898       0.133124  1.000000      0.224874  0.148122   \n",
       "yr_renovated     0.023285       0.071323  0.224874      1.000000  0.029398   \n",
       "lat              0.000816       0.110538  0.148122      0.029398  1.000000   \n",
       "long             0.343803       0.144765  0.409356      0.068372  0.135512   \n",
       "sqft_living15    0.731870       0.200355  0.326229      0.002673  0.048858   \n",
       "sqft_lot15       0.194050       0.017276  0.070958      0.007854  0.086419   \n",
       "date_month       0.009872       0.006035  0.006226      0.012827  0.014961   \n",
       "date_day         0.002475       0.010405  0.005766      0.008127  0.015655   \n",
       "\n",
       "                   long  sqft_living15  sqft_lot15  date_month  date_day  \n",
       "bedrooms       0.129473       0.391638    0.029244    0.001533  0.008031  \n",
       "bathrooms      0.223042       0.568634    0.087175    0.007392  0.005304  \n",
       "sqft_living    0.240223       0.756420    0.183286    0.011810  0.007246  \n",
       "sqft_lot       0.229521       0.144608    0.718557    0.002369  0.000634  \n",
       "grade          0.198372       0.713202    0.119248    0.008376  0.012483  \n",
       "sqft_above     0.343803       0.731870    0.194050    0.009872  0.002475  \n",
       "sqft_basement  0.144765       0.200355    0.017276    0.006035  0.010405  \n",
       "yr_built       0.409356       0.326229    0.070958    0.006226  0.005766  \n",
       "yr_renovated   0.068372       0.002673    0.007854    0.012827  0.008127  \n",
       "lat            0.135512       0.048858    0.086419    0.014961  0.015655  \n",
       "long           1.000000       0.334605    0.254451    0.008134  0.007331  \n",
       "sqft_living15  0.334605       1.000000    0.183192    0.002449  0.008539  \n",
       "sqft_lot15     0.254451       0.183192    1.000000    0.003546  0.002871  \n",
       "date_month     0.008134       0.002449    0.003546    1.000000  0.060664  \n",
       "date_day       0.007331       0.008539    0.002871    0.060664  1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m     counts_\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(kmeans_\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m==\u001b[39mi))\n\u001b[0;32m     70\u001b[0m     mean_k_\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X_train\u001b[38;5;241m.\u001b[39mloc[kmeans_\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m==\u001b[39mi,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     mahalanobis_dist_\u001b[38;5;241m.\u001b[39mappend(mahalanobis(mean_k_, mean_, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     73\u001b[0m dist_df_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmahalanobis_dist\u001b[39m\u001b[38;5;124m'\u001b[39m: mahalanobis_dist_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m: counts_}, index\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(N_CLUSTERS))\n\u001b[0;32m     74\u001b[0m dist_df_\u001b[38;5;241m=\u001b[39mdist_df_\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmahalanobis_dist\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# New new implementation\n",
    "N_CLUSTERS=20\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_scaled)\n",
    "distances=[]\n",
    "mahalanobis_dist=[]\n",
    "counts=[]\n",
    "ideal_len=len(kmeans.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances.append(np.abs(np.sum(kmeans.labels_==i)-ideal_len))\n",
    "    counts.append(np.sum(kmeans.labels_==i))\n",
    "    mean_k= np.mean(X.loc[kmeans.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist.append(mahalanobis(mean_k, mean, np.linalg.inv(cov)))\n",
    "\n",
    "dist_df=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist, 'count': counts}, index=np.arange(N_CLUSTERS))\n",
    "dist_df=dist_df.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df['cumulative_count']=dist_df['count'].cumsum()\n",
    "dist_df['abs_diff']=np.abs(dist_df['cumulative_count']-ideal_len)\n",
    "\n",
    "final=(np.where(dist_df['abs_diff']==np.min(dist_df['abs_diff']))[0])[0]\n",
    "labelss=dist_df.index[0:final+1].to_list()\n",
    "labels=pd.Series(kmeans.labels_).isin(labelss)\n",
    "labels.index=X.index\n",
    "close_index=labels.index[np.where(labels==False)[0]]\n",
    "far_index=labels.index[np.where(labels==True)[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean_ = np.mean(X_train, axis=0)\n",
    "cov_ = np.cov(X_train.T)\n",
    "scaler_ = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_train_scaled = scaler_.fit_transform(X_train)\n",
    "\n",
    "kmeans_ = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_train_scaled)\n",
    "distances_=[]\n",
    "counts_=[]\n",
    "mahalanobis_dist_=[]\n",
    "ideal_len_=len(kmeans_.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances_.append(np.abs(np.sum(kmeans_.labels_==i)-ideal_len_))\n",
    "    counts_.append(np.sum(kmeans_.labels_==i))\n",
    "    mean_k_= np.mean(X_train.loc[kmeans_.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist_.append(mahalanobis(mean_k_, mean_, np.linalg.inv(cov_)))\n",
    "\n",
    "dist_df_=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist_, 'count': counts_}, index=np.arange(N_CLUSTERS))\n",
    "dist_df_=dist_df_.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df_['cumulative_count']=dist_df_['count'].cumsum()\n",
    "dist_df_['abs_diff']=np.abs(dist_df_['cumulative_count']-ideal_len_)\n",
    "\n",
    "final_=(np.where(dist_df_['abs_diff']==np.min(dist_df_['abs_diff']))[0])[0]\n",
    "labelss_=dist_df_.index[0:final_+1].to_list()\n",
    "labels_=pd.Series(kmeans_.labels_).isin(labelss_)\n",
    "labels_.index=X_train.index\n",
    "close_index_=labels_.index[np.where(labels_==False)[0]]\n",
    "far_index_=labels_.index[np.where(labels_==True)[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "    # Standardize the data\n",
    "mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "std_X_train_ = np.std(X_train_, axis=0)\n",
    "X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(np.isnan(X_train.corr().abs())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>5536.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>5731.0</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>6023.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>6023.0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>5813.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17372 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bedrooms  bathrooms  sqft_living  sqft_lot  grade  sqft_above  \\\n",
       "0             3       1.00       1180.0    5650.0      7      1180.0   \n",
       "2             2       1.00        770.0   10000.0      6       770.0   \n",
       "3             4       3.00       1960.0    5000.0      7      1050.0   \n",
       "4             3       2.00       1680.0    8080.0      8      1680.0   \n",
       "6             3       2.25       1715.0    6819.0      7      1715.0   \n",
       "...         ...        ...          ...       ...    ...         ...   \n",
       "21603         3       2.50       2270.0    5536.0      8      2270.0   \n",
       "21605         4       2.50       2520.0    6023.0      9      2520.0   \n",
       "21606         4       3.50       3510.0    7200.0      9      2600.0   \n",
       "21609         4       2.50       2310.0    5813.0      8      2310.0   \n",
       "21611         3       2.50       1600.0    2388.0      8      1600.0   \n",
       "\n",
       "       sqft_basement  yr_built  yr_renovated      lat     long  sqft_living15  \\\n",
       "0                0.0    1955.0           0.0  47.5112 -122.257         1340.0   \n",
       "2                0.0    1933.0           0.0  47.7379 -122.233         2720.0   \n",
       "3              910.0    1965.0           0.0  47.5208 -122.393         1360.0   \n",
       "4                0.0    1987.0           0.0  47.6168 -122.045         1800.0   \n",
       "6                0.0    1995.0           0.0  47.3097 -122.327         2238.0   \n",
       "...              ...       ...           ...      ...      ...            ...   \n",
       "21603            0.0    2003.0           0.0  47.5389 -121.881         2270.0   \n",
       "21605            0.0    2014.0           0.0  47.5137 -122.167         2520.0   \n",
       "21606          910.0    2009.0           0.0  47.5537 -122.398         2050.0   \n",
       "21609            0.0    2014.0           0.0  47.5107 -122.362         1830.0   \n",
       "21611            0.0    2004.0           0.0  47.5345 -122.069         1410.0   \n",
       "\n",
       "       sqft_lot15  date_month  date_day  \n",
       "0          5650.0          10        13  \n",
       "2          8062.0           2        25  \n",
       "3          5000.0          12         9  \n",
       "4          7503.0           2        18  \n",
       "6          6819.0           6        27  \n",
       "...           ...         ...       ...  \n",
       "21603      5731.0           8        25  \n",
       "21605      6023.0          10        14  \n",
       "21606      6200.0           3        26  \n",
       "21609      7200.0           2        23  \n",
       "21611      1287.0           1        16  \n",
       "\n",
       "[17372 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>7639.0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>8062.0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>5813.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bedrooms  bathrooms  sqft_living  sqft_lot  grade  sqft_above  \\\n",
       "0             3       1.00       1180.0    5650.0      7      1180.0   \n",
       "1             3       2.25       2570.0    7242.0      7      2170.0   \n",
       "2             2       1.00        770.0   10000.0      6       770.0   \n",
       "3             4       3.00       1960.0    5000.0      7      1050.0   \n",
       "4             3       2.00       1680.0    8080.0      8      1680.0   \n",
       "...         ...        ...          ...       ...    ...         ...   \n",
       "21608         3       2.50       1530.0    1131.0      8      1530.0   \n",
       "21609         4       2.50       2310.0    5813.0      8      2310.0   \n",
       "21610         2       0.75       1020.0    1350.0      7      1020.0   \n",
       "21611         3       2.50       1600.0    2388.0      8      1600.0   \n",
       "21612         2       0.75       1020.0    1076.0      7      1020.0   \n",
       "\n",
       "       sqft_basement  yr_built  yr_renovated      lat     long  sqft_living15  \\\n",
       "0                0.0    1955.0           0.0  47.5112 -122.257         1340.0   \n",
       "1              400.0    1951.0        1991.0  47.7210 -122.319         1690.0   \n",
       "2                0.0    1933.0           0.0  47.7379 -122.233         2720.0   \n",
       "3              910.0    1965.0           0.0  47.5208 -122.393         1360.0   \n",
       "4                0.0    1987.0           0.0  47.6168 -122.045         1800.0   \n",
       "...              ...       ...           ...      ...      ...            ...   \n",
       "21608            0.0    2009.0           0.0  47.6993 -122.346         1530.0   \n",
       "21609            0.0    2014.0           0.0  47.5107 -122.362         1830.0   \n",
       "21610            0.0    2009.0           0.0  47.5944 -122.299         1020.0   \n",
       "21611            0.0    2004.0           0.0  47.5345 -122.069         1410.0   \n",
       "21612            0.0    2008.0           0.0  47.5941 -122.299         1020.0   \n",
       "\n",
       "       sqft_lot15  date_month  date_day  \n",
       "0          5650.0          10        13  \n",
       "1          7639.0          12         9  \n",
       "2          8062.0           2        25  \n",
       "3          5000.0          12         9  \n",
       "4          7503.0           2        18  \n",
       "...           ...         ...       ...  \n",
       "21608      1509.0           5        21  \n",
       "21609      7200.0           2        23  \n",
       "21610      2007.0           6        23  \n",
       "21611      1287.0           1        16  \n",
       "21612      1357.0          10        15  \n",
       "\n",
       "[21613 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "#task_id=361072\n",
    "for task_id in benchmark_suite.tasks[10:]:\n",
    "\n",
    "    print(f\"Task {task_id}\")\n",
    "\n",
    "    # Create the checkpoint directory if it doesn't exist\n",
    "    os.makedirs('CHECKPOINTS/SPATIAL_DEPTH', exist_ok=True)\n",
    "    CHECKPOINT_PATH = f'CHECKPOINTS/SPATIAL_DEPTH/task_{task_id}.pt'\n",
    "\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    \n",
    "    if len(X)>=100000:\n",
    "        continue\n",
    "    \n",
    "    # Find features with absolute correlation > 0.9\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "    # Drop one of the highly correlated features\n",
    "    X = X.drop(high_corr_features, axis=1)\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    N_TRIALS=100\n",
    "    N_SAMPLES=100\n",
    "    PATIENCE=40\n",
    "    N_EPOCHS=1000\n",
    "    GP_ITERATIONS=1000\n",
    "    BATCH_SIZE=1024\n",
    "    seed=10\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "    # activate pandas conversion for rpy2\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    # import R's \"ddalpha\" package\n",
    "    ddalpha = importr('ddalpha')\n",
    "\n",
    "    # explicitly import the projDepth function\n",
    "    spatialDepth = robjects.r['depth.spatial']\n",
    "\n",
    "    # calculate the spatial depth for each data point\n",
    "    spatial_depth = spatialDepth(X, X)\n",
    "\n",
    "    spatial_depth=pd.Series(spatial_depth,index=X.index)\n",
    "    far_index=spatial_depth.index[np.where(spatial_depth<=np.quantile(spatial_depth,0.2))[0]]\n",
    "    close_index=spatial_depth.index[np.where(spatial_depth>np.quantile(spatial_depth,0.2))[0]]\n",
    "\n",
    "    X_train = X.loc[close_index,:]\n",
    "    X_test = X.loc[far_index,:]\n",
    "    y_train = y.loc[close_index]\n",
    "    y_test = y.loc[far_index]\n",
    "\n",
    "    # convert the R vector to a pandas Series\n",
    "    spatial_depth_ = spatialDepth(X_train, X_train)\n",
    "\n",
    "    spatial_depth_=pd.Series(spatial_depth_,index=X_train.index)\n",
    "    far_index_=spatial_depth_.index[np.where(spatial_depth_<=np.quantile(spatial_depth_,0.2))[0]]\n",
    "    close_index_=spatial_depth_.index[np.where(spatial_depth_>np.quantile(spatial_depth_,0.2))[0]]\n",
    "\n",
    "    X_train_ = X_train.loc[close_index_,:]\n",
    "    X_val = X_train.loc[far_index_,:]\n",
    "    y_train_ = y_train.loc[close_index_]\n",
    "    y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "        # Standardize the data\n",
    "    mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "    std_X_train_ = np.std(X_train_, axis=0)\n",
    "    X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "    X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "    X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "    X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "    y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Convert to use GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU\")\n",
    "        X_train__tensor = X_train__tensor.cuda()\n",
    "        y_train__tensor = y_train__tensor.cuda()\n",
    "        X_train_tensor = X_train_tensor.cuda()\n",
    "        y_train_tensor = y_train_tensor.cuda()\n",
    "        X_val_tensor = X_val_tensor.cuda()\n",
    "        y_val_tensor = y_val_tensor.cuda()\n",
    "        X_test_tensor = X_test_tensor.cuda()\n",
    "        y_test_tensor = y_test_tensor.cuda()\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Create flattened versions of the data\n",
    "    y_val_np = y_val.values.flatten()\n",
    "    y_test_np = y_test.values.flatten()\n",
    "\n",
    "    # Create TensorDatasets for training and validation sets\n",
    "    train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Define d_out and d_in\n",
    "    d_out = 1  \n",
    "    d_in=X_train_.shape[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Task 361085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10081\n",
      "13\n",
      "Task 361086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163065\n",
      "14\n",
      "Task 361087\n",
      "13932\n",
      "15\n",
      "Task 361088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21263\n",
      "16\n",
      "Task 361279\n",
      "8885\n",
      "17\n",
      "Task 361280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177\n",
      "18\n",
      "Task 361281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5465575\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "#task_id=361072\n",
    "i=12\n",
    "for task_id in benchmark_suite.tasks[12:]:\n",
    "    print(i)\n",
    "    i=i+1\n",
    "    print(f\"Task {task_id}\")\n",
    "\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    \n",
    "    print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "[I 2024-03-18 20:14:50,123] A new study created in memory with name: no-name-bef93cda-3f8e-4483-98de-1ea06ce8430f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-18 20:17:09,038] Trial 0 failed with parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 1000} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-18 20:17:09,075] Trial 0 failed with value tensor(nan).\n",
      "[W 2024-03-18 20:17:24,775] Trial 1 failed with parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_22820\\289053326.py\", line 228, in MLP_opt\n",
      "    n_epochs=train(MLP_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\ONLY NUMERICAL FEATURES\\REGRESSION\\RMSE\\utils.py\", line 43, in train\n",
      "    for batch_X, batch_y in train_loader:\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 162, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-18 20:17:24,865] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 247\u001b[0m\n\u001b[0;32m    245\u001b[0m sampler_MLP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    246\u001b[0m study_MLP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_MLP, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 247\u001b[0m \u001b[43mstudy_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m MLP_model \u001b[38;5;241m=\u001b[39m MLP(\n\u001b[0;32m    250\u001b[0m     d_in\u001b[38;5;241m=\u001b[39md_in,\n\u001b[0;32m    251\u001b[0m     d_out\u001b[38;5;241m=\u001b[39md_out,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mstudy_MLP\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 228\u001b[0m, in \u001b[0;36mMLP_opt\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    225\u001b[0m     MLP_model \u001b[38;5;241m=\u001b[39m MLP_model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    227\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mPATIENCE, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mCHECKPOINT_PATH)\n\u001b[1;32m--> 228\u001b[0m n_epochs\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain__loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, n_epochs, n_epochs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Point prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\ONLY NUMERICAL FEATURES\\REGRESSION\\RMSE\\utils.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, training_iterations, train_loader, val_loader, early_stopping, checkpoint_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_iterations):\n\u001b[0;32m     42\u001b[0m     n_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     46\u001b[0m             batch_X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361279\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/CLUSTERING', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/CLUSTERING/task_{task_id}.pt'\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features\n",
    "X = X.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# New new implementation\n",
    "N_CLUSTERS=20\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_scaled)\n",
    "distances=[]\n",
    "mahalanobis_dist=[]\n",
    "counts=[]\n",
    "ideal_len=len(kmeans.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances.append(np.abs(np.sum(kmeans.labels_==i)-ideal_len))\n",
    "    counts.append(np.sum(kmeans.labels_==i))\n",
    "    mean_k= np.mean(X.loc[kmeans.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist.append(mahalanobis(mean_k, mean, np.linalg.inv(cov)))\n",
    "\n",
    "dist_df=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist, 'count': counts}, index=np.arange(N_CLUSTERS))\n",
    "dist_df=dist_df.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df['cumulative_count']=dist_df['count'].cumsum()\n",
    "dist_df['abs_diff']=np.abs(dist_df['cumulative_count']-ideal_len)\n",
    "\n",
    "final=(np.where(dist_df['abs_diff']==np.min(dist_df['abs_diff']))[0])[0]\n",
    "labelss=dist_df.index[0:final+1].to_list()\n",
    "labels=pd.Series(kmeans.labels_).isin(labelss)\n",
    "labels.index=X.index\n",
    "close_index=labels.index[np.where(labels==False)[0]]\n",
    "far_index=labels.index[np.where(labels==True)[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean_ = np.mean(X_train, axis=0)\n",
    "cov_ = np.cov(X_train.T)\n",
    "scaler_ = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_train_scaled = scaler_.fit_transform(X_train)\n",
    "\n",
    "kmeans_ = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_train_scaled)\n",
    "distances_=[]\n",
    "counts_=[]\n",
    "mahalanobis_dist_=[]\n",
    "ideal_len_=len(kmeans_.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances_.append(np.abs(np.sum(kmeans_.labels_==i)-ideal_len_))\n",
    "    counts_.append(np.sum(kmeans_.labels_==i))\n",
    "    mean_k_= np.mean(X_train.loc[kmeans_.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist_.append(mahalanobis(mean_k_, mean_, np.linalg.inv(cov_)))\n",
    "\n",
    "dist_df_=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist_, 'count': counts_}, index=np.arange(N_CLUSTERS))\n",
    "dist_df_=dist_df_.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df_['cumulative_count']=dist_df_['count'].cumsum()\n",
    "dist_df_['abs_diff']=np.abs(dist_df_['cumulative_count']-ideal_len_)\n",
    "\n",
    "final_=(np.where(dist_df_['abs_diff']==np.min(dist_df_['abs_diff']))[0])[0]\n",
    "labelss_=dist_df_.index[0:final_+1].to_list()\n",
    "labels_=pd.Series(kmeans_.labels_).isin(labelss_)\n",
    "labels_.index=X_train.index\n",
    "close_index_=labels_.index[np.where(labels_==False)[0]]\n",
    "far_index_=labels_.index[np.where(labels_==True)[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "    # Standardize the data\n",
    "mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "std_X_train_ = np.std(X_train_, axis=0)\n",
    "X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define d_out and d_in\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "#### MLP\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False, path=CHECKPOINT_PATH)\n",
    "    n_epochs=train(MLP_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in val_loader:\n",
    "            batch_predictions = MLP_model(batch_X).reshape(-1,)\n",
    "            predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    y_val_hat_MLP = torch.Tensor(np.concatenate(predictions))\n",
    "    if torch.cuda.is_available():\n",
    "        y_val_hat_MLP = y_val_hat_MLP.cuda()\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, optimizer, n_epochs, train_loader)\n",
    "\n",
    "# Point prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_predictions = MLP_model(batch_X).reshape(-1,)\n",
    "        predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "y_test_hat_MLP = torch.Tensor(np.concatenate(predictions))\n",
    "if torch.cuda.is_available():\n",
    "    y_test_hat_MLP = y_test_hat_MLP.cuda()\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)\n",
    "del MLP_model, optimizer, criterion, y_test_hat_MLP, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "n_blocks = 1\n",
    "d_block = 20\n",
    "dropout = 0.5\n",
    "MLP_model = MLP(\n",
    "d_in=d_in,\n",
    "d_out=d_out,\n",
    "n_blocks=n_blocks,\n",
    "d_block=d_block,\n",
    "dropout=dropout,\n",
    ")   \n",
    "n_epochs=N_EPOCHS\n",
    "learning_rate=0.001\n",
    "weight_decay=1e-8\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, verbose=False, path=CHECKPOINT_PATH)\n",
    "n_epochs=train(MLP_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
    "#n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in val_loader:\n",
    "        batch_predictions = MLP_model(batch_X).reshape(-1,)\n",
    "        predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "y_val_hat_MLP = torch.Tensor(np.concatenate(predictions))\n",
    "if torch.cuda.is_available():\n",
    "    y_val_hat_MLP = y_val_hat_MLP.cuda()\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (linear): Linear(in_features=38, out_features=20, bias=True)\n",
       "      (activation): ReLU()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.983921\n",
       "1       0.916621\n",
       "3       0.912105\n",
       "4       0.914634\n",
       "5       0.910027\n",
       "          ...   \n",
       "8877    0.944806\n",
       "8878    0.907136\n",
       "8881    0.988708\n",
       "8882    0.903162\n",
       "8883    0.990515\n",
       "Name: oz252, Length: 5744, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034199922696794414"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(X_val['oz87'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz2</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz6</th>\n",
       "      <th>oz9</th>\n",
       "      <th>oz10</th>\n",
       "      <th>oz11</th>\n",
       "      <th>oz12</th>\n",
       "      <th>oz13</th>\n",
       "      <th>oz31</th>\n",
       "      <th>...</th>\n",
       "      <th>oz178</th>\n",
       "      <th>oz183</th>\n",
       "      <th>oz185</th>\n",
       "      <th>oz197</th>\n",
       "      <th>oz246</th>\n",
       "      <th>oz247</th>\n",
       "      <th>oz248</th>\n",
       "      <th>oz249</th>\n",
       "      <th>oz250</th>\n",
       "      <th>oz251</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oz1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.273602</td>\n",
       "      <td>-0.140465</td>\n",
       "      <td>0.010467</td>\n",
       "      <td>-0.025167</td>\n",
       "      <td>-0.131809</td>\n",
       "      <td>-0.039172</td>\n",
       "      <td>0.011603</td>\n",
       "      <td>-0.050456</td>\n",
       "      <td>-0.051287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084987</td>\n",
       "      <td>-0.080928</td>\n",
       "      <td>-0.037376</td>\n",
       "      <td>-0.075661</td>\n",
       "      <td>-0.029370</td>\n",
       "      <td>-0.037655</td>\n",
       "      <td>0.022205</td>\n",
       "      <td>-0.116815</td>\n",
       "      <td>-0.084989</td>\n",
       "      <td>0.057565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz2</th>\n",
       "      <td>-0.273602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.085644</td>\n",
       "      <td>-0.229852</td>\n",
       "      <td>-0.239174</td>\n",
       "      <td>-0.147636</td>\n",
       "      <td>-0.176033</td>\n",
       "      <td>0.479102</td>\n",
       "      <td>0.547327</td>\n",
       "      <td>-0.037036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101362</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>-0.014909</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>-0.028311</td>\n",
       "      <td>0.698548</td>\n",
       "      <td>0.261746</td>\n",
       "      <td>0.152658</td>\n",
       "      <td>0.134318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz5</th>\n",
       "      <td>-0.140465</td>\n",
       "      <td>-0.085644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>0.060127</td>\n",
       "      <td>0.276922</td>\n",
       "      <td>0.328889</td>\n",
       "      <td>-0.215681</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>0.160974</td>\n",
       "      <td>0.300852</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>-0.139639</td>\n",
       "      <td>-0.095501</td>\n",
       "      <td>-0.320399</td>\n",
       "      <td>0.319916</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.081910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz6</th>\n",
       "      <td>0.010467</td>\n",
       "      <td>-0.229852</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058666</td>\n",
       "      <td>-0.042387</td>\n",
       "      <td>-0.056353</td>\n",
       "      <td>-0.134975</td>\n",
       "      <td>-0.178674</td>\n",
       "      <td>0.079225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046867</td>\n",
       "      <td>0.091201</td>\n",
       "      <td>0.109105</td>\n",
       "      <td>0.086495</td>\n",
       "      <td>-0.231269</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.322788</td>\n",
       "      <td>0.286721</td>\n",
       "      <td>0.031031</td>\n",
       "      <td>0.152620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz9</th>\n",
       "      <td>-0.025167</td>\n",
       "      <td>-0.239174</td>\n",
       "      <td>0.060127</td>\n",
       "      <td>-0.058666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432189</td>\n",
       "      <td>0.401716</td>\n",
       "      <td>-0.211196</td>\n",
       "      <td>-0.174930</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168098</td>\n",
       "      <td>-0.042534</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.043691</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>-0.079321</td>\n",
       "      <td>-0.387844</td>\n",
       "      <td>0.183621</td>\n",
       "      <td>-0.077778</td>\n",
       "      <td>0.159365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz10</th>\n",
       "      <td>-0.131809</td>\n",
       "      <td>-0.147636</td>\n",
       "      <td>0.276922</td>\n",
       "      <td>-0.042387</td>\n",
       "      <td>0.432189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440420</td>\n",
       "      <td>-0.154072</td>\n",
       "      <td>-0.128020</td>\n",
       "      <td>0.279366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236421</td>\n",
       "      <td>0.148055</td>\n",
       "      <td>0.202714</td>\n",
       "      <td>-0.023876</td>\n",
       "      <td>-0.186567</td>\n",
       "      <td>0.167520</td>\n",
       "      <td>-0.361038</td>\n",
       "      <td>0.307490</td>\n",
       "      <td>0.140182</td>\n",
       "      <td>0.028103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz11</th>\n",
       "      <td>-0.039172</td>\n",
       "      <td>-0.176033</td>\n",
       "      <td>0.328889</td>\n",
       "      <td>-0.056353</td>\n",
       "      <td>0.401716</td>\n",
       "      <td>0.440420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.140198</td>\n",
       "      <td>-0.138477</td>\n",
       "      <td>0.103611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255415</td>\n",
       "      <td>0.052165</td>\n",
       "      <td>0.065062</td>\n",
       "      <td>-0.060617</td>\n",
       "      <td>-0.192377</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>-0.297188</td>\n",
       "      <td>0.210083</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>0.154084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz12</th>\n",
       "      <td>0.011603</td>\n",
       "      <td>0.479102</td>\n",
       "      <td>-0.215681</td>\n",
       "      <td>-0.134975</td>\n",
       "      <td>-0.211196</td>\n",
       "      <td>-0.154072</td>\n",
       "      <td>-0.140198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571512</td>\n",
       "      <td>-0.090388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088996</td>\n",
       "      <td>0.066911</td>\n",
       "      <td>-0.033344</td>\n",
       "      <td>0.067898</td>\n",
       "      <td>0.774787</td>\n",
       "      <td>-0.192934</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>0.562730</td>\n",
       "      <td>0.084040</td>\n",
       "      <td>0.460516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz13</th>\n",
       "      <td>-0.050456</td>\n",
       "      <td>0.547327</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>-0.178674</td>\n",
       "      <td>-0.174930</td>\n",
       "      <td>-0.128020</td>\n",
       "      <td>-0.138477</td>\n",
       "      <td>0.571512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083885</td>\n",
       "      <td>0.103985</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>0.764423</td>\n",
       "      <td>0.050421</td>\n",
       "      <td>0.626047</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>0.245347</td>\n",
       "      <td>0.233699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz31</th>\n",
       "      <td>-0.051287</td>\n",
       "      <td>-0.037036</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.079225</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.279366</td>\n",
       "      <td>0.103611</td>\n",
       "      <td>-0.090388</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045437</td>\n",
       "      <td>0.395545</td>\n",
       "      <td>0.730116</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>-0.010470</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>-0.192889</td>\n",
       "      <td>0.186449</td>\n",
       "      <td>0.354588</td>\n",
       "      <td>-0.017327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz83</th>\n",
       "      <td>-0.062661</td>\n",
       "      <td>-0.090141</td>\n",
       "      <td>0.070381</td>\n",
       "      <td>0.063248</td>\n",
       "      <td>0.100361</td>\n",
       "      <td>0.448574</td>\n",
       "      <td>0.232498</td>\n",
       "      <td>-0.128178</td>\n",
       "      <td>-0.079572</td>\n",
       "      <td>0.027495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095727</td>\n",
       "      <td>-0.041553</td>\n",
       "      <td>-0.017060</td>\n",
       "      <td>-0.058630</td>\n",
       "      <td>-0.165020</td>\n",
       "      <td>0.546830</td>\n",
       "      <td>-0.206738</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>-0.061150</td>\n",
       "      <td>-0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz87</th>\n",
       "      <td>-0.084474</td>\n",
       "      <td>-0.064982</td>\n",
       "      <td>0.085739</td>\n",
       "      <td>0.029860</td>\n",
       "      <td>0.101034</td>\n",
       "      <td>0.478672</td>\n",
       "      <td>0.183549</td>\n",
       "      <td>-0.112566</td>\n",
       "      <td>-0.071726</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072594</td>\n",
       "      <td>-0.021019</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>-0.046642</td>\n",
       "      <td>-0.132059</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-0.175995</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>-0.023218</td>\n",
       "      <td>-0.159369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz124</th>\n",
       "      <td>-0.057980</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>0.209164</td>\n",
       "      <td>0.134554</td>\n",
       "      <td>-0.063558</td>\n",
       "      <td>0.097367</td>\n",
       "      <td>-0.031885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055280</td>\n",
       "      <td>0.156611</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.076544</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>0.750298</td>\n",
       "      <td>-0.098160</td>\n",
       "      <td>0.179432</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>-0.283180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz125</th>\n",
       "      <td>-0.110838</td>\n",
       "      <td>0.171764</td>\n",
       "      <td>0.161004</td>\n",
       "      <td>0.050860</td>\n",
       "      <td>-0.048013</td>\n",
       "      <td>0.232370</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>0.056805</td>\n",
       "      <td>0.295719</td>\n",
       "      <td>0.281719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079428</td>\n",
       "      <td>0.582479</td>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.425882</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.298156</td>\n",
       "      <td>-0.067109</td>\n",
       "      <td>0.448335</td>\n",
       "      <td>0.743955</td>\n",
       "      <td>-0.386026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz126</th>\n",
       "      <td>-0.100663</td>\n",
       "      <td>-0.138729</td>\n",
       "      <td>0.832812</td>\n",
       "      <td>0.153124</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.293997</td>\n",
       "      <td>0.375237</td>\n",
       "      <td>-0.227698</td>\n",
       "      <td>-0.092439</td>\n",
       "      <td>0.294469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147328</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.284736</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.199924</td>\n",
       "      <td>-0.063579</td>\n",
       "      <td>-0.340757</td>\n",
       "      <td>0.267670</td>\n",
       "      <td>0.081947</td>\n",
       "      <td>0.168708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz127</th>\n",
       "      <td>0.002832</td>\n",
       "      <td>-0.317115</td>\n",
       "      <td>0.131059</td>\n",
       "      <td>0.659413</td>\n",
       "      <td>0.559381</td>\n",
       "      <td>0.293118</td>\n",
       "      <td>0.272046</td>\n",
       "      <td>-0.242139</td>\n",
       "      <td>-0.240356</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097577</td>\n",
       "      <td>0.029831</td>\n",
       "      <td>0.027837</td>\n",
       "      <td>-0.004360</td>\n",
       "      <td>-0.348743</td>\n",
       "      <td>-0.090165</td>\n",
       "      <td>-0.443787</td>\n",
       "      <td>0.293409</td>\n",
       "      <td>-0.087089</td>\n",
       "      <td>0.307672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz128</th>\n",
       "      <td>-0.126454</td>\n",
       "      <td>-0.096013</td>\n",
       "      <td>0.263427</td>\n",
       "      <td>-0.045497</td>\n",
       "      <td>0.381932</td>\n",
       "      <td>0.615999</td>\n",
       "      <td>0.388704</td>\n",
       "      <td>-0.114996</td>\n",
       "      <td>-0.104873</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.086534</td>\n",
       "      <td>0.071376</td>\n",
       "      <td>-0.069609</td>\n",
       "      <td>-0.120017</td>\n",
       "      <td>-0.038930</td>\n",
       "      <td>-0.222176</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>-0.005682</td>\n",
       "      <td>0.234596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz131</th>\n",
       "      <td>-0.016805</td>\n",
       "      <td>-0.150105</td>\n",
       "      <td>0.042318</td>\n",
       "      <td>0.423572</td>\n",
       "      <td>0.297055</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>-0.017244</td>\n",
       "      <td>-0.088841</td>\n",
       "      <td>-0.126314</td>\n",
       "      <td>0.059480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147025</td>\n",
       "      <td>0.047780</td>\n",
       "      <td>0.145826</td>\n",
       "      <td>0.099711</td>\n",
       "      <td>-0.158405</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>-0.291973</td>\n",
       "      <td>0.218147</td>\n",
       "      <td>0.099559</td>\n",
       "      <td>-0.073559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz133</th>\n",
       "      <td>-0.113523</td>\n",
       "      <td>-0.111297</td>\n",
       "      <td>0.264134</td>\n",
       "      <td>0.036760</td>\n",
       "      <td>0.208612</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.218752</td>\n",
       "      <td>-0.096268</td>\n",
       "      <td>-0.120232</td>\n",
       "      <td>0.320723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221444</td>\n",
       "      <td>0.201221</td>\n",
       "      <td>0.271806</td>\n",
       "      <td>0.049240</td>\n",
       "      <td>-0.138252</td>\n",
       "      <td>0.289198</td>\n",
       "      <td>-0.315251</td>\n",
       "      <td>0.299399</td>\n",
       "      <td>0.207080</td>\n",
       "      <td>-0.121057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz150</th>\n",
       "      <td>-0.034373</td>\n",
       "      <td>0.379597</td>\n",
       "      <td>0.072765</td>\n",
       "      <td>-0.114045</td>\n",
       "      <td>-0.046552</td>\n",
       "      <td>-0.024336</td>\n",
       "      <td>-0.011324</td>\n",
       "      <td>0.575443</td>\n",
       "      <td>0.719155</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040828</td>\n",
       "      <td>0.086270</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>-0.061774</td>\n",
       "      <td>0.591720</td>\n",
       "      <td>-0.089908</td>\n",
       "      <td>0.490989</td>\n",
       "      <td>0.507260</td>\n",
       "      <td>0.014406</td>\n",
       "      <td>0.413207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz151</th>\n",
       "      <td>-0.020569</td>\n",
       "      <td>0.321975</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>-0.117297</td>\n",
       "      <td>-0.167408</td>\n",
       "      <td>-0.114912</td>\n",
       "      <td>-0.095915</td>\n",
       "      <td>0.302670</td>\n",
       "      <td>0.608066</td>\n",
       "      <td>-0.072367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062284</td>\n",
       "      <td>0.124322</td>\n",
       "      <td>-0.044567</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.452007</td>\n",
       "      <td>0.070381</td>\n",
       "      <td>0.357723</td>\n",
       "      <td>0.331564</td>\n",
       "      <td>0.177510</td>\n",
       "      <td>0.154381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz165</th>\n",
       "      <td>-0.094264</td>\n",
       "      <td>-0.013436</td>\n",
       "      <td>0.166299</td>\n",
       "      <td>0.111580</td>\n",
       "      <td>-0.023261</td>\n",
       "      <td>0.174906</td>\n",
       "      <td>0.064672</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>-0.063895</td>\n",
       "      <td>0.498050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>0.588437</td>\n",
       "      <td>0.427866</td>\n",
       "      <td>0.335663</td>\n",
       "      <td>0.078302</td>\n",
       "      <td>-0.056602</td>\n",
       "      <td>-0.205677</td>\n",
       "      <td>0.270284</td>\n",
       "      <td>0.489010</td>\n",
       "      <td>-0.140813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz171</th>\n",
       "      <td>-0.038747</td>\n",
       "      <td>-0.268703</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.528252</td>\n",
       "      <td>0.418194</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.387861</td>\n",
       "      <td>-0.249008</td>\n",
       "      <td>-0.174857</td>\n",
       "      <td>0.072752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>-0.068933</td>\n",
       "      <td>-0.319031</td>\n",
       "      <td>-0.092331</td>\n",
       "      <td>-0.399640</td>\n",
       "      <td>0.304439</td>\n",
       "      <td>-0.125118</td>\n",
       "      <td>0.394286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz172</th>\n",
       "      <td>-0.074616</td>\n",
       "      <td>0.291455</td>\n",
       "      <td>0.139503</td>\n",
       "      <td>0.098833</td>\n",
       "      <td>0.036978</td>\n",
       "      <td>0.054426</td>\n",
       "      <td>-0.061248</td>\n",
       "      <td>0.664943</td>\n",
       "      <td>0.424458</td>\n",
       "      <td>0.037659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082008</td>\n",
       "      <td>0.109481</td>\n",
       "      <td>0.137792</td>\n",
       "      <td>0.093393</td>\n",
       "      <td>0.503594</td>\n",
       "      <td>-0.109724</td>\n",
       "      <td>0.299913</td>\n",
       "      <td>0.662195</td>\n",
       "      <td>0.184499</td>\n",
       "      <td>0.267975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz173</th>\n",
       "      <td>-0.066015</td>\n",
       "      <td>0.035322</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>-0.007649</td>\n",
       "      <td>0.072629</td>\n",
       "      <td>0.202694</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>0.039499</td>\n",
       "      <td>0.073015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072911</td>\n",
       "      <td>0.031375</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.032629</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>0.099537</td>\n",
       "      <td>-0.096295</td>\n",
       "      <td>0.078981</td>\n",
       "      <td>0.161610</td>\n",
       "      <td>-0.134101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz175</th>\n",
       "      <td>-0.065573</td>\n",
       "      <td>0.027470</td>\n",
       "      <td>-0.090188</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>-0.090836</td>\n",
       "      <td>0.112199</td>\n",
       "      <td>0.011521</td>\n",
       "      <td>-0.061148</td>\n",
       "      <td>0.088454</td>\n",
       "      <td>-0.087487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>0.196432</td>\n",
       "      <td>-0.082774</td>\n",
       "      <td>0.355980</td>\n",
       "      <td>0.039736</td>\n",
       "      <td>0.739024</td>\n",
       "      <td>-0.074189</td>\n",
       "      <td>0.143047</td>\n",
       "      <td>0.148840</td>\n",
       "      <td>-0.411090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz176</th>\n",
       "      <td>-0.025583</td>\n",
       "      <td>-0.053935</td>\n",
       "      <td>0.199095</td>\n",
       "      <td>0.138908</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.203536</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>-0.055483</td>\n",
       "      <td>-0.048242</td>\n",
       "      <td>0.577631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011094</td>\n",
       "      <td>0.418435</td>\n",
       "      <td>0.406347</td>\n",
       "      <td>0.165333</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>-0.192769</td>\n",
       "      <td>0.202443</td>\n",
       "      <td>0.352066</td>\n",
       "      <td>-0.072793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz177</th>\n",
       "      <td>-0.041919</td>\n",
       "      <td>-0.191515</td>\n",
       "      <td>0.424230</td>\n",
       "      <td>0.197055</td>\n",
       "      <td>0.318310</td>\n",
       "      <td>0.227276</td>\n",
       "      <td>0.222859</td>\n",
       "      <td>-0.206059</td>\n",
       "      <td>-0.187935</td>\n",
       "      <td>0.109222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061142</td>\n",
       "      <td>0.102677</td>\n",
       "      <td>0.177261</td>\n",
       "      <td>0.071842</td>\n",
       "      <td>-0.245215</td>\n",
       "      <td>-0.040030</td>\n",
       "      <td>-0.369886</td>\n",
       "      <td>0.197910</td>\n",
       "      <td>0.048809</td>\n",
       "      <td>0.045196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz178</th>\n",
       "      <td>-0.084987</td>\n",
       "      <td>-0.101362</td>\n",
       "      <td>0.158018</td>\n",
       "      <td>0.046867</td>\n",
       "      <td>0.168098</td>\n",
       "      <td>0.236421</td>\n",
       "      <td>0.255415</td>\n",
       "      <td>-0.088996</td>\n",
       "      <td>-0.083885</td>\n",
       "      <td>0.045437</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024804</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>-0.034880</td>\n",
       "      <td>-0.144542</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>-0.182887</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.034257</td>\n",
       "      <td>-0.023055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz183</th>\n",
       "      <td>-0.080928</td>\n",
       "      <td>0.056882</td>\n",
       "      <td>0.160974</td>\n",
       "      <td>0.091201</td>\n",
       "      <td>-0.042534</td>\n",
       "      <td>0.148055</td>\n",
       "      <td>0.052165</td>\n",
       "      <td>0.066911</td>\n",
       "      <td>0.103985</td>\n",
       "      <td>0.395545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454675</td>\n",
       "      <td>0.470506</td>\n",
       "      <td>0.205660</td>\n",
       "      <td>-0.060972</td>\n",
       "      <td>-0.175043</td>\n",
       "      <td>0.347251</td>\n",
       "      <td>0.659343</td>\n",
       "      <td>-0.183075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz185</th>\n",
       "      <td>-0.037376</td>\n",
       "      <td>-0.014909</td>\n",
       "      <td>0.300852</td>\n",
       "      <td>0.109105</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.202714</td>\n",
       "      <td>0.065062</td>\n",
       "      <td>-0.033344</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>0.730116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>0.454675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023055</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>-0.086198</td>\n",
       "      <td>-0.180706</td>\n",
       "      <td>0.240827</td>\n",
       "      <td>0.458370</td>\n",
       "      <td>0.024335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz197</th>\n",
       "      <td>-0.075661</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.086495</td>\n",
       "      <td>-0.043691</td>\n",
       "      <td>-0.023876</td>\n",
       "      <td>-0.060617</td>\n",
       "      <td>0.067898</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034880</td>\n",
       "      <td>0.470506</td>\n",
       "      <td>0.023055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>-0.030358</td>\n",
       "      <td>-0.131694</td>\n",
       "      <td>0.216317</td>\n",
       "      <td>0.314207</td>\n",
       "      <td>-0.216482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz246</th>\n",
       "      <td>-0.029370</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>-0.139639</td>\n",
       "      <td>-0.231269</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>-0.186567</td>\n",
       "      <td>-0.192377</td>\n",
       "      <td>0.774787</td>\n",
       "      <td>0.764423</td>\n",
       "      <td>-0.010470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144542</td>\n",
       "      <td>0.205660</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092863</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>0.508482</td>\n",
       "      <td>0.313382</td>\n",
       "      <td>0.246451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz247</th>\n",
       "      <td>-0.037655</td>\n",
       "      <td>-0.028311</td>\n",
       "      <td>-0.095501</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.079321</td>\n",
       "      <td>0.167520</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>-0.192934</td>\n",
       "      <td>0.050421</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>-0.060972</td>\n",
       "      <td>-0.086198</td>\n",
       "      <td>-0.030358</td>\n",
       "      <td>-0.092863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>-0.023320</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>-0.415677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz248</th>\n",
       "      <td>0.022205</td>\n",
       "      <td>0.698548</td>\n",
       "      <td>-0.320399</td>\n",
       "      <td>-0.322788</td>\n",
       "      <td>-0.387844</td>\n",
       "      <td>-0.361038</td>\n",
       "      <td>-0.297188</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>0.626047</td>\n",
       "      <td>-0.192889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182887</td>\n",
       "      <td>-0.175043</td>\n",
       "      <td>-0.180706</td>\n",
       "      <td>-0.131694</td>\n",
       "      <td>0.817976</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>-0.059121</td>\n",
       "      <td>0.263623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz249</th>\n",
       "      <td>-0.116815</td>\n",
       "      <td>0.261746</td>\n",
       "      <td>0.319916</td>\n",
       "      <td>0.286721</td>\n",
       "      <td>0.183621</td>\n",
       "      <td>0.307490</td>\n",
       "      <td>0.210083</td>\n",
       "      <td>0.562730</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>0.186449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.347251</td>\n",
       "      <td>0.240827</td>\n",
       "      <td>0.216317</td>\n",
       "      <td>0.508482</td>\n",
       "      <td>-0.023320</td>\n",
       "      <td>0.141908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.417907</td>\n",
       "      <td>0.443207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz250</th>\n",
       "      <td>-0.084989</td>\n",
       "      <td>0.152658</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.031031</td>\n",
       "      <td>-0.077778</td>\n",
       "      <td>0.140182</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>0.084040</td>\n",
       "      <td>0.245347</td>\n",
       "      <td>0.354588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034257</td>\n",
       "      <td>0.659343</td>\n",
       "      <td>0.458370</td>\n",
       "      <td>0.314207</td>\n",
       "      <td>0.313382</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>-0.059121</td>\n",
       "      <td>0.417907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.258709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz251</th>\n",
       "      <td>0.057565</td>\n",
       "      <td>0.134318</td>\n",
       "      <td>0.081910</td>\n",
       "      <td>0.152620</td>\n",
       "      <td>0.159365</td>\n",
       "      <td>0.028103</td>\n",
       "      <td>0.154084</td>\n",
       "      <td>0.460516</td>\n",
       "      <td>0.233699</td>\n",
       "      <td>-0.017327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023055</td>\n",
       "      <td>-0.183075</td>\n",
       "      <td>0.024335</td>\n",
       "      <td>-0.216482</td>\n",
       "      <td>0.246451</td>\n",
       "      <td>-0.415677</td>\n",
       "      <td>0.263623</td>\n",
       "      <td>0.443207</td>\n",
       "      <td>-0.258709</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            oz1       oz2       oz5       oz6       oz9      oz10      oz11  \\\n",
       "oz1    1.000000 -0.273602 -0.140465  0.010467 -0.025167 -0.131809 -0.039172   \n",
       "oz2   -0.273602  1.000000 -0.085644 -0.229852 -0.239174 -0.147636 -0.176033   \n",
       "oz5   -0.140465 -0.085644  1.000000  0.114997  0.060127  0.276922  0.328889   \n",
       "oz6    0.010467 -0.229852  0.114997  1.000000 -0.058666 -0.042387 -0.056353   \n",
       "oz9   -0.025167 -0.239174  0.060127 -0.058666  1.000000  0.432189  0.401716   \n",
       "oz10  -0.131809 -0.147636  0.276922 -0.042387  0.432189  1.000000  0.440420   \n",
       "oz11  -0.039172 -0.176033  0.328889 -0.056353  0.401716  0.440420  1.000000   \n",
       "oz12   0.011603  0.479102 -0.215681 -0.134975 -0.211196 -0.154072 -0.140198   \n",
       "oz13  -0.050456  0.547327  0.016218 -0.178674 -0.174930 -0.128020 -0.138477   \n",
       "oz31  -0.051287 -0.037036  0.276900  0.079225  0.033746  0.279366  0.103611   \n",
       "oz83  -0.062661 -0.090141  0.070381  0.063248  0.100361  0.448574  0.232498   \n",
       "oz87  -0.084474 -0.064982  0.085739  0.029860  0.101034  0.478672  0.183549   \n",
       "oz124 -0.057980  0.016699  0.004620  0.050011 -0.013403  0.209164  0.134554   \n",
       "oz125 -0.110838  0.171764  0.161004  0.050860 -0.048013  0.232370  0.026294   \n",
       "oz126 -0.100663 -0.138729  0.832812  0.153124  0.082208  0.293997  0.375237   \n",
       "oz127  0.002832 -0.317115  0.131059  0.659413  0.559381  0.293118  0.272046   \n",
       "oz128 -0.126454 -0.096013  0.263427 -0.045497  0.381932  0.615999  0.388704   \n",
       "oz131 -0.016805 -0.150105  0.042318  0.423572  0.297055  0.006628 -0.017244   \n",
       "oz133 -0.113523 -0.111297  0.264134  0.036760  0.208612  0.739944  0.218752   \n",
       "oz150 -0.034373  0.379597  0.072765 -0.114045 -0.046552 -0.024336 -0.011324   \n",
       "oz151 -0.020569  0.321975  0.023875 -0.117297 -0.167408 -0.114912 -0.095915   \n",
       "oz165 -0.094264 -0.013436  0.166299  0.111580 -0.023261  0.174906  0.064672   \n",
       "oz171 -0.038747 -0.268703  0.406937  0.528252  0.418194  0.337300  0.387861   \n",
       "oz172 -0.074616  0.291455  0.139503  0.098833  0.036978  0.054426 -0.061248   \n",
       "oz173 -0.066015  0.035322  0.012938 -0.007649  0.072629  0.202694  0.001720   \n",
       "oz175 -0.065573  0.027470 -0.090188  0.042703 -0.090836  0.112199  0.011521   \n",
       "oz176 -0.025583 -0.053935  0.199095  0.138908  0.006947  0.203536  0.027920   \n",
       "oz177 -0.041919 -0.191515  0.424230  0.197055  0.318310  0.227276  0.222859   \n",
       "oz178 -0.084987 -0.101362  0.158018  0.046867  0.168098  0.236421  0.255415   \n",
       "oz183 -0.080928  0.056882  0.160974  0.091201 -0.042534  0.148055  0.052165   \n",
       "oz185 -0.037376 -0.014909  0.300852  0.109105  0.002250  0.202714  0.065062   \n",
       "oz197 -0.075661  0.055853  0.008218  0.086495 -0.043691 -0.023876 -0.060617   \n",
       "oz246 -0.029370  0.776882 -0.139639 -0.231269 -0.281464 -0.186567 -0.192377   \n",
       "oz247 -0.037655 -0.028311 -0.095501 -0.002247 -0.079321  0.167520  0.021815   \n",
       "oz248  0.022205  0.698548 -0.320399 -0.322788 -0.387844 -0.361038 -0.297188   \n",
       "oz249 -0.116815  0.261746  0.319916  0.286721  0.183621  0.307490  0.210083   \n",
       "oz250 -0.084989  0.152658  0.150831  0.031031 -0.077778  0.140182 -0.001328   \n",
       "oz251  0.057565  0.134318  0.081910  0.152620  0.159365  0.028103  0.154084   \n",
       "\n",
       "           oz12      oz13      oz31  ...     oz178     oz183     oz185  \\\n",
       "oz1    0.011603 -0.050456 -0.051287  ... -0.084987 -0.080928 -0.037376   \n",
       "oz2    0.479102  0.547327 -0.037036  ... -0.101362  0.056882 -0.014909   \n",
       "oz5   -0.215681  0.016218  0.276900  ...  0.158018  0.160974  0.300852   \n",
       "oz6   -0.134975 -0.178674  0.079225  ...  0.046867  0.091201  0.109105   \n",
       "oz9   -0.211196 -0.174930  0.033746  ...  0.168098 -0.042534  0.002250   \n",
       "oz10  -0.154072 -0.128020  0.279366  ...  0.236421  0.148055  0.202714   \n",
       "oz11  -0.140198 -0.138477  0.103611  ...  0.255415  0.052165  0.065062   \n",
       "oz12   1.000000  0.571512 -0.090388  ... -0.088996  0.066911 -0.033344   \n",
       "oz13   0.571512  1.000000 -0.039554  ... -0.083885  0.103985 -0.007211   \n",
       "oz31  -0.090388 -0.039554  1.000000  ...  0.045437  0.395545  0.730116   \n",
       "oz83  -0.128178 -0.079572  0.027495  ...  0.095727 -0.041553 -0.017060   \n",
       "oz87  -0.112566 -0.071726  0.029421  ...  0.072594 -0.021019 -0.003745   \n",
       "oz124 -0.063558  0.097367 -0.031885  ...  0.055280  0.156611  0.035206   \n",
       "oz125  0.056805  0.295719  0.281719  ...  0.079428  0.582479  0.375786   \n",
       "oz126 -0.227698 -0.092439  0.294469  ...  0.147328  0.159802  0.284736   \n",
       "oz127 -0.242139 -0.240356  0.070948  ...  0.097577  0.029831  0.027837   \n",
       "oz128 -0.114996 -0.104873  0.146903  ...  0.088239  0.086534  0.071376   \n",
       "oz131 -0.088841 -0.126314  0.059480  ...  0.147025  0.047780  0.145826   \n",
       "oz133 -0.096268 -0.120232  0.320723  ...  0.221444  0.201221  0.271806   \n",
       "oz150  0.575443  0.719155  0.003337  ... -0.040828  0.086270  0.032110   \n",
       "oz151  0.302670  0.608066 -0.072367  ... -0.062284  0.124322 -0.044567   \n",
       "oz165 -0.011551 -0.063895  0.498050  ... -0.012440  0.588437  0.427866   \n",
       "oz171 -0.249008 -0.174857  0.072752  ...  0.032514  0.006142  0.037177   \n",
       "oz172  0.664943  0.424458  0.037659  ...  0.082008  0.109481  0.137792   \n",
       "oz173 -0.013000  0.039499  0.073015  ...  0.072911  0.031375  0.053000   \n",
       "oz175 -0.061148  0.088454 -0.087487  ...  0.012164  0.196432 -0.082774   \n",
       "oz176 -0.055483 -0.048242  0.577631  ... -0.011094  0.418435  0.406347   \n",
       "oz177 -0.206059 -0.187935  0.109222  ...  0.061142  0.102677  0.177261   \n",
       "oz178 -0.088996 -0.083885  0.045437  ...  1.000000 -0.024804  0.036915   \n",
       "oz183  0.066911  0.103985  0.395545  ... -0.024804  1.000000  0.454675   \n",
       "oz185 -0.033344 -0.007211  0.730116  ...  0.036915  0.454675  1.000000   \n",
       "oz197  0.067898 -0.001105 -0.046474  ... -0.034880  0.470506  0.023055   \n",
       "oz246  0.774787  0.764423 -0.010470  ... -0.144542  0.205660  0.043348   \n",
       "oz247 -0.192934  0.050421 -0.050936  ...  0.037087 -0.060972 -0.086198   \n",
       "oz248  0.696208  0.626047 -0.192889  ... -0.182887 -0.175043 -0.180706   \n",
       "oz249  0.562730  0.540296  0.186449  ...  0.095419  0.347251  0.240827   \n",
       "oz250  0.084040  0.245347  0.354588  ...  0.034257  0.659343  0.458370   \n",
       "oz251  0.460516  0.233699 -0.017327  ... -0.023055 -0.183075  0.024335   \n",
       "\n",
       "          oz197     oz246     oz247     oz248     oz249     oz250     oz251  \n",
       "oz1   -0.075661 -0.029370 -0.037655  0.022205 -0.116815 -0.084989  0.057565  \n",
       "oz2    0.055853  0.776882 -0.028311  0.698548  0.261746  0.152658  0.134318  \n",
       "oz5    0.008218 -0.139639 -0.095501 -0.320399  0.319916  0.150831  0.081910  \n",
       "oz6    0.086495 -0.231269 -0.002247 -0.322788  0.286721  0.031031  0.152620  \n",
       "oz9   -0.043691 -0.281464 -0.079321 -0.387844  0.183621 -0.077778  0.159365  \n",
       "oz10  -0.023876 -0.186567  0.167520 -0.361038  0.307490  0.140182  0.028103  \n",
       "oz11  -0.060617 -0.192377  0.021815 -0.297188  0.210083 -0.001328  0.154084  \n",
       "oz12   0.067898  0.774787 -0.192934  0.696208  0.562730  0.084040  0.460516  \n",
       "oz13  -0.001105  0.764423  0.050421  0.626047  0.540296  0.245347  0.233699  \n",
       "oz31  -0.046474 -0.010470 -0.050936 -0.192889  0.186449  0.354588 -0.017327  \n",
       "oz83  -0.058630 -0.165020  0.546830 -0.206738  0.110296 -0.061150 -0.185000  \n",
       "oz87  -0.046642 -0.132059  0.484614 -0.175995  0.121527 -0.023218 -0.159369  \n",
       "oz124  0.076544  0.018639  0.750298 -0.098160  0.179432  0.122329 -0.283180  \n",
       "oz125  0.425882  0.299700  0.298156 -0.067109  0.448335  0.743955 -0.386026  \n",
       "oz126 -0.000861 -0.199924 -0.063579 -0.340757  0.267670  0.081947  0.168708  \n",
       "oz127 -0.004360 -0.348743 -0.090165 -0.443787  0.293409 -0.087089  0.307672  \n",
       "oz128 -0.069609 -0.120017 -0.038930 -0.222176  0.239239 -0.005682  0.234596  \n",
       "oz131  0.099711 -0.158405  0.049180 -0.291973  0.218147  0.099559 -0.073559  \n",
       "oz133  0.049240 -0.138252  0.289198 -0.315251  0.299399  0.207080 -0.121057  \n",
       "oz150 -0.061774  0.591720 -0.089908  0.490989  0.507260  0.014406  0.413207  \n",
       "oz151  0.005493  0.452007  0.070381  0.357723  0.331564  0.177510  0.154381  \n",
       "oz165  0.335663  0.078302 -0.056602 -0.205677  0.270284  0.489010 -0.140813  \n",
       "oz171 -0.068933 -0.319031 -0.092331 -0.399640  0.304439 -0.125118  0.394286  \n",
       "oz172  0.093393  0.503594 -0.109724  0.299913  0.662195  0.184499  0.267975  \n",
       "oz173  0.032629 -0.000531  0.099537 -0.096295  0.078981  0.161610 -0.134101  \n",
       "oz175  0.355980  0.039736  0.739024 -0.074189  0.143047  0.148840 -0.411090  \n",
       "oz176  0.165333  0.005191 -0.059510 -0.192769  0.202443  0.352066 -0.072793  \n",
       "oz177  0.071842 -0.245215 -0.040030 -0.369886  0.197910  0.048809  0.045196  \n",
       "oz178 -0.034880 -0.144542  0.037087 -0.182887  0.095419  0.034257 -0.023055  \n",
       "oz183  0.470506  0.205660 -0.060972 -0.175043  0.347251  0.659343 -0.183075  \n",
       "oz185  0.023055  0.043348 -0.086198 -0.180706  0.240827  0.458370  0.024335  \n",
       "oz197  1.000000  0.108676 -0.030358 -0.131694  0.216317  0.314207 -0.216482  \n",
       "oz246  0.108676  1.000000 -0.092863  0.817976  0.508482  0.313382  0.246451  \n",
       "oz247 -0.030358 -0.092863  1.000000 -0.088830 -0.023320  0.019254 -0.415677  \n",
       "oz248 -0.131694  0.817976 -0.088830  1.000000  0.141908 -0.059121  0.263623  \n",
       "oz249  0.216317  0.508482 -0.023320  0.141908  1.000000  0.417907  0.443207  \n",
       "oz250  0.314207  0.313382  0.019254 -0.059121  0.417907  1.000000 -0.258709  \n",
       "oz251 -0.216482  0.246451 -0.415677  0.263623  0.443207 -0.258709  1.000000  \n",
       "\n",
       "[38 rows x 38 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz2</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz6</th>\n",
       "      <th>oz9</th>\n",
       "      <th>oz10</th>\n",
       "      <th>oz11</th>\n",
       "      <th>oz12</th>\n",
       "      <th>oz13</th>\n",
       "      <th>oz31</th>\n",
       "      <th>...</th>\n",
       "      <th>oz178</th>\n",
       "      <th>oz183</th>\n",
       "      <th>oz185</th>\n",
       "      <th>oz197</th>\n",
       "      <th>oz246</th>\n",
       "      <th>oz247</th>\n",
       "      <th>oz248</th>\n",
       "      <th>oz249</th>\n",
       "      <th>oz250</th>\n",
       "      <th>oz251</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>oz1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.280895</td>\n",
       "      <td>-0.083059</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>-0.060460</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.084494</td>\n",
       "      <td>0.304376</td>\n",
       "      <td>0.156411</td>\n",
       "      <td>-0.098912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020559</td>\n",
       "      <td>-0.147482</td>\n",
       "      <td>-0.086607</td>\n",
       "      <td>-0.054875</td>\n",
       "      <td>0.314936</td>\n",
       "      <td>-0.015068</td>\n",
       "      <td>0.332724</td>\n",
       "      <td>0.201221</td>\n",
       "      <td>-0.073183</td>\n",
       "      <td>0.299677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz2</th>\n",
       "      <td>0.280895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.116349</td>\n",
       "      <td>-0.129057</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>-0.091858</td>\n",
       "      <td>-0.167797</td>\n",
       "      <td>0.378954</td>\n",
       "      <td>0.442770</td>\n",
       "      <td>-0.073512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071750</td>\n",
       "      <td>-0.096255</td>\n",
       "      <td>-0.047670</td>\n",
       "      <td>-0.078564</td>\n",
       "      <td>0.677738</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>0.662103</td>\n",
       "      <td>0.300505</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>0.262485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz5</th>\n",
       "      <td>-0.083059</td>\n",
       "      <td>-0.116349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134819</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>0.053768</td>\n",
       "      <td>0.166708</td>\n",
       "      <td>-0.215724</td>\n",
       "      <td>0.034366</td>\n",
       "      <td>0.131824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054138</td>\n",
       "      <td>0.075510</td>\n",
       "      <td>0.210204</td>\n",
       "      <td>0.019698</td>\n",
       "      <td>-0.166455</td>\n",
       "      <td>-0.160602</td>\n",
       "      <td>-0.350819</td>\n",
       "      <td>0.188942</td>\n",
       "      <td>0.060327</td>\n",
       "      <td>0.063637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz6</th>\n",
       "      <td>0.007783</td>\n",
       "      <td>-0.129057</td>\n",
       "      <td>0.134819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089136</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>-0.038255</td>\n",
       "      <td>-0.059768</td>\n",
       "      <td>-0.155801</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032497</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.206157</td>\n",
       "      <td>0.104599</td>\n",
       "      <td>-0.118130</td>\n",
       "      <td>-0.018723</td>\n",
       "      <td>-0.324202</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.114383</td>\n",
       "      <td>0.012223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz9</th>\n",
       "      <td>-0.060460</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>-0.089136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.259486</td>\n",
       "      <td>0.190425</td>\n",
       "      <td>-0.099696</td>\n",
       "      <td>-0.020137</td>\n",
       "      <td>0.081924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076615</td>\n",
       "      <td>0.043801</td>\n",
       "      <td>0.040417</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>-0.111427</td>\n",
       "      <td>-0.038288</td>\n",
       "      <td>-0.235751</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.029411</td>\n",
       "      <td>-0.021528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz10</th>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.091858</td>\n",
       "      <td>0.053768</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>0.259486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161689</td>\n",
       "      <td>0.049208</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>0.130218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169327</td>\n",
       "      <td>0.147388</td>\n",
       "      <td>0.103499</td>\n",
       "      <td>0.043581</td>\n",
       "      <td>-0.018755</td>\n",
       "      <td>-0.058197</td>\n",
       "      <td>-0.206432</td>\n",
       "      <td>0.157785</td>\n",
       "      <td>0.131084</td>\n",
       "      <td>0.016295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz11</th>\n",
       "      <td>-0.084494</td>\n",
       "      <td>-0.167797</td>\n",
       "      <td>0.166708</td>\n",
       "      <td>-0.038255</td>\n",
       "      <td>0.190425</td>\n",
       "      <td>0.161689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029685</td>\n",
       "      <td>-0.065076</td>\n",
       "      <td>0.063633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031694</td>\n",
       "      <td>0.042334</td>\n",
       "      <td>0.027637</td>\n",
       "      <td>-0.009680</td>\n",
       "      <td>-0.105179</td>\n",
       "      <td>-0.056420</td>\n",
       "      <td>-0.205251</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.057362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz12</th>\n",
       "      <td>0.304376</td>\n",
       "      <td>0.378954</td>\n",
       "      <td>-0.215724</td>\n",
       "      <td>-0.059768</td>\n",
       "      <td>-0.099696</td>\n",
       "      <td>0.049208</td>\n",
       "      <td>-0.029685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533218</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014387</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>-0.046656</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.800944</td>\n",
       "      <td>-0.266241</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.754391</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.633380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz13</th>\n",
       "      <td>0.156411</td>\n",
       "      <td>0.442770</td>\n",
       "      <td>0.034366</td>\n",
       "      <td>-0.155801</td>\n",
       "      <td>-0.020137</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>-0.065076</td>\n",
       "      <td>0.533218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005104</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>-0.018300</td>\n",
       "      <td>-0.051850</td>\n",
       "      <td>0.760464</td>\n",
       "      <td>-0.025088</td>\n",
       "      <td>0.539550</td>\n",
       "      <td>0.679838</td>\n",
       "      <td>0.199864</td>\n",
       "      <td>0.405258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz31</th>\n",
       "      <td>-0.098912</td>\n",
       "      <td>-0.073512</td>\n",
       "      <td>0.131824</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>0.081924</td>\n",
       "      <td>0.130218</td>\n",
       "      <td>0.063633</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>-0.045402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.254547</td>\n",
       "      <td>0.599075</td>\n",
       "      <td>-0.109544</td>\n",
       "      <td>-0.057347</td>\n",
       "      <td>-0.090426</td>\n",
       "      <td>-0.194852</td>\n",
       "      <td>0.036309</td>\n",
       "      <td>0.183754</td>\n",
       "      <td>0.004690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz83</th>\n",
       "      <td>0.031195</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>-0.024143</td>\n",
       "      <td>0.110932</td>\n",
       "      <td>-0.015871</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.033560</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>-0.021117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>-0.085777</td>\n",
       "      <td>-0.048533</td>\n",
       "      <td>-0.052434</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.132959</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>-0.101287</td>\n",
       "      <td>0.001786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz87</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz124</th>\n",
       "      <td>-0.018389</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>-0.076814</td>\n",
       "      <td>0.127664</td>\n",
       "      <td>-0.024457</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>-0.007362</td>\n",
       "      <td>-0.086178</td>\n",
       "      <td>0.035711</td>\n",
       "      <td>-0.112024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.141358</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.095944</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>0.645091</td>\n",
       "      <td>-0.080025</td>\n",
       "      <td>0.054817</td>\n",
       "      <td>0.113884</td>\n",
       "      <td>-0.245342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz125</th>\n",
       "      <td>-0.031138</td>\n",
       "      <td>0.058156</td>\n",
       "      <td>0.061894</td>\n",
       "      <td>0.129290</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>0.102508</td>\n",
       "      <td>-0.033884</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>0.237773</td>\n",
       "      <td>0.108767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040576</td>\n",
       "      <td>0.514682</td>\n",
       "      <td>0.241797</td>\n",
       "      <td>0.423865</td>\n",
       "      <td>0.236878</td>\n",
       "      <td>0.171192</td>\n",
       "      <td>-0.184400</td>\n",
       "      <td>0.337788</td>\n",
       "      <td>0.718157</td>\n",
       "      <td>-0.367490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz126</th>\n",
       "      <td>-0.083457</td>\n",
       "      <td>-0.129452</td>\n",
       "      <td>0.786114</td>\n",
       "      <td>0.185521</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>0.191495</td>\n",
       "      <td>-0.193936</td>\n",
       "      <td>-0.050622</td>\n",
       "      <td>0.135898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.073160</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>0.019605</td>\n",
       "      <td>-0.186010</td>\n",
       "      <td>-0.114914</td>\n",
       "      <td>-0.317218</td>\n",
       "      <td>0.135460</td>\n",
       "      <td>-0.021768</td>\n",
       "      <td>0.143682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz127</th>\n",
       "      <td>-0.048017</td>\n",
       "      <td>-0.162873</td>\n",
       "      <td>0.098103</td>\n",
       "      <td>0.526418</td>\n",
       "      <td>0.447728</td>\n",
       "      <td>0.113916</td>\n",
       "      <td>0.142278</td>\n",
       "      <td>-0.092136</td>\n",
       "      <td>-0.092651</td>\n",
       "      <td>0.121696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037042</td>\n",
       "      <td>0.157494</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>0.105794</td>\n",
       "      <td>-0.129424</td>\n",
       "      <td>-0.053259</td>\n",
       "      <td>-0.304526</td>\n",
       "      <td>0.120092</td>\n",
       "      <td>0.019291</td>\n",
       "      <td>0.028273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz128</th>\n",
       "      <td>-0.099260</td>\n",
       "      <td>-0.026913</td>\n",
       "      <td>0.124771</td>\n",
       "      <td>-0.035903</td>\n",
       "      <td>0.063983</td>\n",
       "      <td>0.284821</td>\n",
       "      <td>0.065743</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>-0.015088</td>\n",
       "      <td>0.159640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140160</td>\n",
       "      <td>0.091074</td>\n",
       "      <td>0.047598</td>\n",
       "      <td>-0.050474</td>\n",
       "      <td>0.055793</td>\n",
       "      <td>-0.123181</td>\n",
       "      <td>-0.023329</td>\n",
       "      <td>0.135255</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.148103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz131</th>\n",
       "      <td>-0.010902</td>\n",
       "      <td>-0.086671</td>\n",
       "      <td>0.091968</td>\n",
       "      <td>0.670682</td>\n",
       "      <td>0.246292</td>\n",
       "      <td>0.043745</td>\n",
       "      <td>-0.022488</td>\n",
       "      <td>-0.069273</td>\n",
       "      <td>-0.136306</td>\n",
       "      <td>0.145489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056929</td>\n",
       "      <td>0.119783</td>\n",
       "      <td>0.278207</td>\n",
       "      <td>0.053613</td>\n",
       "      <td>-0.117071</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>-0.299280</td>\n",
       "      <td>0.142604</td>\n",
       "      <td>0.139326</td>\n",
       "      <td>-0.028346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz133</th>\n",
       "      <td>0.011064</td>\n",
       "      <td>-0.107349</td>\n",
       "      <td>0.102419</td>\n",
       "      <td>0.072680</td>\n",
       "      <td>0.167299</td>\n",
       "      <td>0.610761</td>\n",
       "      <td>0.031858</td>\n",
       "      <td>0.021426</td>\n",
       "      <td>-0.109504</td>\n",
       "      <td>0.097231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071758</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>0.127770</td>\n",
       "      <td>0.099369</td>\n",
       "      <td>-0.067528</td>\n",
       "      <td>-0.021076</td>\n",
       "      <td>-0.237249</td>\n",
       "      <td>0.142560</td>\n",
       "      <td>0.115289</td>\n",
       "      <td>-0.012867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz150</th>\n",
       "      <td>0.131392</td>\n",
       "      <td>0.283455</td>\n",
       "      <td>0.074908</td>\n",
       "      <td>-0.059271</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>0.063627</td>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.564585</td>\n",
       "      <td>0.673718</td>\n",
       "      <td>-0.009092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009142</td>\n",
       "      <td>0.024995</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>-0.117867</td>\n",
       "      <td>0.588622</td>\n",
       "      <td>-0.194516</td>\n",
       "      <td>0.459586</td>\n",
       "      <td>0.571198</td>\n",
       "      <td>-0.085712</td>\n",
       "      <td>0.549881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz151</th>\n",
       "      <td>0.169315</td>\n",
       "      <td>0.231326</td>\n",
       "      <td>0.052524</td>\n",
       "      <td>-0.105388</td>\n",
       "      <td>-0.083610</td>\n",
       "      <td>-0.049056</td>\n",
       "      <td>-0.076693</td>\n",
       "      <td>0.238794</td>\n",
       "      <td>0.529933</td>\n",
       "      <td>-0.057284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.112347</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.393582</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.236179</td>\n",
       "      <td>0.406665</td>\n",
       "      <td>0.169454</td>\n",
       "      <td>0.263421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz165</th>\n",
       "      <td>-0.092314</td>\n",
       "      <td>-0.151410</td>\n",
       "      <td>0.041810</td>\n",
       "      <td>0.211975</td>\n",
       "      <td>0.071047</td>\n",
       "      <td>0.155673</td>\n",
       "      <td>0.083932</td>\n",
       "      <td>-0.065448</td>\n",
       "      <td>-0.139020</td>\n",
       "      <td>0.277811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001987</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.222631</td>\n",
       "      <td>0.309172</td>\n",
       "      <td>-0.024985</td>\n",
       "      <td>-0.063578</td>\n",
       "      <td>-0.319773</td>\n",
       "      <td>0.148286</td>\n",
       "      <td>0.365035</td>\n",
       "      <td>-0.182824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz171</th>\n",
       "      <td>-0.045455</td>\n",
       "      <td>-0.136721</td>\n",
       "      <td>0.569926</td>\n",
       "      <td>0.187605</td>\n",
       "      <td>0.137304</td>\n",
       "      <td>0.072559</td>\n",
       "      <td>0.212476</td>\n",
       "      <td>-0.119154</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.042239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046116</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>0.026854</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>-0.132419</td>\n",
       "      <td>-0.095667</td>\n",
       "      <td>-0.225294</td>\n",
       "      <td>0.139672</td>\n",
       "      <td>-0.111141</td>\n",
       "      <td>0.229288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz172</th>\n",
       "      <td>0.220806</td>\n",
       "      <td>0.277801</td>\n",
       "      <td>0.140732</td>\n",
       "      <td>0.140902</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>0.132536</td>\n",
       "      <td>-0.020734</td>\n",
       "      <td>0.762355</td>\n",
       "      <td>0.455822</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.074796</td>\n",
       "      <td>0.111191</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>0.620160</td>\n",
       "      <td>-0.291348</td>\n",
       "      <td>0.346299</td>\n",
       "      <td>0.745173</td>\n",
       "      <td>0.102203</td>\n",
       "      <td>0.505470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz173</th>\n",
       "      <td>-0.078245</td>\n",
       "      <td>0.039601</td>\n",
       "      <td>-0.038596</td>\n",
       "      <td>0.018726</td>\n",
       "      <td>0.179422</td>\n",
       "      <td>0.151734</td>\n",
       "      <td>-0.019890</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079853</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>0.007574</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>-0.014040</td>\n",
       "      <td>-0.099606</td>\n",
       "      <td>0.041039</td>\n",
       "      <td>0.163147</td>\n",
       "      <td>-0.081244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz175</th>\n",
       "      <td>-0.012380</td>\n",
       "      <td>-0.043645</td>\n",
       "      <td>-0.163051</td>\n",
       "      <td>0.057978</td>\n",
       "      <td>-0.066236</td>\n",
       "      <td>-0.060167</td>\n",
       "      <td>-0.069369</td>\n",
       "      <td>-0.124638</td>\n",
       "      <td>-0.023387</td>\n",
       "      <td>-0.152107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.168775</td>\n",
       "      <td>-0.162662</td>\n",
       "      <td>0.421710</td>\n",
       "      <td>-0.045922</td>\n",
       "      <td>0.627118</td>\n",
       "      <td>-0.132645</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.098809</td>\n",
       "      <td>-0.407880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz176</th>\n",
       "      <td>-0.000298</td>\n",
       "      <td>-0.134429</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>0.257969</td>\n",
       "      <td>0.113824</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>-0.070452</td>\n",
       "      <td>-0.087686</td>\n",
       "      <td>0.217254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.269779</td>\n",
       "      <td>0.076338</td>\n",
       "      <td>0.177741</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>-0.070027</td>\n",
       "      <td>-0.245506</td>\n",
       "      <td>0.056796</td>\n",
       "      <td>0.152480</td>\n",
       "      <td>-0.085493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz177</th>\n",
       "      <td>-0.092176</td>\n",
       "      <td>-0.089704</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.295385</td>\n",
       "      <td>0.128754</td>\n",
       "      <td>0.053406</td>\n",
       "      <td>0.117837</td>\n",
       "      <td>-0.163540</td>\n",
       "      <td>-0.132512</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>0.113712</td>\n",
       "      <td>0.238541</td>\n",
       "      <td>0.064173</td>\n",
       "      <td>-0.164924</td>\n",
       "      <td>-0.045060</td>\n",
       "      <td>-0.306503</td>\n",
       "      <td>0.090259</td>\n",
       "      <td>0.078712</td>\n",
       "      <td>-0.035816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz178</th>\n",
       "      <td>-0.020559</td>\n",
       "      <td>-0.071750</td>\n",
       "      <td>0.054138</td>\n",
       "      <td>0.032497</td>\n",
       "      <td>0.076615</td>\n",
       "      <td>0.169327</td>\n",
       "      <td>0.031694</td>\n",
       "      <td>-0.014387</td>\n",
       "      <td>-0.005104</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>-0.007770</td>\n",
       "      <td>-0.043385</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>-0.081462</td>\n",
       "      <td>0.041546</td>\n",
       "      <td>0.031719</td>\n",
       "      <td>-0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz183</th>\n",
       "      <td>-0.147482</td>\n",
       "      <td>-0.096255</td>\n",
       "      <td>0.075510</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>0.043801</td>\n",
       "      <td>0.147388</td>\n",
       "      <td>0.042334</td>\n",
       "      <td>0.022913</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>0.254547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313175</td>\n",
       "      <td>0.443884</td>\n",
       "      <td>0.108985</td>\n",
       "      <td>-0.130686</td>\n",
       "      <td>-0.322641</td>\n",
       "      <td>0.260439</td>\n",
       "      <td>0.570388</td>\n",
       "      <td>-0.204673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz185</th>\n",
       "      <td>-0.086607</td>\n",
       "      <td>-0.047670</td>\n",
       "      <td>0.210204</td>\n",
       "      <td>0.206157</td>\n",
       "      <td>0.040417</td>\n",
       "      <td>0.103499</td>\n",
       "      <td>0.027637</td>\n",
       "      <td>-0.046656</td>\n",
       "      <td>-0.018300</td>\n",
       "      <td>0.599075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.313175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.059893</td>\n",
       "      <td>-0.010501</td>\n",
       "      <td>-0.126345</td>\n",
       "      <td>-0.210161</td>\n",
       "      <td>0.116276</td>\n",
       "      <td>0.315137</td>\n",
       "      <td>0.057476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz197</th>\n",
       "      <td>-0.054875</td>\n",
       "      <td>-0.078564</td>\n",
       "      <td>0.019698</td>\n",
       "      <td>0.104599</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.043581</td>\n",
       "      <td>-0.009680</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-0.051850</td>\n",
       "      <td>-0.109544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007770</td>\n",
       "      <td>0.443884</td>\n",
       "      <td>-0.059893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>-0.038753</td>\n",
       "      <td>-0.288329</td>\n",
       "      <td>0.176082</td>\n",
       "      <td>0.256706</td>\n",
       "      <td>-0.242877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz246</th>\n",
       "      <td>0.314936</td>\n",
       "      <td>0.677738</td>\n",
       "      <td>-0.166455</td>\n",
       "      <td>-0.118130</td>\n",
       "      <td>-0.111427</td>\n",
       "      <td>-0.018755</td>\n",
       "      <td>-0.105179</td>\n",
       "      <td>0.800944</td>\n",
       "      <td>0.760464</td>\n",
       "      <td>-0.057347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043385</td>\n",
       "      <td>0.108985</td>\n",
       "      <td>-0.010501</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.147460</td>\n",
       "      <td>0.766937</td>\n",
       "      <td>0.743539</td>\n",
       "      <td>0.227586</td>\n",
       "      <td>0.488632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz247</th>\n",
       "      <td>-0.015068</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>-0.160602</td>\n",
       "      <td>-0.018723</td>\n",
       "      <td>-0.038288</td>\n",
       "      <td>-0.058197</td>\n",
       "      <td>-0.056420</td>\n",
       "      <td>-0.266241</td>\n",
       "      <td>-0.025088</td>\n",
       "      <td>-0.090426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>-0.130686</td>\n",
       "      <td>-0.126345</td>\n",
       "      <td>-0.038753</td>\n",
       "      <td>-0.147460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.057101</td>\n",
       "      <td>-0.180335</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>-0.386518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz248</th>\n",
       "      <td>0.332724</td>\n",
       "      <td>0.662103</td>\n",
       "      <td>-0.350819</td>\n",
       "      <td>-0.324202</td>\n",
       "      <td>-0.235751</td>\n",
       "      <td>-0.206432</td>\n",
       "      <td>-0.205251</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.539550</td>\n",
       "      <td>-0.194852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081462</td>\n",
       "      <td>-0.322641</td>\n",
       "      <td>-0.210161</td>\n",
       "      <td>-0.288329</td>\n",
       "      <td>0.766937</td>\n",
       "      <td>-0.057101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282789</td>\n",
       "      <td>-0.207160</td>\n",
       "      <td>0.457207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz249</th>\n",
       "      <td>0.201221</td>\n",
       "      <td>0.300505</td>\n",
       "      <td>0.188942</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>0.157785</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>0.754391</td>\n",
       "      <td>0.679838</td>\n",
       "      <td>0.036309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041546</td>\n",
       "      <td>0.260439</td>\n",
       "      <td>0.116276</td>\n",
       "      <td>0.176082</td>\n",
       "      <td>0.743539</td>\n",
       "      <td>-0.180335</td>\n",
       "      <td>0.282789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>0.544122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz250</th>\n",
       "      <td>-0.073183</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>0.060327</td>\n",
       "      <td>0.114383</td>\n",
       "      <td>0.029411</td>\n",
       "      <td>0.131084</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.199864</td>\n",
       "      <td>0.183754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031719</td>\n",
       "      <td>0.570388</td>\n",
       "      <td>0.315137</td>\n",
       "      <td>0.256706</td>\n",
       "      <td>0.227586</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>-0.207160</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.267518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oz251</th>\n",
       "      <td>0.299677</td>\n",
       "      <td>0.262485</td>\n",
       "      <td>0.063637</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>0.016295</td>\n",
       "      <td>0.057362</td>\n",
       "      <td>0.633380</td>\n",
       "      <td>0.405258</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004813</td>\n",
       "      <td>-0.204673</td>\n",
       "      <td>0.057476</td>\n",
       "      <td>-0.242877</td>\n",
       "      <td>0.488632</td>\n",
       "      <td>-0.386518</td>\n",
       "      <td>0.457207</td>\n",
       "      <td>0.544122</td>\n",
       "      <td>-0.267518</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            oz1       oz2       oz5       oz6       oz9      oz10      oz11  \\\n",
       "oz1    1.000000  0.280895 -0.083059  0.007783 -0.060460 -0.014210 -0.084494   \n",
       "oz2    0.280895  1.000000 -0.116349 -0.129057 -0.105957 -0.091858 -0.167797   \n",
       "oz5   -0.083059 -0.116349  1.000000  0.134819  0.023010  0.053768  0.166708   \n",
       "oz6    0.007783 -0.129057  0.134819  1.000000 -0.089136 -0.039200 -0.038255   \n",
       "oz9   -0.060460 -0.105957  0.023010 -0.089136  1.000000  0.259486  0.190425   \n",
       "oz10  -0.014210 -0.091858  0.053768 -0.039200  0.259486  1.000000  0.161689   \n",
       "oz11  -0.084494 -0.167797  0.166708 -0.038255  0.190425  0.161689  1.000000   \n",
       "oz12   0.304376  0.378954 -0.215724 -0.059768 -0.099696  0.049208 -0.029685   \n",
       "oz13   0.156411  0.442770  0.034366 -0.155801 -0.020137 -0.016150 -0.065076   \n",
       "oz31  -0.098912 -0.073512  0.131824  0.145785  0.081924  0.130218  0.063633   \n",
       "oz83   0.031195  0.032946 -0.024143  0.110932 -0.015871  0.004914  0.105691   \n",
       "oz87        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "oz124 -0.018389  0.018519 -0.076814  0.127664 -0.024457  0.004395 -0.007362   \n",
       "oz125 -0.031138  0.058156  0.061894  0.129290  0.027838  0.102508 -0.033884   \n",
       "oz126 -0.083457 -0.129452  0.786114  0.185521  0.014388  0.034240  0.191495   \n",
       "oz127 -0.048017 -0.162873  0.098103  0.526418  0.447728  0.113916  0.142278   \n",
       "oz128 -0.099260 -0.026913  0.124771 -0.035903  0.063983  0.284821  0.065743   \n",
       "oz131 -0.010902 -0.086671  0.091968  0.670682  0.246292  0.043745 -0.022488   \n",
       "oz133  0.011064 -0.107349  0.102419  0.072680  0.167299  0.610761  0.031858   \n",
       "oz150  0.131392  0.283455  0.074908 -0.059271  0.036976  0.063627  0.047580   \n",
       "oz151  0.169315  0.231326  0.052524 -0.105388 -0.083610 -0.049056 -0.076693   \n",
       "oz165 -0.092314 -0.151410  0.041810  0.211975  0.071047  0.155673  0.083932   \n",
       "oz171 -0.045455 -0.136721  0.569926  0.187605  0.137304  0.072559  0.212476   \n",
       "oz172  0.220806  0.277801  0.140732  0.140902  0.018272  0.132536 -0.020734   \n",
       "oz173 -0.078245  0.039601 -0.038596  0.018726  0.179422  0.151734 -0.019890   \n",
       "oz175 -0.012380 -0.043645 -0.163051  0.057978 -0.066236 -0.060167 -0.069369   \n",
       "oz176 -0.000298 -0.134429  0.038574  0.257969  0.113824  0.081535 -0.018409   \n",
       "oz177 -0.092176 -0.089704  0.442400  0.295385  0.128754  0.053406  0.117837   \n",
       "oz178 -0.020559 -0.071750  0.054138  0.032497  0.076615  0.169327  0.031694   \n",
       "oz183 -0.147482 -0.096255  0.075510  0.184388  0.043801  0.147388  0.042334   \n",
       "oz185 -0.086607 -0.047670  0.210204  0.206157  0.040417  0.103499  0.027637   \n",
       "oz197 -0.054875 -0.078564  0.019698  0.104599  0.028205  0.043581 -0.009680   \n",
       "oz246  0.314936  0.677738 -0.166455 -0.118130 -0.111427 -0.018755 -0.105179   \n",
       "oz247 -0.015068 -0.000836 -0.160602 -0.018723 -0.038288 -0.058197 -0.056420   \n",
       "oz248  0.332724  0.662103 -0.350819 -0.324202 -0.235751 -0.206432 -0.205251   \n",
       "oz249  0.201221  0.300505  0.188942  0.183486  0.038436  0.157785  0.061109   \n",
       "oz250 -0.073183  0.021179  0.060327  0.114383  0.029411  0.131084  0.013738   \n",
       "oz251  0.299677  0.262485  0.063637  0.012223 -0.021528  0.016295  0.057362   \n",
       "\n",
       "           oz12      oz13      oz31  ...     oz178     oz183     oz185  \\\n",
       "oz1    0.304376  0.156411 -0.098912  ... -0.020559 -0.147482 -0.086607   \n",
       "oz2    0.378954  0.442770 -0.073512  ... -0.071750 -0.096255 -0.047670   \n",
       "oz5   -0.215724  0.034366  0.131824  ...  0.054138  0.075510  0.210204   \n",
       "oz6   -0.059768 -0.155801  0.145785  ...  0.032497  0.184388  0.206157   \n",
       "oz9   -0.099696 -0.020137  0.081924  ...  0.076615  0.043801  0.040417   \n",
       "oz10   0.049208 -0.016150  0.130218  ...  0.169327  0.147388  0.103499   \n",
       "oz11  -0.029685 -0.065076  0.063633  ...  0.031694  0.042334  0.027637   \n",
       "oz12   1.000000  0.533218 -0.085253  ... -0.014387  0.022913 -0.046656   \n",
       "oz13   0.533218  1.000000 -0.045402  ... -0.005104  0.041842 -0.018300   \n",
       "oz31  -0.085253 -0.045402  1.000000  ...  0.017798  0.254547  0.599075   \n",
       "oz83   0.033560  0.013491 -0.021117  ...  0.019170 -0.085777 -0.048533   \n",
       "oz87        NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "oz124 -0.086178  0.035711 -0.112024  ...  0.005756  0.141358  0.031869   \n",
       "oz125 -0.011935  0.237773  0.108767  ...  0.040576  0.514682  0.241797   \n",
       "oz126 -0.193936 -0.050622  0.135898  ...  0.032680  0.073160  0.190569   \n",
       "oz127 -0.092136 -0.092651  0.121696  ...  0.037042  0.157494  0.026095   \n",
       "oz128  0.045427 -0.015088  0.159640  ...  0.140160  0.091074  0.047598   \n",
       "oz131 -0.069273 -0.136306  0.145489  ...  0.056929  0.119783  0.278207   \n",
       "oz133  0.021426 -0.109504  0.097231  ...  0.071758  0.176048  0.127770   \n",
       "oz150  0.564585  0.673718 -0.009092  ... -0.009142  0.024995  0.002718   \n",
       "oz151  0.238794  0.529933 -0.057284  ...  0.011987  0.112347 -0.023256   \n",
       "oz165 -0.065448 -0.139020  0.277811  ... -0.001987  0.495800  0.222631   \n",
       "oz171 -0.119154  0.015630  0.042239  ...  0.046116  0.009987  0.026854   \n",
       "oz172  0.762355  0.455822  0.010340  ...  0.038244  0.074796  0.111191   \n",
       "oz173  0.001584  0.028116  0.009634  ...  0.079853  0.024565  0.007574   \n",
       "oz175 -0.124638 -0.023387 -0.152107  ...  0.006351  0.168775 -0.162662   \n",
       "oz176 -0.070452 -0.087686  0.217254  ...  0.024447  0.269779  0.076338   \n",
       "oz177 -0.163540 -0.132512  0.147321  ... -0.002749  0.113712  0.238541   \n",
       "oz178 -0.014387 -0.005104  0.017798  ...  1.000000  0.027107  0.011736   \n",
       "oz183  0.022913  0.041842  0.254547  ...  0.027107  1.000000  0.313175   \n",
       "oz185 -0.046656 -0.018300  0.599075  ...  0.011736  0.313175  1.000000   \n",
       "oz197  0.000600 -0.051850 -0.109544  ... -0.007770  0.443884 -0.059893   \n",
       "oz246  0.800944  0.760464 -0.057347  ... -0.043385  0.108985 -0.010501   \n",
       "oz247 -0.266241 -0.025088 -0.090426  ...  0.008877 -0.130686 -0.126345   \n",
       "oz248  0.625882  0.539550 -0.194852  ... -0.081462 -0.322641 -0.210161   \n",
       "oz249  0.754391  0.679838  0.036309  ...  0.041546  0.260439  0.116276   \n",
       "oz250  0.014906  0.199864  0.183754  ...  0.031719  0.570388  0.315137   \n",
       "oz251  0.633380  0.405258  0.004690  ... -0.004813 -0.204673  0.057476   \n",
       "\n",
       "          oz197     oz246     oz247     oz248     oz249     oz250     oz251  \n",
       "oz1   -0.054875  0.314936 -0.015068  0.332724  0.201221 -0.073183  0.299677  \n",
       "oz2   -0.078564  0.677738 -0.000836  0.662103  0.300505  0.021179  0.262485  \n",
       "oz5    0.019698 -0.166455 -0.160602 -0.350819  0.188942  0.060327  0.063637  \n",
       "oz6    0.104599 -0.118130 -0.018723 -0.324202  0.183486  0.114383  0.012223  \n",
       "oz9    0.028205 -0.111427 -0.038288 -0.235751  0.038436  0.029411 -0.021528  \n",
       "oz10   0.043581 -0.018755 -0.058197 -0.206432  0.157785  0.131084  0.016295  \n",
       "oz11  -0.009680 -0.105179 -0.056420 -0.205251  0.061109  0.013738  0.057362  \n",
       "oz12   0.000600  0.800944 -0.266241  0.625882  0.754391  0.014906  0.633380  \n",
       "oz13  -0.051850  0.760464 -0.025088  0.539550  0.679838  0.199864  0.405258  \n",
       "oz31  -0.109544 -0.057347 -0.090426 -0.194852  0.036309  0.183754  0.004690  \n",
       "oz83  -0.052434  0.001008  0.132959  0.004034  0.026869 -0.101287  0.001786  \n",
       "oz87        NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "oz124  0.095944  0.012910  0.645091 -0.080025  0.054817  0.113884 -0.245342  \n",
       "oz125  0.423865  0.236878  0.171192 -0.184400  0.337788  0.718157 -0.367490  \n",
       "oz126  0.019605 -0.186010 -0.114914 -0.317218  0.135460 -0.021768  0.143682  \n",
       "oz127  0.105794 -0.129424 -0.053259 -0.304526  0.120092  0.019291  0.028273  \n",
       "oz128 -0.050474  0.055793 -0.123181 -0.023329  0.135255  0.011766  0.148103  \n",
       "oz131  0.053613 -0.117071  0.001163 -0.299280  0.142604  0.139326 -0.028346  \n",
       "oz133  0.099369 -0.067528 -0.021076 -0.237249  0.142560  0.115289 -0.012867  \n",
       "oz150 -0.117867  0.588622 -0.194516  0.459586  0.571198 -0.085712  0.549881  \n",
       "oz151  0.016222  0.393582  0.004720  0.236179  0.406665  0.169454  0.263421  \n",
       "oz165  0.309172 -0.024985 -0.063578 -0.319773  0.148286  0.365035 -0.182824  \n",
       "oz171 -0.027411 -0.132419 -0.095667 -0.225294  0.139672 -0.111141  0.229288  \n",
       "oz172  0.012732  0.620160 -0.291348  0.346299  0.745173  0.102203  0.505470  \n",
       "oz173  0.021350  0.006788 -0.014040 -0.099606  0.041039  0.163147 -0.081244  \n",
       "oz175  0.421710 -0.045922  0.627118 -0.132645  0.007176  0.098809 -0.407880  \n",
       "oz176  0.177741 -0.082335 -0.070027 -0.245506  0.056796  0.152480 -0.085493  \n",
       "oz177  0.064173 -0.164924 -0.045060 -0.306503  0.090259  0.078712 -0.035816  \n",
       "oz178 -0.007770 -0.043385  0.008877 -0.081462  0.041546  0.031719 -0.004813  \n",
       "oz183  0.443884  0.108985 -0.130686 -0.322641  0.260439  0.570388 -0.204673  \n",
       "oz185 -0.059893 -0.010501 -0.126345 -0.210161  0.116276  0.315137  0.057476  \n",
       "oz197  1.000000  0.003867 -0.038753 -0.288329  0.176082  0.256706 -0.242877  \n",
       "oz246  0.003867  1.000000 -0.147460  0.766937  0.743539  0.227586  0.488632  \n",
       "oz247 -0.038753 -0.147460  1.000000 -0.057101 -0.180335  0.002180 -0.386518  \n",
       "oz248 -0.288329  0.766937 -0.057101  1.000000  0.282789 -0.207160  0.457207  \n",
       "oz249  0.176082  0.743539 -0.180335  0.282789  1.000000  0.362018  0.544122  \n",
       "oz250  0.256706  0.227586  0.002180 -0.207160  0.362018  1.000000 -0.267518  \n",
       "oz251 -0.242877  0.488632 -0.386518  0.457207  0.544122 -0.267518  1.000000  \n",
       "\n",
       "[38 rows x 38 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan,  ..., nan, nan, nan])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_hat_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x208552eb4c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oz1      0\n",
       "oz2      0\n",
       "oz5      0\n",
       "oz6      0\n",
       "oz9      0\n",
       "oz10     0\n",
       "oz11     0\n",
       "oz12     0\n",
       "oz13     0\n",
       "oz31     0\n",
       "oz83     0\n",
       "oz87     0\n",
       "oz124    0\n",
       "oz125    0\n",
       "oz126    0\n",
       "oz127    0\n",
       "oz128    0\n",
       "oz131    0\n",
       "oz133    0\n",
       "oz150    0\n",
       "oz151    0\n",
       "oz165    0\n",
       "oz171    0\n",
       "oz172    0\n",
       "oz173    0\n",
       "oz175    0\n",
       "oz176    0\n",
       "oz177    0\n",
       "oz178    0\n",
       "oz183    0\n",
       "oz185    0\n",
       "oz197    0\n",
       "oz246    0\n",
       "oz247    0\n",
       "oz248    0\n",
       "oz249    0\n",
       "oz250    0\n",
       "oz251    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(X_train_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20100153011545416"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)/(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:21:31,769] A new study created in memory with name: no-name-d50f5f91-8114-464f-a511-17f8d5a375dc\n",
      "[W 2024-03-18 20:24:35,328] Trial 0 failed with parameters: {'n_blocks': 4, 'd_block': 20, 'dropout1': 0.6336482349262754, 'dropout2': 0.7488038825386119, 'd_hidden_multiplier': 1.7462675307564761, 'learning_rate': 0.00040431458059668457, 'weight_decay': 9.779447595326315e-08, 'n_epochs': 1000} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-18 20:24:35,328] Trial 0 failed with value tensor(nan).\n",
      "[W 2024-03-18 20:26:21,721] Trial 1 failed with parameters: {'n_blocks': 4, 'd_block': 93, 'dropout1': 0.08833981417401027, 'dropout2': 0.6853598183677972, 'd_hidden_multiplier': 2.8834833654873413, 'learning_rate': 0.00010248404356096479, 'weight_decay': 3.6388262327167683e-06} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_22820\\2071741021.py\", line 33, in ResNet_opt\n",
      "    n_epochs=train(ResNet_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\ONLY NUMERICAL FEATURES\\REGRESSION\\RMSE\\utils.py\", line 43, in train\n",
      "    for batch_X, batch_y in train_loader:\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 162, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-18 20:26:21,721] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m sampler_ResNet \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     51\u001b[0m study_ResNet \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_ResNet, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mstudy_ResNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mResNet_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m ResNet_model \u001b[38;5;241m=\u001b[39m ResNet(\n\u001b[0;32m     55\u001b[0m     d_in\u001b[38;5;241m=\u001b[39md_in,\n\u001b[0;32m     56\u001b[0m     d_out\u001b[38;5;241m=\u001b[39md_out,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     dropout2\u001b[38;5;241m=\u001b[39mstudy_ResNet\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36mResNet_opt\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     32\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mPATIENCE, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mCHECKPOINT_PATH)\n\u001b[1;32m---> 33\u001b[0m n_epochs\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mResNet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain__loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, n_epochs, n_epochs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Point prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\ONLY NUMERICAL FEATURES\\REGRESSION\\RMSE\\utils.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, training_iterations, train_loader, val_loader, early_stopping, checkpoint_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_iterations):\n\u001b[0;32m     42\u001b[0m     n_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     46\u001b[0m             batch_X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #### ResNet\n",
    "def ResNet_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout1 = trial.suggest_float(\"dropout1\", 0, 1)\n",
    "    dropout2 = trial.suggest_float(\"dropout2\", 0, 1)\n",
    "    d_hidden_multiplier=trial.suggest_float(\"d_hidden_multiplier\", 0.5, 3)\n",
    "\n",
    "    ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=d_hidden_multiplier,\n",
    "    dropout1=dropout1,\n",
    "    dropout2=dropout2,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        ResNet_model = ResNet_model.cuda()\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False, path=CHECKPOINT_PATH)\n",
    "    n_epochs=train(ResNet_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in val_loader:\n",
    "            batch_predictions = ResNet_model(batch_X).reshape(-1,)\n",
    "            predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    y_val_hat_ResNet = torch.Tensor(np.concatenate(predictions))\n",
    "    if torch.cuda.is_available():\n",
    "        y_val_hat_ResNet = y_val_hat_ResNet.cuda()\n",
    "    RMSE_ResNet=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_ResNet)))\n",
    "\n",
    "    return RMSE_ResNet\n",
    "\n",
    "sampler_ResNet = optuna.samplers.TPESampler(seed=seed)\n",
    "study_ResNet = optuna.create_study(sampler=sampler_ResNet, direction='minimize')\n",
    "study_ResNet.optimize(ResNet_opt, n_trials=N_TRIALS)\n",
    "\n",
    "ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_ResNet.best_params['n_blocks'],\n",
    "    d_block=study_ResNet.best_params['d_block'],\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=study_ResNet.best_params['d_hidden_multiplier'],\n",
    "    dropout1=study_ResNet.best_params['dropout1'],\n",
    "    dropout2=study_ResNet.best_params['dropout2'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ResNet_model = ResNet_model.cuda()\n",
    "\n",
    "n_epochs=study_ResNet.best_params['n_epochs']\n",
    "learning_rate=study_ResNet.best_params['learning_rate']\n",
    "weight_decay=study_ResNet.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_no_early_stopping(ResNet_model, criterion, optimizer, n_epochs, train_loader)\n",
    "\n",
    "# Point prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_predictions = ResNet_model(batch_X).reshape(-1,)\n",
    "        predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "y_test_hat_ResNet = torch.Tensor(np.concatenate(predictions))\n",
    "if torch.cuda.is_available():\n",
    "    y_test_hat_ResNet = y_test_hat_ResNet.cuda()\n",
    "RMSE_ResNet=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_ResNet)))\n",
    "print(\"RMSE ResNet: \", RMSE_ResNet)\n",
    "del ResNet_model, optimizer, criterion, y_test_hat_ResNet, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9399,  0.3442,  0.4443,  ..., -0.3624, -0.4164,  0.1979],\n",
       "        [ 0.4151,  0.3442, -0.9330,  ..., -0.2869, -1.0724,  0.9329],\n",
       "        [ 0.6579,  0.1255,  1.1330,  ..., -0.9978, -1.6237,  0.2799],\n",
       "        ...,\n",
       "        [-0.0025,  0.3442, -0.2443,  ..., -0.0679,  0.0583,  0.0263],\n",
       "        [ 0.3780,  0.3442, -0.9330,  ..., -0.3675, -0.7911,  0.8239],\n",
       "        [-2.2469,  0.3442,  0.4443,  ...,  0.0218,  2.1524,  0.1049]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train__tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
