{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: overflow encountered in square\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "13\n",
      "Task 361060\n",
      "0\n",
      "0\n",
      "Task 361061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: overflow encountered in square\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: overflow encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: overflow encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not converge\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "Task 361069\n",
      "0\n",
      "0\n",
      "Task 361070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Task 361276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m     counts_\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(kmeans_\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m==\u001b[39mi))\n\u001b[0;32m    138\u001b[0m     mean_k_\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X_train\u001b[38;5;241m.\u001b[39mloc[kmeans_\u001b[38;5;241m.\u001b[39mlabels_\u001b[38;5;241m==\u001b[39mi,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m     mahalanobis_dist_\u001b[38;5;241m.\u001b[39mappend(mahalanobis(mean_k_, mean_, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    141\u001b[0m dist_df_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmahalanobis_dist\u001b[39m\u001b[38;5;124m'\u001b[39m: mahalanobis_dist_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m: counts_}, index\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(N_CLUSTERS))\n\u001b[0;32m    142\u001b[0m dist_df_\u001b[38;5;241m=\u001b[39mdist_df_\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmahalanobis_dist\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LogisticGAM, s\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "#task_id=361055\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    # Create the checkpoint directory if it doesn't exist\n",
    "    os.makedirs('CHECKPOINTS/CLUSTERING', exist_ok=True)\n",
    "    CHECKPOINT_PATH = f'CHECKPOINTS/CLUSTERING/task_{task_id}.pt'\n",
    "\n",
    "    print(f\"Task {task_id}\")\n",
    "\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "    # Find features with absolute correlation > 0.9\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "    # Drop one of the highly correlated features\n",
    "    X = X.drop(high_corr_features, axis=1)\n",
    "\n",
    "    # Transform y to int type, to then be able to apply BCEWithLogitsLoss\n",
    "    # Create a label encoder\n",
    "    le = LabelEncoder()\n",
    "    # Fit the label encoder and transform y to get binary labels\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    # Convert the result back to a pandas Series\n",
    "    y = pd.Series(y_encoded, index=y.index)\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    N_TRIALS=100\n",
    "    N_SAMPLES=100\n",
    "    PATIENCE=40\n",
    "    N_EPOCHS=1000\n",
    "    GP_ITERATIONS=1000\n",
    "    BATCH_SIZE=1024\n",
    "    seed=10\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "    # New new implementation\n",
    "    N_CLUSTERS=20\n",
    "    # calculate the mean and covariance matrix of the dataset\n",
    "    mean = np.mean(X, axis=0)\n",
    "    cov = np.cov(X.T)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # transform data to compute the clusters\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_scaled)\n",
    "    distances=[]\n",
    "    mahalanobis_dist=[]\n",
    "    counts=[]\n",
    "    ideal_len=len(kmeans.labels_)/5\n",
    "    for i in np.arange(N_CLUSTERS):\n",
    "        distances.append(np.abs(np.sum(kmeans.labels_==i)-ideal_len))\n",
    "        counts.append(np.sum(kmeans.labels_==i))\n",
    "        mean_k= np.mean(X.loc[kmeans.labels_==i,:], axis=0)\n",
    "        mahalanobis_dist.append(mahalanobis(mean_k, mean, np.linalg.inv(cov)))\n",
    "\n",
    "    dist_df=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist, 'count': counts}, index=np.arange(N_CLUSTERS))\n",
    "    dist_df=dist_df.sort_values('mahalanobis_dist', ascending=False)\n",
    "    dist_df['cumulative_count']=dist_df['count'].cumsum()\n",
    "    dist_df['abs_diff']=np.abs(dist_df['cumulative_count']-ideal_len)\n",
    "\n",
    "    final=(np.where(dist_df['abs_diff']==np.min(dist_df['abs_diff']))[0])[0]\n",
    "    labelss=dist_df.index[0:final+1].to_list()\n",
    "    labels=pd.Series(kmeans.labels_).isin(labelss)\n",
    "    labels.index=X.index\n",
    "    close_index=labels.index[np.where(labels==False)[0]]\n",
    "    far_index=labels.index[np.where(labels==True)[0]]\n",
    "\n",
    "    X_train = X.loc[close_index,:]\n",
    "    X_test = X.loc[far_index,:]\n",
    "    y_train = y.loc[close_index]\n",
    "    y_test = y.loc[far_index]\n",
    "\n",
    "    # calculate the mean and covariance matrix of the dataset\n",
    "    mean_ = np.mean(X_train, axis=0)\n",
    "    cov_ = np.cov(X_train.T)\n",
    "    scaler_ = StandardScaler()\n",
    "\n",
    "    # transform data to compute the clusters\n",
    "    X_train_scaled = scaler_.fit_transform(X_train)\n",
    "\n",
    "    kmeans_ = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_train_scaled)\n",
    "    distances_=[]\n",
    "    counts_=[]\n",
    "    mahalanobis_dist_=[]\n",
    "    ideal_len_=len(kmeans_.labels_)/5\n",
    "    for i in np.arange(N_CLUSTERS):\n",
    "        distances_.append(np.abs(np.sum(kmeans_.labels_==i)-ideal_len_))\n",
    "        counts_.append(np.sum(kmeans_.labels_==i))\n",
    "        mean_k_= np.mean(X_train.loc[kmeans_.labels_==i,:], axis=0)\n",
    "        mahalanobis_dist_.append(mahalanobis(mean_k_, mean_, np.linalg.inv(cov_)))\n",
    "\n",
    "    dist_df_=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist_, 'count': counts_}, index=np.arange(N_CLUSTERS))\n",
    "    dist_df_=dist_df_.sort_values('mahalanobis_dist', ascending=False)\n",
    "    dist_df_['cumulative_count']=dist_df_['count'].cumsum()\n",
    "    dist_df_['abs_diff']=np.abs(dist_df_['cumulative_count']-ideal_len_)\n",
    "\n",
    "    final_=(np.where(dist_df_['abs_diff']==np.min(dist_df_['abs_diff']))[0])[0]\n",
    "    labelss_=dist_df_.index[0:final_+1].to_list()\n",
    "    labels_=pd.Series(kmeans_.labels_).isin(labelss_)\n",
    "    labels_.index=X_train.index\n",
    "    close_index_=labels_.index[np.where(labels_==False)[0]]\n",
    "    far_index_=labels_.index[np.where(labels_==True)[0]]\n",
    "\n",
    "    X_train_ = X_train.loc[close_index_,:]\n",
    "    X_val = X_train.loc[far_index_,:]\n",
    "    y_train_ = y_train.loc[close_index_]\n",
    "    y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "    # Standardize the data\n",
    "    mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "    std_X_train_ = np.std(X_train_, axis=0)\n",
    "    X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "    X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "    mean_X_train = np.mean(X_train, axis=0)\n",
    "    std_X_train = np.std(X_train, axis=0)\n",
    "    X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "    X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "    y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "    X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Convert to use GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        X_train__tensor = X_train__tensor.cuda()\n",
    "        y_train__tensor = y_train__tensor.cuda()\n",
    "        X_train_tensor = X_train_tensor.cuda()\n",
    "        y_train_tensor = y_train_tensor.cuda()\n",
    "        X_val_tensor = X_val_tensor.cuda()\n",
    "        y_val_tensor = y_val_tensor.cuda()\n",
    "        X_test_tensor = X_test_tensor.cuda()\n",
    "        y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "    # Create flattened versions of the data\n",
    "    y_val_np = y_val.values.flatten()\n",
    "    y_test_np = y_test.values.flatten()\n",
    "\n",
    "    # Create TensorDatasets for training and validation sets\n",
    "    train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    d_out = 1  \n",
    "    d_in=X_train_.shape[1]\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LogisticGAM(s(0, n_splines=5, lam=0)).fit(X_train_, y_train_)\n",
    "    # Predict on the validation set and calculate the log loss\n",
    "    y_val_hat_gam = gam.predict_proba(X_val)\n",
    "    print(np.sum(np.isnan(y_val_hat_gam)))\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LogisticGAM(s(0, n_splines=5, lam=1)).fit(X_train, y_train)\n",
    "    # Predict on the validation set and calculate the log loss\n",
    "    y_test_hat_gam = gam.predict_proba(X_test)\n",
    "    print(np.sum(np.isnan(y_test_hat_gam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>...</th>\n",
       "      <th>D929</th>\n",
       "      <th>D933</th>\n",
       "      <th>D935</th>\n",
       "      <th>D937</th>\n",
       "      <th>D938</th>\n",
       "      <th>D946</th>\n",
       "      <th>D947</th>\n",
       "      <th>D948</th>\n",
       "      <th>D950</th>\n",
       "      <th>D951</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255137</td>\n",
       "      <td>0.311990</td>\n",
       "      <td>0.089471</td>\n",
       "      <td>0.362843</td>\n",
       "      <td>0.425464</td>\n",
       "      <td>0.103463</td>\n",
       "      <td>0.190638</td>\n",
       "      <td>0.329269</td>\n",
       "      <td>0.134115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416037</td>\n",
       "      <td>0.121998</td>\n",
       "      <td>0.270514</td>\n",
       "      <td>0.297385</td>\n",
       "      <td>0.383392</td>\n",
       "      <td>0.391688</td>\n",
       "      <td>0.430166</td>\n",
       "      <td>0.067279</td>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.479129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.255137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434960</td>\n",
       "      <td>0.207307</td>\n",
       "      <td>0.148738</td>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.169440</td>\n",
       "      <td>0.357585</td>\n",
       "      <td>0.226661</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022226</td>\n",
       "      <td>0.105852</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>0.164105</td>\n",
       "      <td>0.050552</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>0.191521</td>\n",
       "      <td>0.330083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>0.311990</td>\n",
       "      <td>0.434960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135801</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.199014</td>\n",
       "      <td>0.088895</td>\n",
       "      <td>0.142389</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276039</td>\n",
       "      <td>0.105379</td>\n",
       "      <td>0.150724</td>\n",
       "      <td>0.176491</td>\n",
       "      <td>0.285083</td>\n",
       "      <td>0.310214</td>\n",
       "      <td>0.211288</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.227322</td>\n",
       "      <td>0.107875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5</th>\n",
       "      <td>0.089471</td>\n",
       "      <td>0.207307</td>\n",
       "      <td>0.135801</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034942</td>\n",
       "      <td>0.098635</td>\n",
       "      <td>0.450795</td>\n",
       "      <td>0.198092</td>\n",
       "      <td>0.054048</td>\n",
       "      <td>0.466983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097102</td>\n",
       "      <td>0.027681</td>\n",
       "      <td>0.031046</td>\n",
       "      <td>0.036977</td>\n",
       "      <td>0.075792</td>\n",
       "      <td>0.077651</td>\n",
       "      <td>0.167525</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.295367</td>\n",
       "      <td>0.213655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D6</th>\n",
       "      <td>0.362843</td>\n",
       "      <td>0.148738</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.034942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785271</td>\n",
       "      <td>0.408143</td>\n",
       "      <td>0.859403</td>\n",
       "      <td>0.888090</td>\n",
       "      <td>0.248665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110053</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.154782</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.091264</td>\n",
       "      <td>0.048149</td>\n",
       "      <td>0.356526</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.583836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D946</th>\n",
       "      <td>0.391688</td>\n",
       "      <td>0.050552</td>\n",
       "      <td>0.310214</td>\n",
       "      <td>0.077651</td>\n",
       "      <td>0.048149</td>\n",
       "      <td>0.054211</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.029747</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.032129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783898</td>\n",
       "      <td>0.210663</td>\n",
       "      <td>0.290536</td>\n",
       "      <td>0.316193</td>\n",
       "      <td>0.487899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272062</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.282823</td>\n",
       "      <td>0.266464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D947</th>\n",
       "      <td>0.430166</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.211288</td>\n",
       "      <td>0.167525</td>\n",
       "      <td>0.356526</td>\n",
       "      <td>0.428132</td>\n",
       "      <td>0.265090</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>0.458063</td>\n",
       "      <td>0.708899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414354</td>\n",
       "      <td>0.129694</td>\n",
       "      <td>0.269644</td>\n",
       "      <td>0.255508</td>\n",
       "      <td>0.242798</td>\n",
       "      <td>0.272062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036361</td>\n",
       "      <td>0.175305</td>\n",
       "      <td>0.708477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D948</th>\n",
       "      <td>0.067279</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.050904</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.160702</td>\n",
       "      <td>0.050996</td>\n",
       "      <td>0.229993</td>\n",
       "      <td>0.188304</td>\n",
       "      <td>0.072346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.048616</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.036361</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165090</td>\n",
       "      <td>0.090653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D950</th>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.191521</td>\n",
       "      <td>0.227322</td>\n",
       "      <td>0.295367</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.248351</td>\n",
       "      <td>0.088807</td>\n",
       "      <td>0.294395</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0.163787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343515</td>\n",
       "      <td>0.181610</td>\n",
       "      <td>0.250923</td>\n",
       "      <td>0.226153</td>\n",
       "      <td>0.314023</td>\n",
       "      <td>0.282823</td>\n",
       "      <td>0.175305</td>\n",
       "      <td>0.165090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D951</th>\n",
       "      <td>0.479129</td>\n",
       "      <td>0.330083</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>0.213655</td>\n",
       "      <td>0.583836</td>\n",
       "      <td>0.563482</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.638238</td>\n",
       "      <td>0.672059</td>\n",
       "      <td>0.317955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435534</td>\n",
       "      <td>0.108272</td>\n",
       "      <td>0.249273</td>\n",
       "      <td>0.208179</td>\n",
       "      <td>0.205438</td>\n",
       "      <td>0.266464</td>\n",
       "      <td>0.708477</td>\n",
       "      <td>0.090653</td>\n",
       "      <td>0.432931</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406 rows Ã— 406 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D1        D2        D3        D5        D6        D7        D8  \\\n",
       "D1    1.000000  0.255137  0.311990  0.089471  0.362843  0.425464  0.103463   \n",
       "D2    0.255137  1.000000  0.434960  0.207307  0.148738  0.016596  0.169440   \n",
       "D3    0.311990  0.434960  1.000000  0.135801  0.045007  0.199014  0.088895   \n",
       "D5    0.089471  0.207307  0.135801  1.000000  0.034942  0.098635  0.450795   \n",
       "D6    0.362843  0.148738  0.045007  0.034942  1.000000  0.785271  0.408143   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "D946  0.391688  0.050552  0.310214  0.077651  0.048149  0.054211  0.006797   \n",
       "D947  0.430166  0.026892  0.211288  0.167525  0.356526  0.428132  0.265090   \n",
       "D948  0.067279  0.049136  0.056003  0.050904  0.241758  0.160702  0.050996   \n",
       "D950  0.612300  0.191521  0.227322  0.295367  0.307950  0.248351  0.088807   \n",
       "D951  0.479129  0.330083  0.107875  0.213655  0.583836  0.563482  0.270431   \n",
       "\n",
       "            D9       D10       D11  ...      D929      D933      D935  \\\n",
       "D1    0.190638  0.329269  0.134115  ...  0.416037  0.121998  0.270514   \n",
       "D2    0.357585  0.226661  0.020539  ...  0.022226  0.105852  0.113732   \n",
       "D3    0.142389  0.006725  0.027162  ...  0.276039  0.105379  0.150724   \n",
       "D5    0.198092  0.054048  0.466983  ...  0.097102  0.027681  0.031046   \n",
       "D6    0.859403  0.888090  0.248665  ...  0.110053  0.073124  0.154782   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "D946  0.029747  0.043040  0.032129  ...  0.783898  0.210663  0.290536   \n",
       "D947  0.311173  0.458063  0.708899  ...  0.414354  0.129694  0.269644   \n",
       "D948  0.229993  0.188304  0.072346  ...  0.012483  0.021634  0.048616   \n",
       "D950  0.294395  0.232855  0.163787  ...  0.343515  0.181610  0.250923   \n",
       "D951  0.638238  0.672059  0.317955  ...  0.435534  0.108272  0.249273   \n",
       "\n",
       "          D937      D938      D946      D947      D948      D950      D951  \n",
       "D1    0.297385  0.383392  0.391688  0.430166  0.067279  0.612300  0.479129  \n",
       "D2    0.121147  0.164105  0.050552  0.026892  0.049136  0.191521  0.330083  \n",
       "D3    0.176491  0.285083  0.310214  0.211288  0.056003  0.227322  0.107875  \n",
       "D5    0.036977  0.075792  0.077651  0.167525  0.050904  0.295367  0.213655  \n",
       "D6    0.101266  0.091264  0.048149  0.356526  0.241758  0.307950  0.583836  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "D946  0.316193  0.487899  1.000000  0.272062  0.007217  0.282823  0.266464  \n",
       "D947  0.255508  0.242798  0.272062  1.000000  0.036361  0.175305  0.708477  \n",
       "D948  0.003819  0.005024  0.007217  0.036361  1.000000  0.165090  0.090653  \n",
       "D950  0.226153  0.314023  0.282823  0.175305  0.165090  1.000000  0.432931  \n",
       "D951  0.208179  0.205438  0.266464  0.708477  0.090653  0.432931  1.000000  \n",
       "\n",
       "[406 rows x 406 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = X.corr().abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-15 00:01:41,274] A new study created in memory with name: no-name-c3686d59-39e4-4d00-a71a-791301a0c72c\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:41,528] Trial 0 finished with value: 0.57351659322509 and parameters: {'n_splines': 17, 'lam': 0.001154132971137168}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:41,652] Trial 1 finished with value: 0.6035648660853605 and parameters: {'n_splines': 15, 'lam': 0.17636469336159113}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:41,757] Trial 2 finished with value: 0.588202876804868 and parameters: {'n_splines': 12, 'lam': 0.00472487079152679}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:41,864] Trial 3 finished with value: 0.5755587664358157 and parameters: {'n_splines': 8, 'lam': 0.19124590142517375}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:41,982] Trial 4 finished with value: 0.5755505236332742 and parameters: {'n_splines': 7, 'lam': 0.0018408544111075849}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:42,145] Trial 5 finished with value: 0.5755574771651957 and parameters: {'n_splines': 15, 'lam': 0.7247363402746422}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:42,255] Trial 6 finished with value: 0.5755589587642473 and parameters: {'n_splines': 5, 'lam': 0.03440145332328687}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:42,523] Trial 7 finished with value: 0.5755226849085425 and parameters: {'n_splines': 18, 'lam': 0.06879837817714736}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:42,673] Trial 8 finished with value: 0.5880503389450551 and parameters: {'n_splines': 16, 'lam': 0.007509797119626296}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:42,865] Trial 9 finished with value: 0.5755361733128125 and parameters: {'n_splines': 19, 'lam': 0.13922824544531598}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:42,995] Trial 10 finished with value: 0.5744126716397207 and parameters: {'n_splines': 12, 'lam': 0.0011944699530622646}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:43,092] Trial 11 finished with value: 0.5880241641475835 and parameters: {'n_splines': 11, 'lam': 0.0010501717973976513}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:43,217] Trial 12 finished with value: 0.5755359895604694 and parameters: {'n_splines': 10, 'lam': 0.005983624053805124}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:43,424] Trial 13 finished with value: 0.5736143744790008 and parameters: {'n_splines': 20, 'lam': 0.0026040422203315156}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:43,592] Trial 14 finished with value: 0.5879103489259858 and parameters: {'n_splines': 20, 'lam': 0.012166168964876191}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:43,769] Trial 15 finished with value: 0.5742277034286337 and parameters: {'n_splines': 17, 'lam': 0.002406672578358626}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:43,910] Trial 16 finished with value: 0.5879321450149606 and parameters: {'n_splines': 20, 'lam': 0.01305628749775024}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:44,049] Trial 17 finished with value: 0.5880025260442723 and parameters: {'n_splines': 14, 'lam': 0.003220041571722928}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:44,221] Trial 18 finished with value: 0.5881272050912172 and parameters: {'n_splines': 18, 'lam': 0.01884613839435256}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:44,409] Trial 19 finished with value: 0.5742044618644091 and parameters: {'n_splines': 17, 'lam': 0.002327078686613123}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "[I 2024-03-15 00:01:44,568] Trial 20 finished with value: 0.5748154529776237 and parameters: {'n_splines': 14, 'lam': 0.0010192205429740875}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:44,716] Trial 21 finished with value: 0.5743123879446355 and parameters: {'n_splines': 17, 'lam': 0.002746469619595178}. Best is trial 0 with value: 0.57351659322509.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:44,937] Trial 22 finished with value: 0.5735132371710341 and parameters: {'n_splines': 19, 'lam': 0.0019132531947572914}. Best is trial 22 with value: 0.5735132371710341.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:45,108] Trial 23 finished with value: 0.5741571921840279 and parameters: {'n_splines': 20, 'lam': 0.004568578798640427}. Best is trial 22 with value: 0.5735132371710341.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:45,319] Trial 24 finished with value: 0.5732813301437192 and parameters: {'n_splines': 19, 'lam': 0.0016248470854720496}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:45,501] Trial 25 finished with value: 0.5735728187518212 and parameters: {'n_splines': 18, 'lam': 0.0015682090553749935}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:45,655] Trial 26 finished with value: 0.5879361695615567 and parameters: {'n_splines': 19, 'lam': 0.010505448535081313}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:45,785] Trial 27 finished with value: 0.5878524336437116 and parameters: {'n_splines': 16, 'lam': 0.003674617704675878}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:45,918] Trial 28 finished with value: 0.5882067870216583 and parameters: {'n_splines': 19, 'lam': 0.036277484318018714}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:46,107] Trial 29 finished with value: 0.5743522900196872 and parameters: {'n_splines': 15, 'lam': 0.001622556489002616}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:46,257] Trial 30 finished with value: 0.5881188566241556 and parameters: {'n_splines': 14, 'lam': 0.005746609962584673}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:46,484] Trial 31 finished with value: 0.5735689547973359 and parameters: {'n_splines': 18, 'lam': 0.0015634515412515997}. Best is trial 24 with value: 0.5732813301437192.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:46,669] Trial 32 finished with value: 0.5729028723731181 and parameters: {'n_splines': 18, 'lam': 0.0010159610611920038}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:46,911] Trial 33 finished with value: 0.573822261674323 and parameters: {'n_splines': 16, 'lam': 0.0011248826928286575}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:47,110] Trial 34 finished with value: 0.5734434608973111 and parameters: {'n_splines': 19, 'lam': 0.00181693063042714}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:47,287] Trial 35 finished with value: 0.5742514376431889 and parameters: {'n_splines': 19, 'lam': 0.00415368793717745}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:47,439] Trial 36 finished with value: 0.5755566666078774 and parameters: {'n_splines': 17, 'lam': 0.8370157768230794}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:47,636] Trial 37 finished with value: 0.573459764637205 and parameters: {'n_splines': 19, 'lam': 0.0018386117114104208}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:47,840] Trial 38 finished with value: 0.5879481367366608 and parameters: {'n_splines': 18, 'lam': 0.008557726704177837}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:47,995] Trial 39 finished with value: 0.5880501086156611 and parameters: {'n_splines': 20, 'lam': 0.02062549811021482}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:48,076] Trial 40 finished with value: 0.5755586500797575 and parameters: {'n_splines': 9, 'lam': 0.24371165556171648}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:48,287] Trial 41 finished with value: 0.5735222991148151 and parameters: {'n_splines': 19, 'lam': 0.001926469307802356}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:48,490] Trial 42 finished with value: 0.5732996163122849 and parameters: {'n_splines': 19, 'lam': 0.001644583940789512}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "[I 2024-03-15 00:01:48,563] Trial 43 finished with value: 0.5755582682456356 and parameters: {'n_splines': 5, 'lam': 0.0014103949978319557}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:48,789] Trial 44 finished with value: 0.5742448087683715 and parameters: {'n_splines': 18, 'lam': 0.0032151103280038448}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:48,898] Trial 45 finished with value: 0.5755589274654879 and parameters: {'n_splines': 7, 'lam': 0.43469422782238243}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:49,072] Trial 46 finished with value: 0.5755244699402656 and parameters: {'n_splines': 17, 'lam': 0.055931710312013376}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:49,300] Trial 47 finished with value: 0.5733875748417351 and parameters: {'n_splines': 20, 'lam': 0.0021946716431594927}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:49,470] Trial 48 finished with value: 0.5742545000865314 and parameters: {'n_splines': 20, 'lam': 0.005253763055455753}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:49,643] Trial 49 finished with value: 0.5742162325204611 and parameters: {'n_splines': 15, 'lam': 0.0013061547016940207}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:49,763] Trial 50 finished with value: 0.5880257141336538 and parameters: {'n_splines': 13, 'lam': 0.002477441002381351}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:49,953] Trial 51 finished with value: 0.5735980455378269 and parameters: {'n_splines': 19, 'lam': 0.002044032592229593}. Best is trial 32 with value: 0.5729028723731181.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:50,161] Trial 52 finished with value: 0.5724078176170925 and parameters: {'n_splines': 20, 'lam': 0.0012775423389647005}. Best is trial 52 with value: 0.5724078176170925.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:50,353] Trial 53 finished with value: 0.5719397892215615 and parameters: {'n_splines': 20, 'lam': 0.0010528640014696895}. Best is trial 53 with value: 0.5719397892215615.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:50,577] Trial 54 finished with value: 0.5718597113679488 and parameters: {'n_splines': 20, 'lam': 0.0010213233749514882}. Best is trial 54 with value: 0.5718597113679488.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:50,777] Trial 55 finished with value: 0.5719218198834823 and parameters: {'n_splines': 20, 'lam': 0.0010456395222563446}. Best is trial 54 with value: 0.5718597113679488.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:50,985] Trial 56 finished with value: 0.571850162670848 and parameters: {'n_splines': 20, 'lam': 0.001017672290319613}. Best is trial 56 with value: 0.571850162670848.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:51,169] Trial 57 finished with value: 0.5718233267995099 and parameters: {'n_splines': 20, 'lam': 0.0010075317555515411}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:51,359] Trial 58 finished with value: 0.5724318499659423 and parameters: {'n_splines': 20, 'lam': 0.0012913684844364916}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:51,552] Trial 59 finished with value: 0.5722545432359432 and parameters: {'n_splines': 20, 'lam': 0.0011952468364415732}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:51,815] Trial 60 finished with value: 0.5718951530525545 and parameters: {'n_splines': 20, 'lam': 0.001035076709571373}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:52,005] Trial 61 finished with value: 0.5718822262026575 and parameters: {'n_splines': 20, 'lam': 0.0010300231630020478}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:52,248] Trial 62 finished with value: 0.5720286595796964 and parameters: {'n_splines': 20, 'lam': 0.0010899168268364045}. Best is trial 57 with value: 0.5718233267995099.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:52,486] Trial 63 finished with value: 0.5718182293975203 and parameters: {'n_splines': 20, 'lam': 0.0010056254069949107}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:52,707] Trial 64 finished with value: 0.5741053718305494 and parameters: {'n_splines': 18, 'lam': 0.0026531390549043832}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:52,904] Trial 65 finished with value: 0.5738862711101282 and parameters: {'n_splines': 20, 'lam': 0.0033316907890355426}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:53,097] Trial 66 finished with value: 0.5730571170704979 and parameters: {'n_splines': 19, 'lam': 0.0014144315791020836}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:53,228] Trial 67 finished with value: 0.5880778695481876 and parameters: {'n_splines': 11, 'lam': 0.0013922882075125596}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:53,419] Trial 68 finished with value: 0.5729263855671617 and parameters: {'n_splines': 18, 'lam': 0.001029046838434125}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:53,575] Trial 69 finished with value: 0.5737186548761432 and parameters: {'n_splines': 19, 'lam': 0.0022618220202669636}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:53,836] Trial 70 finished with value: 0.5718256848194988 and parameters: {'n_splines': 20, 'lam': 0.0010084157409363608}. Best is trial 63 with value: 0.5718182293975203.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:54,052] Trial 71 finished with value: 0.5718128722203681 and parameters: {'n_splines': 20, 'lam': 0.001003628640375279}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:54,244] Trial 72 finished with value: 0.572767461370676 and parameters: {'n_splines': 20, 'lam': 0.0015168468026852825}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:54,447] Trial 73 finished with value: 0.5728941332589514 and parameters: {'n_splines': 19, 'lam': 0.0012907952979341148}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:54,659] Trial 74 finished with value: 0.5730831923568283 and parameters: {'n_splines': 20, 'lam': 0.001804546857806355}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:54,839] Trial 75 finished with value: 0.5728021390384528 and parameters: {'n_splines': 19, 'lam': 0.001229438846545851}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:55,028] Trial 76 finished with value: 0.5741853422013474 and parameters: {'n_splines': 18, 'lam': 0.002949958569268898}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:55,248] Trial 77 finished with value: 0.5733125939198973 and parameters: {'n_splines': 19, 'lam': 0.0016588642400091308}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:55,424] Trial 78 finished with value: 0.5883783568234224 and parameters: {'n_splines': 20, 'lam': 0.0925042240168343}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:55,607] Trial 79 finished with value: 0.5741125391068954 and parameters: {'n_splines': 17, 'lam': 0.0020562182542903156}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:55,828] Trial 80 finished with value: 0.5729237754994033 and parameters: {'n_splines': 18, 'lam': 0.0010275790076390858}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:55,991] Trial 81 finished with value: 0.57226686586604 and parameters: {'n_splines': 20, 'lam': 0.0012015104021039658}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:56,162] Trial 82 finished with value: 0.5718693720246198 and parameters: {'n_splines': 20, 'lam': 0.001025040537524992}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:56,323] Trial 83 finished with value: 0.5731181054247454 and parameters: {'n_splines': 19, 'lam': 0.0014664666725732612}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:56,568] Trial 84 finished with value: 0.5730666983265285 and parameters: {'n_splines': 20, 'lam': 0.001787079901659372}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:56,756] Trial 85 finished with value: 0.5731234178599286 and parameters: {'n_splines': 19, 'lam': 0.0014711670366825112}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:56,961] Trial 86 finished with value: 0.5722377065088305 and parameters: {'n_splines': 20, 'lam': 0.0011867821011944345}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:57,032] Trial 87 finished with value: 0.603786456726596 and parameters: {'n_splines': 6, 'lam': 0.002401733748241679}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:57,227] Trial 88 finished with value: 0.5736066836133952 and parameters: {'n_splines': 19, 'lam': 0.0020583012919760577}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:57,431] Trial 89 finished with value: 0.5727527332774578 and parameters: {'n_splines': 20, 'lam': 0.0015054806597256754}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:57,638] Trial 90 finished with value: 0.5727630183181315 and parameters: {'n_splines': 19, 'lam': 0.0012049381888211386}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:57,878] Trial 91 finished with value: 0.5719300534409386 and parameters: {'n_splines': 20, 'lam': 0.0010489389844816944}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:58,101] Trial 92 finished with value: 0.5718256476550491 and parameters: {'n_splines': 20, 'lam': 0.0010084017981169138}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:58,293] Trial 93 finished with value: 0.5729750235682427 and parameters: {'n_splines': 20, 'lam': 0.0016954182550109821}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:58,482] Trial 94 finished with value: 0.5729101955688266 and parameters: {'n_splines': 19, 'lam': 0.001302086192468348}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:58,688] Trial 95 finished with value: 0.5718676008926287 and parameters: {'n_splines': 20, 'lam': 0.0010243572907953724}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "[I 2024-03-15 00:01:58,847] Trial 96 finished with value: 0.5880175097881175 and parameters: {'n_splines': 20, 'lam': 0.017929365203655314}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:59,034] Trial 97 finished with value: 0.5723711306386655 and parameters: {'n_splines': 19, 'lam': 0.0010011290216935104}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:59,228] Trial 98 finished with value: 0.5734428109749035 and parameters: {'n_splines': 18, 'lam': 0.0014218077693895971}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-03-15 00:01:59,463] Trial 99 finished with value: 0.5731258233029896 and parameters: {'n_splines': 20, 'lam': 0.001851186339673481}. Best is trial 71 with value: 0.5718128722203681.\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss GAM:  0.5633269210014135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\utils.py:801: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[mask] = y[mask] * np.log(y[mask] / u[mask])\n"
     ]
    }
   ],
   "source": [
    "# GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "            'lam': trial.suggest_float('lam', 1e-3, 1, log=True)}\n",
    "    \n",
    "    # Create and train the model\n",
    "    gam = LogisticGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the log loss\n",
    "    y_val_hat_gam = gam.predict_proba(X_val)\n",
    "    y_val_hat_gam_df = pd.DataFrame(y_val_hat_gam)\n",
    "    y_val_hat_gam_df.fillna(0.5, inplace=True)\n",
    "    y_val_hat_gam = y_val_hat_gam_df.values\n",
    "    log_loss_gam = log_loss(y_val, y_val_hat_gam)\n",
    "\n",
    "    return log_loss_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')  # We want to minimize log loss\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LogisticGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict_proba(X_test)\n",
    "y_test_hat_gam_df = pd.DataFrame(y_test_hat_gam)\n",
    "y_test_hat_gam_df.fillna(0.5, inplace=True)\n",
    "y_test_hat_gam = y_test_hat_gam_df.values\n",
    "# Calculate the log loss\n",
    "log_loss_gam = log_loss(y_test, y_test_hat_gam)\n",
    "print(\"Log Loss GAM: \", log_loss_gam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:151: RuntimeWarning: divide by zero encountered in divide\n",
      "  return dist.levels / (mu * (dist.levels - mu))\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:629: RuntimeWarning: overflow encountered in square\n",
      "  self.link.gradient(mu, self.distribution) ** 2\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "gam = LogisticGAM(s(0, n_splines=5, lam=0)).fit(X_train_, y_train_)\n",
    "# Predict on the validation set and calculate the log loss\n",
    "y_val_hat_gam = gam.predict_proba(X_val)\n",
    "np.sum(np.isnan(y_val_hat_gam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25079435],\n",
       "       [0.68667415],\n",
       "       [0.23950299],\n",
       "       ...,\n",
       "       [0.77106915],\n",
       "       [0.56891915],\n",
       "       [0.60820294]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_hat_gam\n",
    "y_val_hat_gam_df = pd.DataFrame(y_val_hat_gam)\n",
    "y_val_hat_gam_df.fillna(0.5, inplace=True)\n",
    "y_val_hat_gam = y_val_hat_gam_df.values\n",
    "y_val_hat_gam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_gam = log_loss(y_val, y_val_hat_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5709890342165368"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_gam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(y_val_hat_gam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RevolvingUtilizationOfUnsecuredLines    0\n",
       "age                                     0\n",
       "NumberOfTime30-59DaysPastDueNotWorse    0\n",
       "DebtRatio                               0\n",
       "MonthlyIncome                           0\n",
       "NumberOfOpenCreditLinesAndLoans         0\n",
       "NumberRealEstateLoansOrLines            0\n",
       "NumberOfDependents                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(X_train_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
